{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rub setup.ipynb first when using on jupyterlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE FOR TRAINING DIFFERENT MODELS\n",
    "TYPE = 'DQN'\n",
    "NAME = 'dueling'\n",
    "DATASET = 'cicddos2019-top'\n",
    "AMOUNT = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/stable_baselines/__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n",
      "  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 8859676435590725852,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 6941747268461902661\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 12949064853774944845\n",
       " physical_device_desc: \"device: XLA_GPU device\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from datetime import datetime\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.deepq.policies import FeedForwardPolicy, register_policy\n",
    "from stable_baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "\n",
    "from stable_baselines import deepq\n",
    "from stable_baselines import A2C\n",
    "from stable_baselines import bench\n",
    "from stable_baselines import logger\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [dueling, double, per, pn]\n",
    "parameters_presets = {\n",
    "    'vanilla': [False, False, False, False],\n",
    "    'dueling': [True, False, False, False],\n",
    "    'double': [False, True, False, False],\n",
    "    'dd': [True, True, False, False],\n",
    "    'pn_dd': [True, True, False, True],\n",
    "    'per_dd': [True, True, True, False],\n",
    "    'pn_per_dd': [True, True, True, True],\n",
    "}\n",
    "# [separate datasets?, train set dir, test set dir or empty, dummies list(only for sets with split train and test), topfeat?]\n",
    "dataset_presets = {\n",
    "    'nslkdd': [\n",
    "        True, \n",
    "        \"/project/datasets/clean-ids-collection/nsl-kdd/clean/KDDTrain.feather\", \n",
    "        \"/project/datasets/clean-ids-collection/nsl-kdd/clean/KDDTest.feather\",\n",
    "        [\n",
    "            'protocol_type', \n",
    "            'service',\n",
    "            'flag'\n",
    "        ],\n",
    "        'class',\n",
    "        'normal',\n",
    "        False\n",
    "              ],\n",
    "    'unswnb15': [\n",
    "        True, \n",
    "        \"/project/datasets/clean-ids-collection/unsw-nb15/clean/designated-train-test-sets/UNSW_NB15_training-set.feather\", \n",
    "        \"/project/datasets/clean-ids-collection/unsw-nb15/clean/designated-train-test-sets/UNSW_NB15_testing-set.feather\",\n",
    "        [\n",
    "            'proto', \n",
    "            'service',\n",
    "            'state'\n",
    "        ],\n",
    "        'attack_cat',\n",
    "        'normal',\n",
    "        False\n",
    "              ],\n",
    "    'cicddos2019': [\n",
    "        False, \n",
    "        \"/project/datasets/clean-ids-collection/cic-ddos2019/clean/cicddos2019.feather\", \n",
    "        None,\n",
    "        [],\n",
    "        'Label',\n",
    "        'Benign',\n",
    "        False\n",
    "              ],\n",
    "    'cicddos2019-top': [\n",
    "        False, \n",
    "        \"/project/datasets/clean-ids-collection/cic-ddos2019/clean/cicddos2019.feather\", \n",
    "        None,\n",
    "        [],\n",
    "        'Label',\n",
    "        'Benign',\n",
    "        True\n",
    "              ],\n",
    "    'cicdos2017': [\n",
    "        False, \n",
    "        \"/project/datasets/clean-ids-collection/cic-dos2017/clean/cicdos2017.feather\", \n",
    "        None,\n",
    "        [],\n",
    "        'Label',\n",
    "        'Benign',\n",
    "        False\n",
    "              ],\n",
    "    'cicids2017': [\n",
    "        False, \n",
    "        \"/project/datasets/clean-ids-collection/cic-ids2017/clean/cicids2017.feather\", \n",
    "        None,\n",
    "        [],\n",
    "        'Label',\n",
    "        'Benign',\n",
    "        False\n",
    "              ],\n",
    "}\n",
    "parameters = parameters_presets[NAME]\n",
    "data_parameters = dataset_presets[DATASET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mal_ben(df, label, normal):\n",
    "    print(\"Preprocessing: make labels binary\")\n",
    "    print()\n",
    "    df[label] = df[label].astype('object')\n",
    "    atk_idx = df.loc[df[label] != normal].index\n",
    "    df.loc[atk_idx, label] = 1.0\n",
    "    df.loc[df.index.difference(atk_idx), label] = 0.0\n",
    "    df[label] = df[label].astype(dtype=np.float32)\n",
    "\n",
    "def preproc(df_train, df_test, columns):\n",
    "    print(\"Preprocessing: One hot encoding + abundant features\")\n",
    "    print()\n",
    "    df_train = pd.get_dummies(df_train,columns=columns)\n",
    "    df_test = pd.get_dummies(df_test,columns=columns)\n",
    "    extra_removables = df_test.columns ^ df_train.columns\n",
    "    for to_remove in extra_removables:\n",
    "            try:\n",
    "                df_test.drop(to_remove, inplace=True, axis=1)\n",
    "            except:\n",
    "                print(f\"{to_remove} already not part of test set, skipping\")\n",
    "            try:\n",
    "                df_train.drop(to_remove, inplace=True, axis=1)\n",
    "            except:\n",
    "                print(f\"{to_remove} already not part of train set, skipping\")\n",
    "    return df_train, df_test\n",
    "\n",
    "def split_df(df):\n",
    "    print(\"Preprocessing: split into X and Y sets\")\n",
    "    print()\n",
    "    col = df.columns[-1]\n",
    "    cols = df.columns[:-1]\n",
    "    return df[cols], df[col]\n",
    "\n",
    "def top_feat(df, dataset):\n",
    "    print(\"Preprocessing: convert to top features\")\n",
    "    print()\n",
    "    features = {\n",
    "        'cicdos2017-top': [\"Init Bwd Win Bytes\",  \"Idle Min\", \"ACK Flag Count\", \"Fwd Packet Length Min\", \"Fwd PSH Flags\"],\n",
    "        'cicids2017-top': [\"Avg Packet Size\", \"Packet Length Variance\", \"Fwd Packet Length Mean\", \"Init Fwd Win Bytes\", \"Bwd IAT Total\"],\n",
    "        'cicddos2019-top': [\"URG Flag Count\", \"Down/Up Ratio\", \"Bwd Packet Length Min\", \"ACK Flag Count\", \"Fwd Packets Length Total\"],\n",
    "        'nslkdd-top': [\"dst_host_serror_rate\", \"service_private\", \"count\",\"dst_host_count\", \"service_domain_u\", \"flag_REJ\", \"dst_host_diff_srv_rate\"],\n",
    "        'unswnb15-top': [\"sttl\", \"dttl\", \"ct_state_ttl\", \"service\", \"dload\", \"rate\", \"dmean\", \"dbytes\", \"dur\", \"is_sm_ips_ports\", \"dloss\"],\n",
    "    }\n",
    "    important_features = features[dataset] + [\"Label\"] # Adding class for custom environment logic\n",
    "    important_features = list(set(important_features))\n",
    "    removable_features = df.columns ^ important_features\n",
    "    \n",
    "    return df.drop(labels=removable_features, axis='columns')\n",
    "\n",
    "def nslkdd_split_df(train_df, test_df):\n",
    "    train_col = train_df.columns[-1]\n",
    "    print(train_col)\n",
    "    train_cols = train_df.columns[:-1]\n",
    "    print(train_cols)\n",
    "    test_col = test_df.columns[-1]\n",
    "    print(test_col)\n",
    "    test_cols = test_df.columns[:-1]\n",
    "    print(test_cols)\n",
    "\n",
    "    return train_df[train_cols], train_df[train_col], test_df[test_cols], test_df[test_col]\n",
    "\n",
    "def balancing_dataset_sampler_df(df, train_frac=0.2, val_frac=0.1, test_frac=0.7):\n",
    "    col = df.columns[-1]\n",
    "    print(col)\n",
    "    cols = df.columns[:-1]\n",
    "    print(cols)\n",
    "    vc = df[col].value_counts()\n",
    "    print(vc)\n",
    "    n = vc.iloc[-1]\n",
    "    print(n)\n",
    "    m = vc.iloc[0]\n",
    "    print(m)\n",
    "    print(int(m-n))\n",
    "    initial_cut = df.loc[df[col] == vc.index[0]].sample(n=int(m-n), replace=False)\n",
    "    print(initial_cut.index)\n",
    "    df = df.drop(index=initial_cut.index)\n",
    "    vc = df[col].value_counts()\n",
    "    print(vc)\n",
    "    print(int(n*train_frac))\n",
    "    train_df = df.groupby(col).apply(lambda x: x.sample(n=int(n*train_frac), replace=False))\n",
    "    train_df.reset_index(level=0, inplace=True, drop=True)\n",
    "    df = df.drop(index=train_df.index)\n",
    "\n",
    "    validation_df = df.groupby(col).apply(lambda x: x.sample(n=int(n*val_frac), replace=False))\n",
    "    validation_df.reset_index(level=0, inplace=True, drop=True)\n",
    "    df = df.drop(index=validation_df.index)\n",
    "\n",
    "    test_df = df.groupby(col).apply(lambda x: x.sample(n=int(n*test_frac), replace=False))\n",
    "    test_df.reset_index(level=0, inplace=True, drop=True)\n",
    "    df = df.drop(index=test_df.index)\n",
    "\n",
    "    return train_df[cols], train_df[col], validation_df[cols], validation_df[col], test_df[cols], test_df[col]\n",
    "\n",
    "\n",
    "def ids_eval(model, x_test, y_test):\n",
    "    print(\"Evaluation: starting validation of the model\")\n",
    "    print()\n",
    "    TP, FP, TN, FN = 0,0,0,0\n",
    "    env = IdsEnv(images_per_episode=1, dataset=(x_test, y_test), random=False)\n",
    "    obs, done = env.reset(), False\n",
    "    try:\n",
    "        while True:\n",
    "            obs, done = env.reset(), False\n",
    "            while not done:\n",
    "                obs, rew, done, info = env.step(model.predict(obs)[0])\n",
    "                label = info['label']\n",
    "                if label == 0 and rew > 0:\n",
    "                    TP += 1\n",
    "                if label == 0 and rew == 0:\n",
    "                    FP += 1\n",
    "                if label == 1 and rew > 0:\n",
    "                    TN += 1\n",
    "                if label == 1 and rew == 0:\n",
    "                    FN += 1\n",
    "\n",
    "    except StopIteration:\n",
    "        accuracy = (float(TP + TN) / (TP + FP + FN + TN)) \n",
    "        precision = (float(TP) / (TP + FP))\n",
    "        recall = (float(TP) / (TP + FN)) # = TPR = Sensitivity\n",
    "        try:\n",
    "            FPR = (float(FP) / (TN + FP)) # 1 - specificity\n",
    "        except:\n",
    "            FPR = 0.0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        print()\n",
    "        print('Evaluation: validation done...')\n",
    "        print('Accuracy: {0}%'.format(accuracy * 100))\n",
    "        print('Precision: {0}%'.format(precision * 100))\n",
    "        print('Recall/TPR/Sensitivity: {0}%'.format(recall * 100))\n",
    "        print('FPR: {0}%'.format(FPR * 100))\n",
    "        print('F1 score: {0}'.format(f1_score))\n",
    "    return [accuracy, precision, recall, FPR, f1_score]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_parameters[0]:\n",
    "    df_train = pd.read_feather(data_parameters[1])\n",
    "    df_test = pd.read_feather(data_parameters[2])\n",
    "    df_train, df_test = preproc(df_train, df_test, data_parameters[3])\n",
    "    LENGTH = len(df_train.columns) - 1\n",
    "else:\n",
    "    df = pd.read_feather(data_parameters[1])\n",
    "    LENGTH = len(df.columns) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 138007 entries, 0 to 138006\n",
      "Data columns (total 67 columns):\n",
      " #   Column                    Non-Null Count   Dtype   \n",
      "---  ------                    --------------   -----   \n",
      " 0   Flow Duration             138007 non-null  int32   \n",
      " 1   Total Fwd Packets         138007 non-null  int16   \n",
      " 2   Total Backward Packets    138007 non-null  int16   \n",
      " 3   Fwd Packets Length Total  138007 non-null  float32 \n",
      " 4   Bwd Packets Length Total  138007 non-null  float32 \n",
      " 5   Fwd Packet Length Max     138007 non-null  float32 \n",
      " 6   Fwd Packet Length Min     138007 non-null  float32 \n",
      " 7   Fwd Packet Length Mean    138007 non-null  float32 \n",
      " 8   Fwd Packet Length Std     138007 non-null  float32 \n",
      " 9   Bwd Packet Length Max     138007 non-null  float32 \n",
      " 10  Bwd Packet Length Min     138007 non-null  float32 \n",
      " 11  Bwd Packet Length Mean    138007 non-null  float32 \n",
      " 12  Bwd Packet Length Std     138007 non-null  float32 \n",
      " 13  Flow Bytes/s              138007 non-null  float32 \n",
      " 14  Flow Packets/s            138007 non-null  float32 \n",
      " 15  Flow IAT Mean             138007 non-null  float32 \n",
      " 16  Flow IAT Std              138007 non-null  float32 \n",
      " 17  Flow IAT Max              138007 non-null  float32 \n",
      " 18  Flow IAT Min              138007 non-null  float32 \n",
      " 19  Fwd IAT Total             138007 non-null  float32 \n",
      " 20  Fwd IAT Mean              138007 non-null  float32 \n",
      " 21  Fwd IAT Std               138007 non-null  float32 \n",
      " 22  Fwd IAT Max               138007 non-null  float32 \n",
      " 23  Fwd IAT Min               138007 non-null  float32 \n",
      " 24  Bwd IAT Total             138007 non-null  float32 \n",
      " 25  Bwd IAT Mean              138007 non-null  float32 \n",
      " 26  Bwd IAT Std               138007 non-null  float32 \n",
      " 27  Bwd IAT Max               138007 non-null  float32 \n",
      " 28  Bwd IAT Min               138007 non-null  float32 \n",
      " 29  Fwd PSH Flags             138007 non-null  int8    \n",
      " 30  Fwd Header Length         138007 non-null  int64   \n",
      " 31  Bwd Header Length         138007 non-null  int64   \n",
      " 32  Fwd Packets/s             138007 non-null  float32 \n",
      " 33  Bwd Packets/s             138007 non-null  float32 \n",
      " 34  Packet Length Min         138007 non-null  float32 \n",
      " 35  Packet Length Max         138007 non-null  float32 \n",
      " 36  Packet Length Mean        138007 non-null  float32 \n",
      " 37  Packet Length Std         138007 non-null  float32 \n",
      " 38  Packet Length Variance    138007 non-null  float32 \n",
      " 39  FIN Flag Count            138007 non-null  int8    \n",
      " 40  SYN Flag Count            138007 non-null  int8    \n",
      " 41  RST Flag Count            138007 non-null  int8    \n",
      " 42  PSH Flag Count            138007 non-null  int8    \n",
      " 43  ACK Flag Count            138007 non-null  int8    \n",
      " 44  URG Flag Count            138007 non-null  int8    \n",
      " 45  ECE Flag Count            138007 non-null  int8    \n",
      " 46  Down/Up Ratio             138007 non-null  float32 \n",
      " 47  Avg Packet Size           138007 non-null  float32 \n",
      " 48  Avg Fwd Segment Size      138007 non-null  float32 \n",
      " 49  Avg Bwd Segment Size      138007 non-null  float32 \n",
      " 50  Subflow Fwd Packets       138007 non-null  int16   \n",
      " 51  Subflow Fwd Bytes         138007 non-null  int32   \n",
      " 52  Subflow Bwd Packets       138007 non-null  int16   \n",
      " 53  Subflow Bwd Bytes         138007 non-null  int32   \n",
      " 54  Init Fwd Win Bytes        138007 non-null  int32   \n",
      " 55  Init Bwd Win Bytes        138007 non-null  int32   \n",
      " 56  Fwd Act Data Packets      138007 non-null  int16   \n",
      " 57  Fwd Seg Size Min          138007 non-null  int32   \n",
      " 58  Active Mean               138007 non-null  float32 \n",
      " 59  Active Std                138007 non-null  float32 \n",
      " 60  Active Max                138007 non-null  float32 \n",
      " 61  Active Min                138007 non-null  float32 \n",
      " 62  Idle Mean                 138007 non-null  float32 \n",
      " 63  Idle Std                  138007 non-null  float32 \n",
      " 64  Idle Max                  138007 non-null  float32 \n",
      " 65  Idle Min                  138007 non-null  float32 \n",
      " 66  Label                     138007 non-null  category\n",
      "dtypes: category(1), float32(45), int16(5), int32(6), int64(2), int8(8)\n",
      "memory usage: 31.5 MB\n"
     ]
    }
   ],
   "source": [
    "if data_parameters[0]:\n",
    "    df_train.info()\n",
    "    df_train.describe()\n",
    "else:\n",
    "    df.info()\n",
    "    df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing: convert to top features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if data_parameters[6]:\n",
    "    if data_parameters[0]:\n",
    "        df_train = top_feat(df_train, DATASET)\n",
    "        df_test = top_feat(df_test, DATASET)\n",
    "    else:\n",
    "        df = top_feat(df, DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing done by Laurens D'Hooge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "- Build an agent to classify network flow automatically\n",
    "- Feed a packet that gets classified\n",
    "- Want the classification to be equal to the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing: make labels binary\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if data_parameters[0]:\n",
    "    mal_ben(df_train, data_parameters[4], data_parameters[5])\n",
    "    mal_ben(df_test, data_parameters[4], data_parameters[5])\n",
    "else: \n",
    "    mal_ben(df, data_parameters[4], data_parameters[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_parameters[0]:\n",
    "    LENGTH = len(df_train.columns) - 1\n",
    "else:\n",
    "    LENGTH = len(df.columns) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "Index(['Fwd Packets Length Total', 'Bwd Packet Length Min', 'ACK Flag Count',\n",
      "       'URG Flag Count', 'Down/Up Ratio'],\n",
      "      dtype='object')\n",
      "0.0    95092\n",
      "1.0    42915\n",
      "Name: Label, dtype: int64\n",
      "42915\n",
      "95092\n",
      "52177\n",
      "Int64Index([113332, 131988, 118772, 121012,  79720,  93331, 110990,  56478,\n",
      "             29298,  82362,\n",
      "            ...\n",
      "             60272,  75449, 104232, 111552, 127563,  31014, 112030,  65670,\n",
      "            125591, 111488],\n",
      "           dtype='int64', length=52177)\n",
      "0.0    42915\n",
      "1.0    42915\n",
      "Name: Label, dtype: int64\n",
      "34332\n"
     ]
    }
   ],
   "source": [
    "if data_parameters[0]:\n",
    "    x_train, y_train, x_test, y_test  = nslkdd_split_df(df_train, df_test)\n",
    "else:\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = balancing_dataset_sampler_df(df, train_frac=0.8, val_frac=0.0, test_frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.to_frame()\n",
    "y_test = y_test.to_frame()\n",
    "\n",
    "# custom keys -> replace by index\n",
    "\n",
    "x_train = x_train.set_index([pd.Index(range (0, len(x_train)))])\n",
    "y_train = y_train.set_index([pd.Index(range (0, len(y_train)))])\n",
    "x_test = x_test.set_index([pd.Index(range (0, len(x_test)))])\n",
    "y_test = y_test.set_index([pd.Index(range (0, len(y_test)))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdsEnv(gym.Env):\n",
    "    def __init__(self, images_per_episode=1, dataset=(x_train, y_train), random=True):\n",
    "        # Actions we can take, classify as malicious or non-malicious (later also the correct attack)\n",
    "        # change to 19 if detectiong all different attacks\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "         # All the features we have, len(important_features) - 1 features and 1 label. Label should not be included\n",
    "        self.observation_space = gym.spaces.Box(low=float('-inf'), high=float('inf'), shape=(LENGTH,))\n",
    "        self.images_per_episode = images_per_episode\n",
    "        self.step_count = 0\n",
    "\n",
    "        self.x, self.y = dataset\n",
    "        self.random = random\n",
    "        self.dataset_idx = 0\n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = int(action == self.expected_action)\n",
    "        current_label = self.expected_action\n",
    "        obs = self._next_obs()\n",
    "\n",
    "        self.step_count += 1\n",
    "        if self.step_count >= self.images_per_episode:\n",
    "            done = True\n",
    "\n",
    "        return obs, reward, done, {'label': current_label}\n",
    "\n",
    "    def _next_obs(self):\n",
    "        if self.random:\n",
    "            next_obs_idx = random.randint(0, len(self.x) - 1)\n",
    "            self.expected_action = int(self.y.iloc[next_obs_idx,:])\n",
    "            obs = self.x.iloc[next_obs_idx,:]\n",
    "\n",
    "        else:\n",
    "            obs = self.x.iloc[self.dataset_idx]\n",
    "            self.expected_action = int(self.y.iloc[self.dataset_idx])\n",
    "\n",
    "            self.dataset_idx += 1\n",
    "            if self.dataset_idx >= len(self.x):\n",
    "                raise StopIteration()\n",
    "        return obs\n",
    "    \n",
    "    def reset(self):\n",
    "        self.step_count = 0\n",
    "\n",
    "        obs = self._next_obs()\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train classifier using dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPolicy(FeedForwardPolicy):\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch,\n",
    "                 reuse=False, obs_phs=None, dueling=True, **_kwargs):\n",
    "        super(CustomPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse,\n",
    "                                        feature_extraction=\"mlp\", obs_phs=obs_phs, dueling=dueling,\n",
    "                                        layer_norm=True, **_kwargs)\n",
    "        \n",
    "register_policy(\"CustomPolicy\", CustomPolicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f16dc128810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f16dc128810>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16dc0c16d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16dc0c16d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16dc0b90d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16dc0b90d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16e93cdf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16e93cdf90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16dc0a13d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16dc0a13d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16dc0a1790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16dc0a1790>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16d478b9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16d478b9d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f16d4680910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f16d4680910>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16bc0aa590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16bc0aa590>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16bc0b8250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16bc0b8250>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16d47aa450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16d47aa450>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16dc02ead0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16dc02ead0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16d46a2310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16d46a2310>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16d46a2310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16d46a2310>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f16d46f01d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f16d46f01d0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16e070e850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16e070e850>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16d479ba10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16d479ba10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16bc097d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16bc097d50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16a8d7cc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16a8d7cc10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16dc0b99d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16dc0b99d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16dc02a690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f16dc02a690>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 10       |\n",
      "| episodes                | 10000    |\n",
      "| mean 100 episode reward | 0.8      |\n",
      "| steps                   | 9999     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 20000    |\n",
      "| mean 100 episode reward | 0.9      |\n",
      "| steps                   | 19999    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 30000    |\n",
      "| mean 100 episode reward | 0.8      |\n",
      "| steps                   | 29999    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 40000    |\n",
      "| mean 100 episode reward | 0.8      |\n",
      "| steps                   | 39999    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 50000    |\n",
      "| mean 100 episode reward | 0.9      |\n",
      "| steps                   | 49999    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 60000    |\n",
      "| mean 100 episode reward | 0.9      |\n",
      "| steps                   | 59999    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 70000    |\n",
      "| mean 100 episode reward | 0.9      |\n",
      "| steps                   | 69999    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 80000    |\n",
      "| mean 100 episode reward | 0.9      |\n",
      "| steps                   | 79999    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 90000    |\n",
      "| mean 100 episode reward | 0.9      |\n",
      "| steps                   | 89999    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 100000   |\n",
      "| mean 100 episode reward | 1        |\n",
      "| steps                   | 99999    |\n",
      "--------------------------------------\n",
      "\n",
      "DQN 1 Training Time: 216.88627862930298\n"
     ]
    }
   ],
   "source": [
    "def ids(params, tp):\n",
    "    \n",
    "    env = IdsEnv(images_per_episode=1)\n",
    "    if tp == 'DQN':\n",
    "        model = deepq.DQN(\n",
    "            CustomPolicy, \n",
    "            env, \n",
    "            policy_kwargs=dict(dueling=params[0], layers=[128,128]),\n",
    "            double_q=params[1],\n",
    "            verbose=1, \n",
    "            learning_rate=0.00025,\n",
    "            buffer_size=1000000,\n",
    "            exploration_fraction=0.1,\n",
    "            exploration_final_eps=0.1,\n",
    "            train_freq=4,\n",
    "            learning_starts=5000,\n",
    "            target_network_update_freq=10000,\n",
    "            gamma=1.0,\n",
    "            param_noise=params[3],\n",
    "            prioritized_replay=params[2],\n",
    "            prioritized_replay_alpha=0.6,\n",
    "            batch_size=32,\n",
    "        )\n",
    "    if tp == 'A2C':\n",
    "        model = A2C(\n",
    "            MlpPolicy, \n",
    "            env, \n",
    "            verbose=1,\n",
    "        )\n",
    "    model.learn(\n",
    "        total_timesteps=int(1.0e5),\n",
    "        log_interval=int(1.0e4),\n",
    "    )\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "    return model\n",
    "\n",
    "times = []\n",
    "models = []\n",
    "\n",
    "for i in range(AMOUNT):\n",
    "    start_time = time.time()\n",
    "    models.append(ids(parameters, TYPE))\n",
    "    print()\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"{TYPE} {i + 1} Training Time:\", duration)\n",
    "    times.append(duration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation done...\n",
      "Accuracy: 97.89093451409929%\n",
      "Precision: 96.17804707527384%\n",
      "Recall/TPR/Sensitivity: 99.58976833976834%\n",
      "FPR: 3.695358269490762%\n",
      "F1 score: 0.9785417901600474\n",
      "\n",
      "[0.9789093451409928, 0.9617804707527383, 0.9958976833976834, 0.036953582694907616, 0.9785417901600474]\n",
      "\n",
      "Total validation done...\n",
      "Accuracy: 97.89093451409929%\n",
      "Precision: 96.17804707527384%\n",
      "Recall/TPR/Sensitivity: 99.58976833976834%\n",
      "FPR: 3.695358269490762%\n",
      "F1 score: 0.9785417901600474\n",
      "/project/to-run-for-results/nsl-kdd/128/models/cicddos2019-top doesn't exist, creating it + subfolders\n",
      "\n",
      "Saving model 1\n"
     ]
    }
   ],
   "source": [
    "# 0 is benign (positive), 1 is malicious (negative) \n",
    "def ids_eval(model):\n",
    "    TP, FP, TN, FN = 0,0,0,0\n",
    "    env = IdsEnv(images_per_episode=1, dataset=(x_test, y_test), random=False)\n",
    "    obs, done = env.reset(), False\n",
    "    try:\n",
    "        while True:\n",
    "            obs, done = env.reset(), False\n",
    "            while not done:\n",
    "                obs, rew, done, info = env.step(model.predict(obs)[0])\n",
    "                label = info['label']\n",
    "                if label == 0 and rew > 0:\n",
    "                    TP += 1\n",
    "                if label == 0 and rew == 0:\n",
    "                    FP += 1\n",
    "                if label == 1 and rew > 0:\n",
    "                    TN += 1\n",
    "                if label == 1 and rew == 0:\n",
    "                    FN += 1\n",
    "\n",
    "    except StopIteration:\n",
    "        accuracy = (float(TP + TN) / (TP + FP + FN + TN)) \n",
    "        precision = (float(TP) / (TP + FP))\n",
    "        try:\n",
    "            recall = (float(TP) / (TP + FN)) # = TPR = Sensitivity\n",
    "        except:\n",
    "            recall = 0\n",
    "        try:\n",
    "            FPR = (float(FP) / (TN + FP)) # 1 - specificity\n",
    "        except:\n",
    "            FPR = 0\n",
    "        try:\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        except:\n",
    "            f1_score = 0\n",
    "        print()\n",
    "        print('validation done...')\n",
    "        print('Accuracy: {0}%'.format(accuracy * 100))\n",
    "        print('Precision: {0}%'.format(precision * 100))\n",
    "        print('Recall/TPR/Sensitivity: {0}%'.format(recall * 100))\n",
    "        print('FPR: {0}%'.format(FPR * 100))\n",
    "        print('F1 score: {0}'.format(f1_score))\n",
    "    return [accuracy, precision, recall, FPR, f1_score]\n",
    "\n",
    "results = []\n",
    "for i, m in enumerate(models):\n",
    "    results.append(ids_eval(m))\n",
    "\n",
    "total_results = []\n",
    "total_results = [0] * len(results[0])\n",
    "accuracies = [result[0] for result in results]\n",
    "\n",
    "for j in range(AMOUNT):\n",
    "    for i in range(len(results[0])):\n",
    "        total_results[i] =  total_results[i] + results[j][i]\n",
    "for i in range(len(results[0])):\n",
    "        total_results[i] =  total_results[i] / AMOUNT\n",
    "        \n",
    "print()    \n",
    "print('Total validation done...')\n",
    "print('Accuracy: {0}%'.format(total_results[0] * 100))\n",
    "print('Precision: {0}%'.format(total_results[1] * 100))\n",
    "print('Recall/TPR/Sensitivity: {0}%'.format(total_results[2] * 100))\n",
    "print('FPR: {0}%'.format(total_results[3] * 100))\n",
    "print('F1 score: {0}'.format(total_results[4]))\n",
    "\n",
    "\n",
    "path = f'/project/normalization-runs/models'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    print(f\"{path} doesn't exist, creating it + subfolders\")\n",
    "    os.makedirs(path)\n",
    "    os.makedirs(f'{path}/{DATASET}')\n",
    "    os.makedirs(f'{path}/{DATASET}/DQN')\n",
    "    os.makedirs(f'{path}/{DATASET}/A2C')\n",
    "    \n",
    "if not os.path.exists(f'{path}/{DATASET}'):\n",
    "    print(f\"{path}/{DATASET} doesn't exist, creating it + subfolders\")\n",
    "    os.makedirs(f'{path}/{DATASET}')\n",
    "    os.makedirs(f'{path}/{DATASET}/DQN')\n",
    "    os.makedirs(f'{path}/{DATASET}/A2C')\n",
    "\n",
    "for i in range(AMOUNT):\n",
    "    print()\n",
    "    print(f'Saving model {i + 1} to {path}/{DATASET}/{TYPE}/{NAME}_{TYPE}_{i + 1}.pkl')\n",
    "    models[i].save(f'{path}/{DATASET}/{TYPE}/{NAME}_{TYPE}_{i + 1}.pkl')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

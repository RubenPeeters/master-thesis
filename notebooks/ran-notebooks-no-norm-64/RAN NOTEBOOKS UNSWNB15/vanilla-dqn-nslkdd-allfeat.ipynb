{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'baselines' already exists and is not an empty directory.\n",
      "/project/to-run-for-results/unsw-nb15/baselines\n",
      "Obtaining file:///project/to-run-for-results/unsw-nb15/baselines\n",
      "Requirement already satisfied: gym<0.16.0,>=0.15.4 in /opt/conda/lib/python3.7/site-packages (from baselines==0.1.6) (0.15.7)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from baselines==0.1.6) (1.4.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from baselines==0.1.6) (4.46.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from baselines==0.1.6) (0.15.1)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.7/site-packages (from baselines==0.1.6) (1.2.2)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from baselines==0.1.6) (7.1.2)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from baselines==0.1.6) (4.5.5.64)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.5.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/conda/lib/python3.7/site-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.18.5)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.16.0,>=0.15.4->baselines==0.1.6) (0.18.2)\n",
      "Installing collected packages: baselines\n",
      "  Attempting uninstall: baselines\n",
      "    Found existing installation: baselines 0.1.6\n",
      "    Uninstalling baselines-0.1.6:\n",
      "      Successfully uninstalled baselines-0.1.6\n",
      "  Running setup.py develop for baselines\n",
      "Successfully installed baselines\n",
      "Requirement already satisfied: stable-baselines in /opt/conda/lib/python3.7/site-packages (2.10.2)\n",
      "Requirement already satisfied: cloudpickle>=0.5.5 in /opt/conda/lib/python3.7/site-packages (from stable-baselines) (1.2.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from stable-baselines) (1.0.4)\n",
      "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /opt/conda/lib/python3.7/site-packages (from stable-baselines) (0.15.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from stable-baselines) (0.15.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from stable-baselines) (3.2.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from stable-baselines) (1.4.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from stable-baselines) (1.18.5)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from stable-baselines) (4.5.5.64)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->stable-baselines) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->stable-baselines) (2020.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gym[atari,classic_control]>=0.11->stable-baselines) (1.15.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from gym[atari,classic_control]>=0.11->stable-baselines) (1.5.0)\n",
      "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /opt/conda/lib/python3.7/site-packages (from gym[atari,classic_control]>=0.11->stable-baselines) (0.2.9)\n",
      "Requirement already satisfied: Pillow; extra == \"atari\" in /opt/conda/lib/python3.7/site-packages (from gym[atari,classic_control]>=0.11->stable-baselines) (7.1.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->stable-baselines) (0.10.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines) (0.18.2)\n",
      "Requirement already up-to-date: tensorflow==1.14.0 in /opt/conda/lib/python3.7/site-packages (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<1.15.0,>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.29.0)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (3.11.4)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.2.2)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (47.1.1.post20200529)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.6.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.1.0)\n",
      "Requirement already up-to-date: tensorflow-gpu==1.14.0 in /opt/conda/lib/python3.7/site-packages (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<1.15.0,>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (3.11.4)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu==1.14.0) (1.29.0)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.2.2)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (47.1.1.post20200529)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (1.6.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.1.0)\n",
      "Requirement already satisfied: gym in /opt/conda/lib/python3.7/site-packages (0.15.7)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/conda/lib/python3.7/site-packages (from gym) (1.18.5)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym) (1.2.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gym) (1.15.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from gym) (1.4.1)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.7/site-packages (8.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.7/site-packages (from pyarrow) (1.18.5)\n",
      "\u001b[33mWARNING: Skipping tensorboard-plugin-wit as it is not installed.\u001b[0m\n",
      "/project/to-run-for-results/unsw-nb15\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/openai/baselines.git\n",
    "%cd baselines\n",
    "!pip install -e .\n",
    "!pip install stable-baselines\n",
    "!pip install --upgrade tensorflow==1.14.0\n",
    "!pip install --upgrade tensorflow-gpu==1.14.0\n",
    "!pip install gym\n",
    "!pip install pyarrow\n",
    "!pip uninstall --yes tensorboard-plugin-wit\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/stable_baselines/__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n",
      "  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 7833508316373107591,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 716499741290373344\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 11625465096567502219\n",
       " physical_device_desc: \"device: XLA_GPU device\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from datetime import datetime\n",
    "\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "\n",
    "from stable_baselines import deepq\n",
    "from stable_baselines import bench\n",
    "from stable_baselines import logger\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dur</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>sload</th>\n",
       "      <th>dload</th>\n",
       "      <th>...</th>\n",
       "      <th>service_snmp</th>\n",
       "      <th>service_ssh</th>\n",
       "      <th>service_ssl</th>\n",
       "      <th>state_ACC</th>\n",
       "      <th>state_CLO</th>\n",
       "      <th>state_CON</th>\n",
       "      <th>state_FIN</th>\n",
       "      <th>state_INT</th>\n",
       "      <th>state_REQ</th>\n",
       "      <th>state_RST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>496</td>\n",
       "      <td>0</td>\n",
       "      <td>90909.09375</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>180363632.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1762</td>\n",
       "      <td>0</td>\n",
       "      <td>125000.00000</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>881000000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000005</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1068</td>\n",
       "      <td>0</td>\n",
       "      <td>200000.00000</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>854400000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>900</td>\n",
       "      <td>0</td>\n",
       "      <td>166666.65625</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>600000000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000010</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2126</td>\n",
       "      <td>0</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>850400000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 192 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dur  spkts  dpkts  sbytes  dbytes          rate  sttl  dttl  \\\n",
       "0  0.000011      2      0     496       0   90909.09375   254     0   \n",
       "1  0.000008      2      0    1762       0  125000.00000   254     0   \n",
       "2  0.000005      2      0    1068       0  200000.00000   254     0   \n",
       "3  0.000006      2      0     900       0  166666.65625   254     0   \n",
       "4  0.000010      2      0    2126       0  100000.00000   254     0   \n",
       "\n",
       "         sload  dload  ...  service_snmp  service_ssh  service_ssl  state_ACC  \\\n",
       "0  180363632.0    0.0  ...             0            0            0          0   \n",
       "1  881000000.0    0.0  ...             0            0            0          0   \n",
       "2  854400000.0    0.0  ...             0            0            0          0   \n",
       "3  600000000.0    0.0  ...             0            0            0          0   \n",
       "4  850400000.0    0.0  ...             0            0            0          0   \n",
       "\n",
       "   state_CLO  state_CON  state_FIN  state_INT  state_REQ  state_RST  \n",
       "0          0          0          0          1          0          0  \n",
       "1          0          0          0          1          0          0  \n",
       "2          0          0          0          1          0          0  \n",
       "3          0          0          0          1          0          0  \n",
       "4          0          0          0          1          0          0  \n",
       "\n",
       "[5 rows x 192 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nslkdd_train = pd.read_feather(\"/project/datasets/clean-ids-collection/unsw-nb15/clean/designated-train-test-sets/UNSW_NB15_training-set.feather\")\n",
    "nslkdd_test = pd.read_feather(\"/project/datasets/clean-ids-collection/unsw-nb15/clean/designated-train-test-sets/UNSW_NB15_testing-set.feather\")\n",
    "\n",
    "nslkdd_train = pd.get_dummies(nslkdd_train,columns=['proto' ,'service', 'state'])\n",
    "nslkdd_test = pd.get_dummies(nslkdd_test, columns=['proto' ,'service', 'state'])\n",
    "nslkdd_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53946 entries, 0 to 53945\n",
      "Columns: 192 entries, dur to state_RST\n",
      "dtypes: bool(3), category(1), float32(11), int16(11), int32(3), int64(2), int8(10), uint8(151)\n",
      "memory usage: 13.3 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dur</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>sload</th>\n",
       "      <th>dload</th>\n",
       "      <th>...</th>\n",
       "      <th>service_snmp</th>\n",
       "      <th>service_ssh</th>\n",
       "      <th>service_ssl</th>\n",
       "      <th>state_ACC</th>\n",
       "      <th>state_CLO</th>\n",
       "      <th>state_CON</th>\n",
       "      <th>state_FIN</th>\n",
       "      <th>state_INT</th>\n",
       "      <th>state_REQ</th>\n",
       "      <th>state_RST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>53946.000000</td>\n",
       "      <td>53946.000000</td>\n",
       "      <td>53946.000000</td>\n",
       "      <td>5.394600e+04</td>\n",
       "      <td>5.394600e+04</td>\n",
       "      <td>53946.000000</td>\n",
       "      <td>53946.000000</td>\n",
       "      <td>53946.000000</td>\n",
       "      <td>5.394600e+04</td>\n",
       "      <td>5.394600e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>53946.000000</td>\n",
       "      <td>53946.000000</td>\n",
       "      <td>53946.000000</td>\n",
       "      <td>53946.000000</td>\n",
       "      <td>53946.000000</td>\n",
       "      <td>53946.000000</td>\n",
       "      <td>53946.000000</td>\n",
       "      <td>53946.000000</td>\n",
       "      <td>53946.000000</td>\n",
       "      <td>53946.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.022188</td>\n",
       "      <td>25.869759</td>\n",
       "      <td>25.967060</td>\n",
       "      <td>1.131112e+04</td>\n",
       "      <td>1.985872e+04</td>\n",
       "      <td>36171.152344</td>\n",
       "      <td>150.903681</td>\n",
       "      <td>136.375728</td>\n",
       "      <td>4.417149e+07</td>\n",
       "      <td>9.582494e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.003782</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.124143</td>\n",
       "      <td>0.691859</td>\n",
       "      <td>0.161495</td>\n",
       "      <td>0.022393</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.918624</td>\n",
       "      <td>161.154040</td>\n",
       "      <td>141.707459</td>\n",
       "      <td>2.072230e+05</td>\n",
       "      <td>1.865964e+05</td>\n",
       "      <td>112012.312500</td>\n",
       "      <td>106.750997</td>\n",
       "      <td>117.269586</td>\n",
       "      <td>1.947156e+08</td>\n",
       "      <td>2.900805e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021088</td>\n",
       "      <td>0.061379</td>\n",
       "      <td>0.023576</td>\n",
       "      <td>0.008611</td>\n",
       "      <td>0.004305</td>\n",
       "      <td>0.329747</td>\n",
       "      <td>0.461730</td>\n",
       "      <td>0.367990</td>\n",
       "      <td>0.147959</td>\n",
       "      <td>0.004305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.400000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.004238</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.320000e+02</td>\n",
       "      <td>1.780000e+02</td>\n",
       "      <td>24.097402</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>8.323655e+03</td>\n",
       "      <td>1.787040e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.388905</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000e+02</td>\n",
       "      <td>3.540000e+02</td>\n",
       "      <td>65.447559</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>4.005692e+04</td>\n",
       "      <td>5.700335e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.937146</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>2.158000e+03</td>\n",
       "      <td>1.824000e+03</td>\n",
       "      <td>3289.241089</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>9.414949e+05</td>\n",
       "      <td>2.485340e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>59.999989</td>\n",
       "      <td>10646.000000</td>\n",
       "      <td>11018.000000</td>\n",
       "      <td>1.435577e+07</td>\n",
       "      <td>1.465753e+07</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>5.268000e+09</td>\n",
       "      <td>2.082111e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                dur         spkts         dpkts        sbytes        dbytes  \\\n",
       "count  53946.000000  53946.000000  53946.000000  5.394600e+04  5.394600e+04   \n",
       "mean       1.022188     25.869759     25.967060  1.131112e+04  1.985872e+04   \n",
       "std        3.918624    161.154040    141.707459  2.072230e+05  1.865964e+05   \n",
       "min        0.000000      1.000000      0.000000  2.400000e+01  0.000000e+00   \n",
       "25%        0.004238      4.000000      2.000000  5.320000e+02  1.780000e+02   \n",
       "50%        0.388905     10.000000      8.000000  9.000000e+02  3.540000e+02   \n",
       "75%        0.937146     16.000000     18.000000  2.158000e+03  1.824000e+03   \n",
       "max       59.999989  10646.000000  11018.000000  1.435577e+07  1.465753e+07   \n",
       "\n",
       "                 rate          sttl          dttl         sload         dload  \\\n",
       "count    53946.000000  53946.000000  53946.000000  5.394600e+04  5.394600e+04   \n",
       "mean     36171.152344    150.903681    136.375728  4.417149e+07  9.582494e+05   \n",
       "std     112012.312500    106.750997    117.269586  1.947156e+08  2.900805e+06   \n",
       "min          0.000000      0.000000      0.000000  0.000000e+00  0.000000e+00   \n",
       "25%         24.097402     31.000000     29.000000  8.323655e+03  1.787040e+03   \n",
       "50%         65.447559    254.000000    252.000000  4.005692e+04  5.700335e+03   \n",
       "75%       3289.241089    254.000000    252.000000  9.414949e+05  2.485340e+05   \n",
       "max    1000000.000000    255.000000    253.000000  5.268000e+09  2.082111e+07   \n",
       "\n",
       "       ...  service_snmp   service_ssh   service_ssl     state_ACC  \\\n",
       "count  ...  53946.000000  53946.000000  53946.000000  53946.000000   \n",
       "mean   ...      0.000445      0.003782      0.000556      0.000074   \n",
       "std    ...      0.021088      0.061379      0.023576      0.008611   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "max    ...      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "          state_CLO     state_CON     state_FIN     state_INT     state_REQ  \\\n",
       "count  53946.000000  53946.000000  53946.000000  53946.000000  53946.000000   \n",
       "mean       0.000019      0.124143      0.691859      0.161495      0.022393   \n",
       "std        0.004305      0.329747      0.461730      0.367990      0.147959   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      1.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      1.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "          state_RST  \n",
       "count  53946.000000  \n",
       "mean       0.000019  \n",
       "std        0.004305  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 188 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nslkdd_train.info()\n",
    "nslkdd_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing done by Laurens D'Hooge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "- Build an agent to classify network flow automatically\n",
    "- Feed a packet that gets classified\n",
    "- Want the classification to be equal to the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balancing_dataset_sampler_df(df, train_frac=0.2, val_frac=0.1, test_frac=0.7):\n",
    "    col = df.columns[-1]\n",
    "    print(col)\n",
    "    cols = df.columns[:-1]\n",
    "    print(cols)\n",
    "    vc = df[col].value_counts()\n",
    "    print(vc)\n",
    "    n = vc.iloc[-1]\n",
    "    print(n)\n",
    "    m = vc.iloc[0]\n",
    "    print(m)\n",
    "    print(int(m-n))\n",
    "    initial_cut = df.loc[df[col] == vc.index[0]].sample(n=int(m-n), replace=False)\n",
    "    print(initial_cut.index)\n",
    "    df = df.drop(index=initial_cut.index)\n",
    "    vc = df[col].value_counts()\n",
    "    print(vc)\n",
    "    print(int(n*train_frac))\n",
    "    train_df = df.groupby(col).apply(lambda x: x.sample(n=int(n*train_frac), replace=False))\n",
    "    train_df.reset_index(level=0, inplace=True, drop=True)\n",
    "    df = df.drop(index=train_df.index)\n",
    "\n",
    "    validation_df = df.groupby(col).apply(lambda x: x.sample(n=int(n*val_frac), replace=False))\n",
    "    validation_df.reset_index(level=0, inplace=True, drop=True)\n",
    "    df = df.drop(index=validation_df.index)\n",
    "\n",
    "    test_df = df.groupby(col).apply(lambda x: x.sample(n=int(n*test_frac), replace=False))\n",
    "    test_df.reset_index(level=0, inplace=True, drop=True)\n",
    "    df = df.drop(index=test_df.index)\n",
    "\n",
    "    return train_df[cols], train_df[col], validation_df[cols], validation_df[col], test_df[cols], test_df[col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nslkdd_split_df(train_df, test_df):\n",
    "    train_col = train_df.columns[-1]\n",
    "    print(train_col)\n",
    "    train_cols = train_df.columns[:-1]\n",
    "    print(train_cols)\n",
    "    test_col = test_df.columns[-1]\n",
    "    print(test_col)\n",
    "    test_cols = test_df.columns[:-1]\n",
    "    print(test_cols)\n",
    "\n",
    "    return train_df[train_cols], train_df[train_col], test_df[test_cols], test_df[test_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal            34200\n",
      "exploits           7126\n",
      "fuzzers            4394\n",
      "generic            3640\n",
      "reconnaissance     2469\n",
      "dos                1315\n",
      "shellcode           377\n",
      "analysis            312\n",
      "backdoor             70\n",
      "worms                43\n",
      "Name: attack_cat, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(nslkdd_train['attack_cat'].value_counts())\n",
    "nslkdd_train.drop('label', inplace=True, axis=1)\n",
    "nslkdd_train['attack_cat'] = nslkdd_train['attack_cat'].astype('object')\n",
    "atk_idx = nslkdd_train.loc[nslkdd_train['attack_cat'] != \"normal\"].index\n",
    "nslkdd_train.loc[atk_idx, 'attack_cat'] = 1.0\n",
    "nslkdd_train.loc[nslkdd_train.index.difference(atk_idx), 'attack_cat'] = 0.0\n",
    "nslkdd_train['attack_cat'] = nslkdd_train['attack_cat'].astype(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal            51815\n",
      "exploits          18721\n",
      "fuzzers           14889\n",
      "reconnaissance     6422\n",
      "generic            3924\n",
      "dos                3004\n",
      "shellcode          1089\n",
      "analysis            542\n",
      "backdoor            513\n",
      "worms               121\n",
      "Name: attack_cat, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(nslkdd_test['attack_cat'].value_counts())\n",
    "nslkdd_test.drop('label', inplace=True, axis=1)\n",
    "nslkdd_test['attack_cat'] = nslkdd_test['attack_cat'].astype('object')\n",
    "atk_idx = nslkdd_test.loc[nslkdd_test['attack_cat'] != \"normal\"].index\n",
    "nslkdd_test.loc[atk_idx, 'attack_cat'] = 1.0\n",
    "nslkdd_test.loc[nslkdd_test.index.difference(atk_idx), 'attack_cat'] = 0.0\n",
    "nslkdd_test['attack_cat'] = nslkdd_test['attack_cat'].astype(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proto_icmp already not part of train set, skipping\n",
      "proto_rtp already not part of train set, skipping\n",
      "state_ACC already not part of test set, skipping\n",
      "state_CLO already not part of test set, skipping\n",
      "state_ECO already not part of train set, skipping\n",
      "state_PAR already not part of train set, skipping\n",
      "state_URN already not part of train set, skipping\n",
      "state_no already not part of train set, skipping\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53946 entries, 0 to 53945\n",
      "Columns: 189 entries, dur to state_RST\n",
      "dtypes: bool(2), float32(12), int16(11), int32(3), int64(2), int8(10), uint8(149)\n",
      "memory usage: 13.3 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101040 entries, 0 to 101039\n",
      "Columns: 189 entries, dur to state_RST\n",
      "dtypes: bool(2), float32(12), int16(11), int32(3), int64(2), int8(10), uint8(149)\n",
      "memory usage: 25.0 MB\n"
     ]
    }
   ],
   "source": [
    "extra_removables = nslkdd_test.columns ^ nslkdd_train.columns\n",
    "for to_remove in extra_removables:\n",
    "            try:\n",
    "                nslkdd_test.drop(to_remove, inplace=True, axis=1)\n",
    "            except:\n",
    "                print(f\"{to_remove} already not part of test set, skipping\")\n",
    "            try:\n",
    "                nslkdd_train.drop(to_remove, inplace=True, axis=1)\n",
    "            except:\n",
    "                print(f\"{to_remove} already not part of train set, skipping\")\n",
    "nslkdd_train.info()\n",
    "nslkdd_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53946 entries, 0 to 53945\n",
      "Columns: 189 entries, dur to state_RST\n",
      "dtypes: bool(2), float32(12), int16(11), int32(3), int64(2), int8(10), uint8(149)\n",
      "memory usage: 13.3 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101040 entries, 0 to 101039\n",
      "Columns: 189 entries, dur to state_RST\n",
      "dtypes: bool(2), float32(12), int16(11), int32(3), int64(2), int8(10), uint8(149)\n",
      "memory usage: 25.0 MB\n"
     ]
    }
   ],
   "source": [
    "extra_removables = nslkdd_test.columns ^ nslkdd_train.columns\n",
    "for to_remove in extra_removables:\n",
    "            try:\n",
    "                nslkdd_test.drop(to_remove, inplace=True, axis=1)\n",
    "            except:\n",
    "                print(f\"{to_remove} already not part of test set, skipping\")\n",
    "            try:\n",
    "                nslkdd_train.drop(to_remove, inplace=True, axis=1)\n",
    "            except:\n",
    "                print(f\"{to_remove} already not part of train set, skipping\")\n",
    "nslkdd_train.info()\n",
    "nslkdd_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_RST\n",
      "Index(['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl',\n",
      "       'sload', 'dload',\n",
      "       ...\n",
      "       'service_pop3', 'service_radius', 'service_smtp', 'service_snmp',\n",
      "       'service_ssh', 'service_ssl', 'state_CON', 'state_FIN', 'state_INT',\n",
      "       'state_REQ'],\n",
      "      dtype='object', length=188)\n",
      "state_RST\n",
      "Index(['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl',\n",
      "       'sload', 'dload',\n",
      "       ...\n",
      "       'service_pop3', 'service_radius', 'service_smtp', 'service_snmp',\n",
      "       'service_ssh', 'service_ssl', 'state_CON', 'state_FIN', 'state_INT',\n",
      "       'state_REQ'],\n",
      "      dtype='object', length=188)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test  = nslkdd_split_df(nslkdd_train, nslkdd_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.to_frame()\n",
    "y_test = y_test.to_frame()\n",
    "\n",
    "# custom keys -> replace by index\n",
    "\n",
    "x_train = x_train.set_index([pd.Index(range (0, len(x_train)))])\n",
    "y_train = y_train.set_index([pd.Index(range (0, len(y_train)))])\n",
    "x_test = x_test.set_index([pd.Index(range (0, len(x_test)))])\n",
    "y_test = y_test.set_index([pd.Index(range (0, len(y_test)))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdsEnv(gym.Env):\n",
    "    def __init__(self, images_per_episode=1, dataset=(x_train, y_train), random=True):\n",
    "        # Actions we can take, classify as malicious or non-malicious (later also the correct attack)\n",
    "        # change to 19 if detectiong all different attacks\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "         # All the features we have, len(important_features) - 1 features and 1 label. Label should not be included\n",
    "        self.observation_space = gym.spaces.Box(low=float('-inf'), high=float('inf'), shape=(len(nslkdd_train.columns) - 1,))\n",
    "        self.images_per_episode = images_per_episode\n",
    "        self.step_count = 0\n",
    "\n",
    "        self.x, self.y = dataset\n",
    "        self.random = random\n",
    "        self.dataset_idx = 0\n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = int(action == self.expected_action)\n",
    "        current_label = self.expected_action\n",
    "        obs = self._next_obs()\n",
    "\n",
    "        self.step_count += 1\n",
    "        if self.step_count >= self.images_per_episode:\n",
    "            done = True\n",
    "\n",
    "        return obs, reward, done, {'label': current_label}\n",
    "\n",
    "    def _next_obs(self):\n",
    "        if self.random:\n",
    "            next_obs_idx = random.randint(0, len(self.x) - 1)\n",
    "            self.expected_action = int(self.y.iloc[next_obs_idx,:])\n",
    "            obs = self.x.iloc[next_obs_idx,:]\n",
    "\n",
    "        else:\n",
    "            obs = self.x.iloc[self.dataset_idx]\n",
    "            self.expected_action = int(self.y.iloc[self.dataset_idx])\n",
    "\n",
    "            self.dataset_idx += 1\n",
    "            if self.dataset_idx >= len(self.x):\n",
    "                raise StopIteration()\n",
    "        return obs\n",
    "    \n",
    "    def reset(self):\n",
    "        self.step_count = 0\n",
    "\n",
    "        obs = self._next_obs()\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train classifier using dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa9f40b3b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa9f40b3b90>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9f4087190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9f4087190>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9f4db0850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9f4db0850>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9ec2ef150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9ec2ef150>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa9e4098990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa9e4098990>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9e40cea90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9e40cea90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9e407da90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9e407da90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9ec02d990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9ec02d990>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa9e4098990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa9e4098990>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9e407de50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9e407de50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9e407de50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9e407de50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9ec04de90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9ec04de90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 91       |\n",
      "| episodes                | 2        |\n",
      "| mean 100 episode reward | 508      |\n",
      "| steps                   | 1000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 82       |\n",
      "| episodes                | 3        |\n",
      "| mean 100 episode reward | 517      |\n",
      "| steps                   | 2000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 73       |\n",
      "| episodes                | 4        |\n",
      "| mean 100 episode reward | 534      |\n",
      "| steps                   | 3000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 63       |\n",
      "| episodes                | 5        |\n",
      "| mean 100 episode reward | 540      |\n",
      "| steps                   | 4000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 55       |\n",
      "| episodes                | 6        |\n",
      "| mean 100 episode reward | 551      |\n",
      "| steps                   | 5000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 46       |\n",
      "| episodes                | 7        |\n",
      "| mean 100 episode reward | 550      |\n",
      "| steps                   | 6000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 37       |\n",
      "| episodes                | 8        |\n",
      "| mean 100 episode reward | 543      |\n",
      "| steps                   | 7000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 27       |\n",
      "| episodes                | 9        |\n",
      "| mean 100 episode reward | 526      |\n",
      "| steps                   | 8000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 18       |\n",
      "| episodes                | 10       |\n",
      "| mean 100 episode reward | 526      |\n",
      "| steps                   | 9000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 11       |\n",
      "| mean 100 episode reward | 528      |\n",
      "| steps                   | 10000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 12       |\n",
      "| mean 100 episode reward | 523      |\n",
      "| steps                   | 11000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 13       |\n",
      "| mean 100 episode reward | 520      |\n",
      "| steps                   | 12000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 14       |\n",
      "| mean 100 episode reward | 523      |\n",
      "| steps                   | 13000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 15       |\n",
      "| mean 100 episode reward | 531      |\n",
      "| steps                   | 14000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 16       |\n",
      "| mean 100 episode reward | 528      |\n",
      "| steps                   | 15000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 17       |\n",
      "| mean 100 episode reward | 528      |\n",
      "| steps                   | 16000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 18       |\n",
      "| mean 100 episode reward | 527      |\n",
      "| steps                   | 17000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 19       |\n",
      "| mean 100 episode reward | 528      |\n",
      "| steps                   | 18000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 20       |\n",
      "| mean 100 episode reward | 527      |\n",
      "| steps                   | 19000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 21       |\n",
      "| mean 100 episode reward | 530      |\n",
      "| steps                   | 20000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 22       |\n",
      "| mean 100 episode reward | 529      |\n",
      "| steps                   | 21000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 23       |\n",
      "| mean 100 episode reward | 530      |\n",
      "| steps                   | 22000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 24       |\n",
      "| mean 100 episode reward | 532      |\n",
      "| steps                   | 23000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 25       |\n",
      "| mean 100 episode reward | 536      |\n",
      "| steps                   | 24000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 26       |\n",
      "| mean 100 episode reward | 539      |\n",
      "| steps                   | 25000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 27       |\n",
      "| mean 100 episode reward | 542      |\n",
      "| steps                   | 26000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 28       |\n",
      "| mean 100 episode reward | 544      |\n",
      "| steps                   | 27000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 29       |\n",
      "| mean 100 episode reward | 546      |\n",
      "| steps                   | 28000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 30       |\n",
      "| mean 100 episode reward | 549      |\n",
      "| steps                   | 29000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 31       |\n",
      "| mean 100 episode reward | 550      |\n",
      "| steps                   | 30000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 32       |\n",
      "| mean 100 episode reward | 553      |\n",
      "| steps                   | 31000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 33       |\n",
      "| mean 100 episode reward | 554      |\n",
      "| steps                   | 32000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 34       |\n",
      "| mean 100 episode reward | 555      |\n",
      "| steps                   | 33000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 35       |\n",
      "| mean 100 episode reward | 556      |\n",
      "| steps                   | 34000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 36       |\n",
      "| mean 100 episode reward | 558      |\n",
      "| steps                   | 35000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 37       |\n",
      "| mean 100 episode reward | 559      |\n",
      "| steps                   | 36000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 38       |\n",
      "| mean 100 episode reward | 560      |\n",
      "| steps                   | 37000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 39       |\n",
      "| mean 100 episode reward | 562      |\n",
      "| steps                   | 38000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 40       |\n",
      "| mean 100 episode reward | 564      |\n",
      "| steps                   | 39000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 41       |\n",
      "| mean 100 episode reward | 565      |\n",
      "| steps                   | 40000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 42       |\n",
      "| mean 100 episode reward | 567      |\n",
      "| steps                   | 41000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 43       |\n",
      "| mean 100 episode reward | 570      |\n",
      "| steps                   | 42000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 44       |\n",
      "| mean 100 episode reward | 571      |\n",
      "| steps                   | 43000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 45       |\n",
      "| mean 100 episode reward | 571      |\n",
      "| steps                   | 44000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 46       |\n",
      "| mean 100 episode reward | 571      |\n",
      "| steps                   | 45000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 47       |\n",
      "| mean 100 episode reward | 571      |\n",
      "| steps                   | 46000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 48       |\n",
      "| mean 100 episode reward | 572      |\n",
      "| steps                   | 47000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 49       |\n",
      "| mean 100 episode reward | 572      |\n",
      "| steps                   | 48000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 50       |\n",
      "| mean 100 episode reward | 574      |\n",
      "| steps                   | 49000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 51       |\n",
      "| mean 100 episode reward | 575      |\n",
      "| steps                   | 50000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 52       |\n",
      "| mean 100 episode reward | 576      |\n",
      "| steps                   | 51000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 53       |\n",
      "| mean 100 episode reward | 576      |\n",
      "| steps                   | 52000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 54       |\n",
      "| mean 100 episode reward | 577      |\n",
      "| steps                   | 53000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 55       |\n",
      "| mean 100 episode reward | 578      |\n",
      "| steps                   | 54000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 56       |\n",
      "| mean 100 episode reward | 578      |\n",
      "| steps                   | 55000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 57       |\n",
      "| mean 100 episode reward | 579      |\n",
      "| steps                   | 56000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 58       |\n",
      "| mean 100 episode reward | 579      |\n",
      "| steps                   | 57000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 59       |\n",
      "| mean 100 episode reward | 580      |\n",
      "| steps                   | 58000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 60       |\n",
      "| mean 100 episode reward | 580      |\n",
      "| steps                   | 59000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 61       |\n",
      "| mean 100 episode reward | 581      |\n",
      "| steps                   | 60000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 62       |\n",
      "| mean 100 episode reward | 581      |\n",
      "| steps                   | 61000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 63       |\n",
      "| mean 100 episode reward | 582      |\n",
      "| steps                   | 62000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 64       |\n",
      "| mean 100 episode reward | 582      |\n",
      "| steps                   | 63000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 65       |\n",
      "| mean 100 episode reward | 582      |\n",
      "| steps                   | 64000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 66       |\n",
      "| mean 100 episode reward | 583      |\n",
      "| steps                   | 65000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 67       |\n",
      "| mean 100 episode reward | 583      |\n",
      "| steps                   | 66000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 68       |\n",
      "| mean 100 episode reward | 584      |\n",
      "| steps                   | 67000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 69       |\n",
      "| mean 100 episode reward | 584      |\n",
      "| steps                   | 68000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 70       |\n",
      "| mean 100 episode reward | 584      |\n",
      "| steps                   | 69000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 71       |\n",
      "| mean 100 episode reward | 585      |\n",
      "| steps                   | 70000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 72       |\n",
      "| mean 100 episode reward | 586      |\n",
      "| steps                   | 71000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 73       |\n",
      "| mean 100 episode reward | 586      |\n",
      "| steps                   | 72000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 74       |\n",
      "| mean 100 episode reward | 586      |\n",
      "| steps                   | 73000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 75       |\n",
      "| mean 100 episode reward | 586      |\n",
      "| steps                   | 74000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 76       |\n",
      "| mean 100 episode reward | 586      |\n",
      "| steps                   | 75000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 77       |\n",
      "| mean 100 episode reward | 586      |\n",
      "| steps                   | 76000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 78       |\n",
      "| mean 100 episode reward | 586      |\n",
      "| steps                   | 77000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 79       |\n",
      "| mean 100 episode reward | 587      |\n",
      "| steps                   | 78000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 80       |\n",
      "| mean 100 episode reward | 586      |\n",
      "| steps                   | 79000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 81       |\n",
      "| mean 100 episode reward | 587      |\n",
      "| steps                   | 80000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 82       |\n",
      "| mean 100 episode reward | 588      |\n",
      "| steps                   | 81000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 83       |\n",
      "| mean 100 episode reward | 587      |\n",
      "| steps                   | 82000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 84       |\n",
      "| mean 100 episode reward | 588      |\n",
      "| steps                   | 83000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 85       |\n",
      "| mean 100 episode reward | 588      |\n",
      "| steps                   | 84000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 86       |\n",
      "| mean 100 episode reward | 588      |\n",
      "| steps                   | 85000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 87       |\n",
      "| mean 100 episode reward | 588      |\n",
      "| steps                   | 86000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 88       |\n",
      "| mean 100 episode reward | 588      |\n",
      "| steps                   | 87000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 89       |\n",
      "| mean 100 episode reward | 588      |\n",
      "| steps                   | 88000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 90       |\n",
      "| mean 100 episode reward | 588      |\n",
      "| steps                   | 89000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 91       |\n",
      "| mean 100 episode reward | 588      |\n",
      "| steps                   | 90000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 92       |\n",
      "| mean 100 episode reward | 590      |\n",
      "| steps                   | 91000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 93       |\n",
      "| mean 100 episode reward | 589      |\n",
      "| steps                   | 92000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 94       |\n",
      "| mean 100 episode reward | 589      |\n",
      "| steps                   | 93000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 95       |\n",
      "| mean 100 episode reward | 590      |\n",
      "| steps                   | 94000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 96       |\n",
      "| mean 100 episode reward | 589      |\n",
      "| steps                   | 95000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 97       |\n",
      "| mean 100 episode reward | 590      |\n",
      "| steps                   | 96000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 98       |\n",
      "| mean 100 episode reward | 590      |\n",
      "| steps                   | 97000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 99       |\n",
      "| mean 100 episode reward | 590      |\n",
      "| steps                   | 98000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 590      |\n",
      "| steps                   | 99000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 101      |\n",
      "| mean 100 episode reward | 590      |\n",
      "| steps                   | 100000   |\n",
      "--------------------------------------\n",
      "\n",
      "DQN 1 Training Time: 330.7453887462616\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa8337162d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa8337162d0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa833716410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa833716410>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa833716510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa833716510>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9ec07b150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9ec07b150>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa833737d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa833737d90>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa833737610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa833737610>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa83372c750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa83372c750>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa83372cad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa83372cad0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa9ec04d1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa9ec04d1d0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa831e3d150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa831e3d150>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa833730910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa833730910>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa833730d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa833730d10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 91       |\n",
      "| episodes                | 2        |\n",
      "| mean 100 episode reward | 514      |\n",
      "| steps                   | 1000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 82       |\n",
      "| episodes                | 3        |\n",
      "| mean 100 episode reward | 524      |\n",
      "| steps                   | 2000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 73       |\n",
      "| episodes                | 4        |\n",
      "| mean 100 episode reward | 533      |\n",
      "| steps                   | 3000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 63       |\n",
      "| episodes                | 5        |\n",
      "| mean 100 episode reward | 541      |\n",
      "| steps                   | 4000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 55       |\n",
      "| episodes                | 6        |\n",
      "| mean 100 episode reward | 553      |\n",
      "| steps                   | 5000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 46       |\n",
      "| episodes                | 7        |\n",
      "| mean 100 episode reward | 538      |\n",
      "| steps                   | 6000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 37       |\n",
      "| episodes                | 8        |\n",
      "| mean 100 episode reward | 528      |\n",
      "| steps                   | 7000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 27       |\n",
      "| episodes                | 9        |\n",
      "| mean 100 episode reward | 530      |\n",
      "| steps                   | 8000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 18       |\n",
      "| episodes                | 10       |\n",
      "| mean 100 episode reward | 545      |\n",
      "| steps                   | 9000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 11       |\n",
      "| mean 100 episode reward | 549      |\n",
      "| steps                   | 10000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 12       |\n",
      "| mean 100 episode reward | 560      |\n",
      "| steps                   | 11000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 13       |\n",
      "| mean 100 episode reward | 564      |\n",
      "| steps                   | 12000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 14       |\n",
      "| mean 100 episode reward | 560      |\n",
      "| steps                   | 13000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 15       |\n",
      "| mean 100 episode reward | 564      |\n",
      "| steps                   | 14000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 16       |\n",
      "| mean 100 episode reward | 564      |\n",
      "| steps                   | 15000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 17       |\n",
      "| mean 100 episode reward | 570      |\n",
      "| steps                   | 16000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 18       |\n",
      "| mean 100 episode reward | 577      |\n",
      "| steps                   | 17000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 19       |\n",
      "| mean 100 episode reward | 580      |\n",
      "| steps                   | 18000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 20       |\n",
      "| mean 100 episode reward | 584      |\n",
      "| steps                   | 19000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 21       |\n",
      "| mean 100 episode reward | 587      |\n",
      "| steps                   | 20000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 22       |\n",
      "| mean 100 episode reward | 587      |\n",
      "| steps                   | 21000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 23       |\n",
      "| mean 100 episode reward | 591      |\n",
      "| steps                   | 22000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 24       |\n",
      "| mean 100 episode reward | 594      |\n",
      "| steps                   | 23000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 25       |\n",
      "| mean 100 episode reward | 595      |\n",
      "| steps                   | 24000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 26       |\n",
      "| mean 100 episode reward | 591      |\n",
      "| steps                   | 25000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 27       |\n",
      "| mean 100 episode reward | 590      |\n",
      "| steps                   | 26000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 28       |\n",
      "| mean 100 episode reward | 589      |\n",
      "| steps                   | 27000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 29       |\n",
      "| mean 100 episode reward | 592      |\n",
      "| steps                   | 28000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 30       |\n",
      "| mean 100 episode reward | 593      |\n",
      "| steps                   | 29000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 31       |\n",
      "| mean 100 episode reward | 591      |\n",
      "| steps                   | 30000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 32       |\n",
      "| mean 100 episode reward | 588      |\n",
      "| steps                   | 31000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 33       |\n",
      "| mean 100 episode reward | 587      |\n",
      "| steps                   | 32000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 34       |\n",
      "| mean 100 episode reward | 588      |\n",
      "| steps                   | 33000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 35       |\n",
      "| mean 100 episode reward | 588      |\n",
      "| steps                   | 34000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 36       |\n",
      "| mean 100 episode reward | 588      |\n",
      "| steps                   | 35000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 37       |\n",
      "| mean 100 episode reward | 589      |\n",
      "| steps                   | 36000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 38       |\n",
      "| mean 100 episode reward | 590      |\n",
      "| steps                   | 37000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 39       |\n",
      "| mean 100 episode reward | 590      |\n",
      "| steps                   | 38000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 40       |\n",
      "| mean 100 episode reward | 589      |\n",
      "| steps                   | 39000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 41       |\n",
      "| mean 100 episode reward | 591      |\n",
      "| steps                   | 40000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 42       |\n",
      "| mean 100 episode reward | 593      |\n",
      "| steps                   | 41000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 43       |\n",
      "| mean 100 episode reward | 592      |\n",
      "| steps                   | 42000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 44       |\n",
      "| mean 100 episode reward | 590      |\n",
      "| steps                   | 43000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 45       |\n",
      "| mean 100 episode reward | 591      |\n",
      "| steps                   | 44000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 46       |\n",
      "| mean 100 episode reward | 590      |\n",
      "| steps                   | 45000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 47       |\n",
      "| mean 100 episode reward | 590      |\n",
      "| steps                   | 46000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 48       |\n",
      "| mean 100 episode reward | 590      |\n",
      "| steps                   | 47000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 49       |\n",
      "| mean 100 episode reward | 589      |\n",
      "| steps                   | 48000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 50       |\n",
      "| mean 100 episode reward | 590      |\n",
      "| steps                   | 49000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 51       |\n",
      "| mean 100 episode reward | 590      |\n",
      "| steps                   | 50000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 52       |\n",
      "| mean 100 episode reward | 590      |\n",
      "| steps                   | 51000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 53       |\n",
      "| mean 100 episode reward | 590      |\n",
      "| steps                   | 52000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 54       |\n",
      "| mean 100 episode reward | 590      |\n",
      "| steps                   | 53000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 55       |\n",
      "| mean 100 episode reward | 589      |\n",
      "| steps                   | 54000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 56       |\n",
      "| mean 100 episode reward | 590      |\n",
      "| steps                   | 55000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 57       |\n",
      "| mean 100 episode reward | 591      |\n",
      "| steps                   | 56000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 58       |\n",
      "| mean 100 episode reward | 591      |\n",
      "| steps                   | 57000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 59       |\n",
      "| mean 100 episode reward | 591      |\n",
      "| steps                   | 58000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 60       |\n",
      "| mean 100 episode reward | 592      |\n",
      "| steps                   | 59000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 61       |\n",
      "| mean 100 episode reward | 592      |\n",
      "| steps                   | 60000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 62       |\n",
      "| mean 100 episode reward | 593      |\n",
      "| steps                   | 61000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 63       |\n",
      "| mean 100 episode reward | 593      |\n",
      "| steps                   | 62000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 64       |\n",
      "| mean 100 episode reward | 594      |\n",
      "| steps                   | 63000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 65       |\n",
      "| mean 100 episode reward | 595      |\n",
      "| steps                   | 64000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 66       |\n",
      "| mean 100 episode reward | 596      |\n",
      "| steps                   | 65000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 67       |\n",
      "| mean 100 episode reward | 596      |\n",
      "| steps                   | 66000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 68       |\n",
      "| mean 100 episode reward | 595      |\n",
      "| steps                   | 67000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 69       |\n",
      "| mean 100 episode reward | 596      |\n",
      "| steps                   | 68000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 70       |\n",
      "| mean 100 episode reward | 596      |\n",
      "| steps                   | 69000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 71       |\n",
      "| mean 100 episode reward | 596      |\n",
      "| steps                   | 70000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 72       |\n",
      "| mean 100 episode reward | 596      |\n",
      "| steps                   | 71000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 73       |\n",
      "| mean 100 episode reward | 597      |\n",
      "| steps                   | 72000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 74       |\n",
      "| mean 100 episode reward | 596      |\n",
      "| steps                   | 73000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 75       |\n",
      "| mean 100 episode reward | 596      |\n",
      "| steps                   | 74000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 76       |\n",
      "| mean 100 episode reward | 596      |\n",
      "| steps                   | 75000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 77       |\n",
      "| mean 100 episode reward | 597      |\n",
      "| steps                   | 76000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 78       |\n",
      "| mean 100 episode reward | 597      |\n",
      "| steps                   | 77000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 79       |\n",
      "| mean 100 episode reward | 598      |\n",
      "| steps                   | 78000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 80       |\n",
      "| mean 100 episode reward | 598      |\n",
      "| steps                   | 79000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 81       |\n",
      "| mean 100 episode reward | 598      |\n",
      "| steps                   | 80000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 82       |\n",
      "| mean 100 episode reward | 598      |\n",
      "| steps                   | 81000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 83       |\n",
      "| mean 100 episode reward | 598      |\n",
      "| steps                   | 82000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 84       |\n",
      "| mean 100 episode reward | 598      |\n",
      "| steps                   | 83000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 85       |\n",
      "| mean 100 episode reward | 598      |\n",
      "| steps                   | 84000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 86       |\n",
      "| mean 100 episode reward | 598      |\n",
      "| steps                   | 85000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 87       |\n",
      "| mean 100 episode reward | 598      |\n",
      "| steps                   | 86000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 88       |\n",
      "| mean 100 episode reward | 598      |\n",
      "| steps                   | 87000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 89       |\n",
      "| mean 100 episode reward | 599      |\n",
      "| steps                   | 88000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 90       |\n",
      "| mean 100 episode reward | 599      |\n",
      "| steps                   | 89000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 91       |\n",
      "| mean 100 episode reward | 599      |\n",
      "| steps                   | 90000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 92       |\n",
      "| mean 100 episode reward | 599      |\n",
      "| steps                   | 91000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 93       |\n",
      "| mean 100 episode reward | 599      |\n",
      "| steps                   | 92000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 94       |\n",
      "| mean 100 episode reward | 598      |\n",
      "| steps                   | 93000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 95       |\n",
      "| mean 100 episode reward | 599      |\n",
      "| steps                   | 94000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 96       |\n",
      "| mean 100 episode reward | 599      |\n",
      "| steps                   | 95000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 97       |\n",
      "| mean 100 episode reward | 599      |\n",
      "| steps                   | 96000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 98       |\n",
      "| mean 100 episode reward | 599      |\n",
      "| steps                   | 97000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 99       |\n",
      "| mean 100 episode reward | 599      |\n",
      "| steps                   | 98000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 599      |\n",
      "| steps                   | 99000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 101      |\n",
      "| mean 100 episode reward | 598      |\n",
      "| steps                   | 100000   |\n",
      "--------------------------------------\n",
      "\n",
      "DQN 2 Training Time: 330.562716960907\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa7f7fa42d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa7f7fa42d0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7f7fa4250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7f7fa4250>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7f7fa4450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7f7fa4450>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7f7fa4450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7f7fa4450>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa7f6729490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa7f6729490>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7f66ce410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7f66ce410>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7f670f850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7f670f850>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7f66cee50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7f66cee50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa7f667e190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa7f667e190>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7f6745cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7f6745cd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7f67453d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7f67453d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9ec060510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9ec060510>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 91       |\n",
      "| episodes                | 2        |\n",
      "| mean 100 episode reward | 480      |\n",
      "| steps                   | 1000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 82       |\n",
      "| episodes                | 3        |\n",
      "| mean 100 episode reward | 460      |\n",
      "| steps                   | 2000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 73       |\n",
      "| episodes                | 4        |\n",
      "| mean 100 episode reward | 457      |\n",
      "| steps                   | 3000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 63       |\n",
      "| episodes                | 5        |\n",
      "| mean 100 episode reward | 440      |\n",
      "| steps                   | 4000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 55       |\n",
      "| episodes                | 6        |\n",
      "| mean 100 episode reward | 421      |\n",
      "| steps                   | 5000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 46       |\n",
      "| episodes                | 7        |\n",
      "| mean 100 episode reward | 425      |\n",
      "| steps                   | 6000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 37       |\n",
      "| episodes                | 8        |\n",
      "| mean 100 episode reward | 437      |\n",
      "| steps                   | 7000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 27       |\n",
      "| episodes                | 9        |\n",
      "| mean 100 episode reward | 434      |\n",
      "| steps                   | 8000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 18       |\n",
      "| episodes                | 10       |\n",
      "| mean 100 episode reward | 455      |\n",
      "| steps                   | 9000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 11       |\n",
      "| mean 100 episode reward | 479      |\n",
      "| steps                   | 10000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 12       |\n",
      "| mean 100 episode reward | 485      |\n",
      "| steps                   | 11000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 13       |\n",
      "| mean 100 episode reward | 493      |\n",
      "| steps                   | 12000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 14       |\n",
      "| mean 100 episode reward | 492      |\n",
      "| steps                   | 13000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 15       |\n",
      "| mean 100 episode reward | 499      |\n",
      "| steps                   | 14000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 16       |\n",
      "| mean 100 episode reward | 497      |\n",
      "| steps                   | 15000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 17       |\n",
      "| mean 100 episode reward | 496      |\n",
      "| steps                   | 16000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 18       |\n",
      "| mean 100 episode reward | 491      |\n",
      "| steps                   | 17000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 19       |\n",
      "| mean 100 episode reward | 496      |\n",
      "| steps                   | 18000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 20       |\n",
      "| mean 100 episode reward | 499      |\n",
      "| steps                   | 19000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 21       |\n",
      "| mean 100 episode reward | 499      |\n",
      "| steps                   | 20000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 22       |\n",
      "| mean 100 episode reward | 501      |\n",
      "| steps                   | 21000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 23       |\n",
      "| mean 100 episode reward | 506      |\n",
      "| steps                   | 22000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 24       |\n",
      "| mean 100 episode reward | 510      |\n",
      "| steps                   | 23000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 25       |\n",
      "| mean 100 episode reward | 517      |\n",
      "| steps                   | 24000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 26       |\n",
      "| mean 100 episode reward | 523      |\n",
      "| steps                   | 25000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 27       |\n",
      "| mean 100 episode reward | 528      |\n",
      "| steps                   | 26000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 28       |\n",
      "| mean 100 episode reward | 529      |\n",
      "| steps                   | 27000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 29       |\n",
      "| mean 100 episode reward | 526      |\n",
      "| steps                   | 28000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 30       |\n",
      "| mean 100 episode reward | 528      |\n",
      "| steps                   | 29000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 31       |\n",
      "| mean 100 episode reward | 533      |\n",
      "| steps                   | 30000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 32       |\n",
      "| mean 100 episode reward | 537      |\n",
      "| steps                   | 31000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 33       |\n",
      "| mean 100 episode reward | 538      |\n",
      "| steps                   | 32000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 34       |\n",
      "| mean 100 episode reward | 537      |\n",
      "| steps                   | 33000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 35       |\n",
      "| mean 100 episode reward | 539      |\n",
      "| steps                   | 34000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 36       |\n",
      "| mean 100 episode reward | 541      |\n",
      "| steps                   | 35000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 37       |\n",
      "| mean 100 episode reward | 541      |\n",
      "| steps                   | 36000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 38       |\n",
      "| mean 100 episode reward | 543      |\n",
      "| steps                   | 37000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 39       |\n",
      "| mean 100 episode reward | 545      |\n",
      "| steps                   | 38000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 40       |\n",
      "| mean 100 episode reward | 548      |\n",
      "| steps                   | 39000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 41       |\n",
      "| mean 100 episode reward | 548      |\n",
      "| steps                   | 40000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 42       |\n",
      "| mean 100 episode reward | 550      |\n",
      "| steps                   | 41000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 43       |\n",
      "| mean 100 episode reward | 552      |\n",
      "| steps                   | 42000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 44       |\n",
      "| mean 100 episode reward | 553      |\n",
      "| steps                   | 43000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 45       |\n",
      "| mean 100 episode reward | 554      |\n",
      "| steps                   | 44000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 46       |\n",
      "| mean 100 episode reward | 555      |\n",
      "| steps                   | 45000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 47       |\n",
      "| mean 100 episode reward | 556      |\n",
      "| steps                   | 46000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 48       |\n",
      "| mean 100 episode reward | 556      |\n",
      "| steps                   | 47000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 49       |\n",
      "| mean 100 episode reward | 558      |\n",
      "| steps                   | 48000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 50       |\n",
      "| mean 100 episode reward | 558      |\n",
      "| steps                   | 49000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 51       |\n",
      "| mean 100 episode reward | 560      |\n",
      "| steps                   | 50000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 52       |\n",
      "| mean 100 episode reward | 560      |\n",
      "| steps                   | 51000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 53       |\n",
      "| mean 100 episode reward | 561      |\n",
      "| steps                   | 52000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 54       |\n",
      "| mean 100 episode reward | 562      |\n",
      "| steps                   | 53000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 55       |\n",
      "| mean 100 episode reward | 562      |\n",
      "| steps                   | 54000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 56       |\n",
      "| mean 100 episode reward | 563      |\n",
      "| steps                   | 55000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 57       |\n",
      "| mean 100 episode reward | 563      |\n",
      "| steps                   | 56000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 58       |\n",
      "| mean 100 episode reward | 564      |\n",
      "| steps                   | 57000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 59       |\n",
      "| mean 100 episode reward | 564      |\n",
      "| steps                   | 58000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 60       |\n",
      "| mean 100 episode reward | 565      |\n",
      "| steps                   | 59000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 61       |\n",
      "| mean 100 episode reward | 566      |\n",
      "| steps                   | 60000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 62       |\n",
      "| mean 100 episode reward | 566      |\n",
      "| steps                   | 61000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 63       |\n",
      "| mean 100 episode reward | 567      |\n",
      "| steps                   | 62000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 64       |\n",
      "| mean 100 episode reward | 568      |\n",
      "| steps                   | 63000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 65       |\n",
      "| mean 100 episode reward | 568      |\n",
      "| steps                   | 64000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 66       |\n",
      "| mean 100 episode reward | 569      |\n",
      "| steps                   | 65000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 67       |\n",
      "| mean 100 episode reward | 569      |\n",
      "| steps                   | 66000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 68       |\n",
      "| mean 100 episode reward | 570      |\n",
      "| steps                   | 67000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 69       |\n",
      "| mean 100 episode reward | 570      |\n",
      "| steps                   | 68000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 70       |\n",
      "| mean 100 episode reward | 570      |\n",
      "| steps                   | 69000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 71       |\n",
      "| mean 100 episode reward | 571      |\n",
      "| steps                   | 70000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 72       |\n",
      "| mean 100 episode reward | 572      |\n",
      "| steps                   | 71000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 73       |\n",
      "| mean 100 episode reward | 571      |\n",
      "| steps                   | 72000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 74       |\n",
      "| mean 100 episode reward | 572      |\n",
      "| steps                   | 73000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 75       |\n",
      "| mean 100 episode reward | 572      |\n",
      "| steps                   | 74000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 76       |\n",
      "| mean 100 episode reward | 573      |\n",
      "| steps                   | 75000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 77       |\n",
      "| mean 100 episode reward | 573      |\n",
      "| steps                   | 76000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 78       |\n",
      "| mean 100 episode reward | 574      |\n",
      "| steps                   | 77000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 79       |\n",
      "| mean 100 episode reward | 574      |\n",
      "| steps                   | 78000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 80       |\n",
      "| mean 100 episode reward | 574      |\n",
      "| steps                   | 79000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 81       |\n",
      "| mean 100 episode reward | 575      |\n",
      "| steps                   | 80000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 82       |\n",
      "| mean 100 episode reward | 576      |\n",
      "| steps                   | 81000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 83       |\n",
      "| mean 100 episode reward | 577      |\n",
      "| steps                   | 82000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 84       |\n",
      "| mean 100 episode reward | 577      |\n",
      "| steps                   | 83000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 85       |\n",
      "| mean 100 episode reward | 577      |\n",
      "| steps                   | 84000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 86       |\n",
      "| mean 100 episode reward | 577      |\n",
      "| steps                   | 85000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 87       |\n",
      "| mean 100 episode reward | 576      |\n",
      "| steps                   | 86000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 88       |\n",
      "| mean 100 episode reward | 577      |\n",
      "| steps                   | 87000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 89       |\n",
      "| mean 100 episode reward | 578      |\n",
      "| steps                   | 88000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 90       |\n",
      "| mean 100 episode reward | 578      |\n",
      "| steps                   | 89000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 91       |\n",
      "| mean 100 episode reward | 578      |\n",
      "| steps                   | 90000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 92       |\n",
      "| mean 100 episode reward | 579      |\n",
      "| steps                   | 91000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 93       |\n",
      "| mean 100 episode reward | 580      |\n",
      "| steps                   | 92000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 94       |\n",
      "| mean 100 episode reward | 580      |\n",
      "| steps                   | 93000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 95       |\n",
      "| mean 100 episode reward | 581      |\n",
      "| steps                   | 94000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 96       |\n",
      "| mean 100 episode reward | 581      |\n",
      "| steps                   | 95000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 97       |\n",
      "| mean 100 episode reward | 581      |\n",
      "| steps                   | 96000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 98       |\n",
      "| mean 100 episode reward | 580      |\n",
      "| steps                   | 97000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 99       |\n",
      "| mean 100 episode reward | 580      |\n",
      "| steps                   | 98000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 580      |\n",
      "| steps                   | 99000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 101      |\n",
      "| mean 100 episode reward | 580      |\n",
      "| steps                   | 100000   |\n",
      "--------------------------------------\n",
      "\n",
      "DQN 3 Training Time: 329.67772603034973\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa7bc829990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa7bc829990>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7bc831150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7bc831150>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7bc831fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7bc831fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7bc831fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7bc831fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa7bc83b550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa7bc83b550>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7baf0f1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7baf0f1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7bafc9890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7bafc9890>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9f406c390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa9f406c390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa7bafd0c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7fa7bafd0c90>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7baf3e7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7baf3e7d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7baf4f990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7baf4f990>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7baf55810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa7baf55810>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 91       |\n",
      "| episodes                | 2        |\n",
      "| mean 100 episode reward | 459      |\n",
      "| steps                   | 1000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 82       |\n",
      "| episodes                | 3        |\n",
      "| mean 100 episode reward | 442      |\n",
      "| steps                   | 2000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 73       |\n",
      "| episodes                | 4        |\n",
      "| mean 100 episode reward | 426      |\n",
      "| steps                   | 3000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 63       |\n",
      "| episodes                | 5        |\n",
      "| mean 100 episode reward | 404      |\n",
      "| steps                   | 4000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 55       |\n",
      "| episodes                | 6        |\n",
      "| mean 100 episode reward | 385      |\n",
      "| steps                   | 5000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 46       |\n",
      "| episodes                | 7        |\n",
      "| mean 100 episode reward | 414      |\n",
      "| steps                   | 6000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 37       |\n",
      "| episodes                | 8        |\n",
      "| mean 100 episode reward | 424      |\n",
      "| steps                   | 7000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 27       |\n",
      "| episodes                | 9        |\n",
      "| mean 100 episode reward | 432      |\n",
      "| steps                   | 8000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 18       |\n",
      "| episodes                | 10       |\n",
      "| mean 100 episode reward | 444      |\n",
      "| steps                   | 9000     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 11       |\n",
      "| mean 100 episode reward | 453      |\n",
      "| steps                   | 10000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 12       |\n",
      "| mean 100 episode reward | 457      |\n",
      "| steps                   | 11000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 13       |\n",
      "| mean 100 episode reward | 466      |\n",
      "| steps                   | 12000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 14       |\n",
      "| mean 100 episode reward | 463      |\n",
      "| steps                   | 13000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 15       |\n",
      "| mean 100 episode reward | 465      |\n",
      "| steps                   | 14000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 16       |\n",
      "| mean 100 episode reward | 463      |\n",
      "| steps                   | 15000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 17       |\n",
      "| mean 100 episode reward | 469      |\n",
      "| steps                   | 16000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 18       |\n",
      "| mean 100 episode reward | 470      |\n",
      "| steps                   | 17000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 19       |\n",
      "| mean 100 episode reward | 470      |\n",
      "| steps                   | 18000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 20       |\n",
      "| mean 100 episode reward | 468      |\n",
      "| steps                   | 19000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 21       |\n",
      "| mean 100 episode reward | 466      |\n",
      "| steps                   | 20000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 22       |\n",
      "| mean 100 episode reward | 465      |\n",
      "| steps                   | 21000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 23       |\n",
      "| mean 100 episode reward | 467      |\n",
      "| steps                   | 22000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 24       |\n",
      "| mean 100 episode reward | 467      |\n",
      "| steps                   | 23000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 25       |\n",
      "| mean 100 episode reward | 469      |\n",
      "| steps                   | 24000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 26       |\n",
      "| mean 100 episode reward | 468      |\n",
      "| steps                   | 25000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 27       |\n",
      "| mean 100 episode reward | 469      |\n",
      "| steps                   | 26000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 28       |\n",
      "| mean 100 episode reward | 466      |\n",
      "| steps                   | 27000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 29       |\n",
      "| mean 100 episode reward | 467      |\n",
      "| steps                   | 28000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 30       |\n",
      "| mean 100 episode reward | 463      |\n",
      "| steps                   | 29000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 31       |\n",
      "| mean 100 episode reward | 461      |\n",
      "| steps                   | 30000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 32       |\n",
      "| mean 100 episode reward | 460      |\n",
      "| steps                   | 31000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 33       |\n",
      "| mean 100 episode reward | 459      |\n",
      "| steps                   | 32000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 34       |\n",
      "| mean 100 episode reward | 462      |\n",
      "| steps                   | 33000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 35       |\n",
      "| mean 100 episode reward | 464      |\n",
      "| steps                   | 34000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 36       |\n",
      "| mean 100 episode reward | 466      |\n",
      "| steps                   | 35000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 37       |\n",
      "| mean 100 episode reward | 467      |\n",
      "| steps                   | 36000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 38       |\n",
      "| mean 100 episode reward | 470      |\n",
      "| steps                   | 37000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 39       |\n",
      "| mean 100 episode reward | 473      |\n",
      "| steps                   | 38000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 40       |\n",
      "| mean 100 episode reward | 475      |\n",
      "| steps                   | 39000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 41       |\n",
      "| mean 100 episode reward | 476      |\n",
      "| steps                   | 40000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 42       |\n",
      "| mean 100 episode reward | 477      |\n",
      "| steps                   | 41000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 43       |\n",
      "| mean 100 episode reward | 479      |\n",
      "| steps                   | 42000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 44       |\n",
      "| mean 100 episode reward | 481      |\n",
      "| steps                   | 43000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 45       |\n",
      "| mean 100 episode reward | 481      |\n",
      "| steps                   | 44000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 46       |\n",
      "| mean 100 episode reward | 482      |\n",
      "| steps                   | 45000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 47       |\n",
      "| mean 100 episode reward | 483      |\n",
      "| steps                   | 46000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 48       |\n",
      "| mean 100 episode reward | 484      |\n",
      "| steps                   | 47000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 49       |\n",
      "| mean 100 episode reward | 485      |\n",
      "| steps                   | 48000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 50       |\n",
      "| mean 100 episode reward | 486      |\n",
      "| steps                   | 49000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 51       |\n",
      "| mean 100 episode reward | 488      |\n",
      "| steps                   | 50000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 52       |\n",
      "| mean 100 episode reward | 488      |\n",
      "| steps                   | 51000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 53       |\n",
      "| mean 100 episode reward | 489      |\n",
      "| steps                   | 52000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 54       |\n",
      "| mean 100 episode reward | 489      |\n",
      "| steps                   | 53000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 55       |\n",
      "| mean 100 episode reward | 490      |\n",
      "| steps                   | 54000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 56       |\n",
      "| mean 100 episode reward | 491      |\n",
      "| steps                   | 55000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 57       |\n",
      "| mean 100 episode reward | 490      |\n",
      "| steps                   | 56000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 58       |\n",
      "| mean 100 episode reward | 490      |\n",
      "| steps                   | 57000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 59       |\n",
      "| mean 100 episode reward | 489      |\n",
      "| steps                   | 58000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 60       |\n",
      "| mean 100 episode reward | 488      |\n",
      "| steps                   | 59000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 61       |\n",
      "| mean 100 episode reward | 487      |\n",
      "| steps                   | 60000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 62       |\n",
      "| mean 100 episode reward | 485      |\n",
      "| steps                   | 61000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 63       |\n",
      "| mean 100 episode reward | 485      |\n",
      "| steps                   | 62000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 64       |\n",
      "| mean 100 episode reward | 485      |\n",
      "| steps                   | 63000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 65       |\n",
      "| mean 100 episode reward | 485      |\n",
      "| steps                   | 64000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 66       |\n",
      "| mean 100 episode reward | 484      |\n",
      "| steps                   | 65000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 67       |\n",
      "| mean 100 episode reward | 485      |\n",
      "| steps                   | 66000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 68       |\n",
      "| mean 100 episode reward | 485      |\n",
      "| steps                   | 67000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 69       |\n",
      "| mean 100 episode reward | 485      |\n",
      "| steps                   | 68000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 70       |\n",
      "| mean 100 episode reward | 485      |\n",
      "| steps                   | 69000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 71       |\n",
      "| mean 100 episode reward | 485      |\n",
      "| steps                   | 70000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 72       |\n",
      "| mean 100 episode reward | 484      |\n",
      "| steps                   | 71000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 73       |\n",
      "| mean 100 episode reward | 484      |\n",
      "| steps                   | 72000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 74       |\n",
      "| mean 100 episode reward | 484      |\n",
      "| steps                   | 73000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 75       |\n",
      "| mean 100 episode reward | 484      |\n",
      "| steps                   | 74000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 76       |\n",
      "| mean 100 episode reward | 484      |\n",
      "| steps                   | 75000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 77       |\n",
      "| mean 100 episode reward | 484      |\n",
      "| steps                   | 76000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 78       |\n",
      "| mean 100 episode reward | 483      |\n",
      "| steps                   | 77000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 79       |\n",
      "| mean 100 episode reward | 482      |\n",
      "| steps                   | 78000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 80       |\n",
      "| mean 100 episode reward | 483      |\n",
      "| steps                   | 79000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 81       |\n",
      "| mean 100 episode reward | 482      |\n",
      "| steps                   | 80000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 82       |\n",
      "| mean 100 episode reward | 482      |\n",
      "| steps                   | 81000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 83       |\n",
      "| mean 100 episode reward | 482      |\n",
      "| steps                   | 82000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 84       |\n",
      "| mean 100 episode reward | 482      |\n",
      "| steps                   | 83000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 85       |\n",
      "| mean 100 episode reward | 482      |\n",
      "| steps                   | 84000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 86       |\n",
      "| mean 100 episode reward | 482      |\n",
      "| steps                   | 85000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 87       |\n",
      "| mean 100 episode reward | 482      |\n",
      "| steps                   | 86000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 88       |\n",
      "| mean 100 episode reward | 482      |\n",
      "| steps                   | 87000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 89       |\n",
      "| mean 100 episode reward | 482      |\n",
      "| steps                   | 88000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 90       |\n",
      "| mean 100 episode reward | 483      |\n",
      "| steps                   | 89000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 91       |\n",
      "| mean 100 episode reward | 484      |\n",
      "| steps                   | 90000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 92       |\n",
      "| mean 100 episode reward | 484      |\n",
      "| steps                   | 91000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 93       |\n",
      "| mean 100 episode reward | 485      |\n",
      "| steps                   | 92000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 94       |\n",
      "| mean 100 episode reward | 484      |\n",
      "| steps                   | 93000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 95       |\n",
      "| mean 100 episode reward | 485      |\n",
      "| steps                   | 94000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 96       |\n",
      "| mean 100 episode reward | 485      |\n",
      "| steps                   | 95000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 97       |\n",
      "| mean 100 episode reward | 486      |\n",
      "| steps                   | 96000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 98       |\n",
      "| mean 100 episode reward | 486      |\n",
      "| steps                   | 97000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 99       |\n",
      "| mean 100 episode reward | 487      |\n",
      "| steps                   | 98000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 488      |\n",
      "| steps                   | 99000    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 101      |\n",
      "| mean 100 episode reward | 488      |\n",
      "| steps                   | 100000   |\n",
      "--------------------------------------\n",
      "\n",
      "DQN 4 Training Time: 330.38319540023804\n"
     ]
    }
   ],
   "source": [
    "def ids_ddqn():\n",
    "    \n",
    "    env = IdsEnv(images_per_episode=1000)\n",
    "\n",
    "    model = deepq.DQN(\n",
    "        MlpPolicy, \n",
    "        env, \n",
    "        policy_kwargs=dict(dueling=False),\n",
    "        double_q=False,\n",
    "        verbose=1, \n",
    "        learning_rate=0.00025,\n",
    "        buffer_size=1000000,\n",
    "        exploration_fraction=0.1,\n",
    "        exploration_final_eps=0.1,\n",
    "        train_freq=4,\n",
    "        learning_starts=5000,\n",
    "        target_network_update_freq=10000,\n",
    "        gamma=1.0,\n",
    "        prioritized_replay=False,\n",
    "        prioritized_replay_alpha=0.6,\n",
    "        batch_size=32,\n",
    "    )\n",
    "    model.learn(\n",
    "        total_timesteps=int(1.0e5),\n",
    "        log_interval=1,\n",
    "    )\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "    return model\n",
    "\n",
    "start_time = time.time()\n",
    "ddqn_model_1 = ids_ddqn()\n",
    "print()\n",
    "print(\"DQN 1 Training Time:\", time.time() - start_time)\n",
    "start_time = time.time()\n",
    "ddqn_model_2 = ids_ddqn()\n",
    "print()\n",
    "print(\"DQN 2 Training Time:\", time.time() - start_time)\n",
    "start_time = time.time()\n",
    "ddqn_model_3 = ids_ddqn()\n",
    "print()\n",
    "print(\"DQN 3 Training Time:\", time.time() - start_time)\n",
    "start_time = time.time()\n",
    "ddqn_model_4 = ids_ddqn()\n",
    "print()\n",
    "print(\"DQN 4 Training Time:\", time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation done...\n",
      "Accuracy: 80.04909044122014%\n",
      "Precision: 80.09667386437924%\n",
      "Recall/TPR/Sensitivity: 99.92091735863978%\n",
      "FPR: 99.9204375932372%\n",
      "F1 score: 0.8891723387176582\n",
      "\n",
      "validation done...\n",
      "Accuracy: 76.31188265800985%\n",
      "Precision: 76.35056161968343%\n",
      "Recall/TPR/Sensitivity: 99.92481203007519%\n",
      "FPR: 99.90794208720395%\n",
      "F1 score: 0.8656133140181249\n",
      "\n",
      "validation done...\n",
      "Accuracy: 68.70880262871395%\n",
      "Precision: 68.7196656035183%\n",
      "Recall/TPR/Sensitivity: 99.94813726337627%\n",
      "FPR: 99.86086516569695%\n",
      "F1 score: 0.8144296011081634\n",
      "\n",
      "validation done...\n",
      "Accuracy: 34.78097349512065%\n",
      "Precision: 34.73523643495315%\n",
      "Recall/TPR/Sensitivity: 99.98289331128471%\n",
      "FPR: 99.88781759747741%\n",
      "F1 score: 0.5155845683368618\n",
      "\n",
      "Total validation done...\n",
      "Accuracy: 64.96268730576615%\n",
      "Precision: 64.97553438063352%\n",
      "Recall/TPR/Sensitivity: 99.94418999084398%\n",
      "FPR: 99.89426561090387%\n",
      "F1 score: 0.771199955545202\n",
      "\n",
      "Saving model 1\n"
     ]
    }
   ],
   "source": [
    "# 0 is benign (positive), 1 is malicious (negative) \n",
    "def ids_eval(model):\n",
    "    TP, FP, TN, FN = 0,0,0,0\n",
    "    env = IdsEnv(images_per_episode=1, dataset=(x_test, y_test), random=False)\n",
    "    obs, done = env.reset(), False\n",
    "    try:\n",
    "        while True:\n",
    "            obs, done = env.reset(), False\n",
    "            while not done:\n",
    "                obs, rew, done, info = env.step(model.predict(obs)[0])\n",
    "                label = info['label']\n",
    "                if label == 0 and rew > 0:\n",
    "                    TP += 1\n",
    "                if label == 0 and rew == 0:\n",
    "                    FP += 1\n",
    "                if label == 1 and rew > 0:\n",
    "                    TN += 1\n",
    "                if label == 1 and rew == 0:\n",
    "                    FN += 1\n",
    "\n",
    "    except StopIteration:\n",
    "        accuracy = (float(TP + TN) / (TP + FP + FN + TN)) \n",
    "        precision = (float(TP) / (TP + FP))\n",
    "        try:\n",
    "            recall = (float(TP) / (TP + FN)) # = TPR = Sensitivity\n",
    "        except:\n",
    "            recall = 0\n",
    "        try:\n",
    "            FPR = (float(FP) / (TN + FP)) # 1 - specificity\n",
    "        except:\n",
    "            FPR = 0\n",
    "        try:\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        except:\n",
    "            f1_score = 0\n",
    "        print()\n",
    "        print('validation done...')\n",
    "        print('Accuracy: {0}%'.format(accuracy * 100))\n",
    "        print('Precision: {0}%'.format(precision * 100))\n",
    "        print('Recall/TPR/Sensitivity: {0}%'.format(recall * 100))\n",
    "        print('FPR: {0}%'.format(FPR * 100))\n",
    "        print('F1 score: {0}'.format(f1_score))\n",
    "    return [accuracy, precision, recall, FPR, f1_score]\n",
    "\n",
    "models = [ddqn_model_1, ddqn_model_2, ddqn_model_3, ddqn_model_4]\n",
    "\n",
    "results_1 = ids_eval(ddqn_model_1)\n",
    "results_2 = ids_eval(ddqn_model_2)\n",
    "results_3 = ids_eval(ddqn_model_3)\n",
    "results_4 = ids_eval(ddqn_model_4)\n",
    "total_results = [-1,-1,-1,-1,-1]\n",
    "accuracies = [results_1[0], results_2[0], results_3[0], results_4[0]]\n",
    "\n",
    "for i in range(len(results_1)):\n",
    "    total_results[i] = (results_1[i] + results_2[i] + results_3[i] + results_4[i] )/ 4\n",
    "\n",
    "print()    \n",
    "print('Total validation done...')\n",
    "print('Accuracy: {0}%'.format(total_results[0] * 100))\n",
    "print('Precision: {0}%'.format(total_results[1] * 100))\n",
    "print('Recall/TPR/Sensitivity: {0}%'.format(total_results[2] * 100))\n",
    "print('FPR: {0}%'.format(total_results[3] * 100))\n",
    "print('F1 score: {0}'.format(total_results[4]))\n",
    "\n",
    "highest = 0\n",
    "for i in range(4):\n",
    "    if accuracies[i] > accuracies[highest]:\n",
    "        highest = i\n",
    "\n",
    "print()\n",
    "print(\"Saving model {0}\".format(highest + 1))\n",
    "models[highest].save('vanilla_dqn_cicddos2019_allfeat_2.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

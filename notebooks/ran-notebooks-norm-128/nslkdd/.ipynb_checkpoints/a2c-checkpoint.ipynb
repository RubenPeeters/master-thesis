{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rub setup.ipynb first when using on jupyterlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE FOR TRAINING DIFFERENT MODELS\n",
    "TYPE = 'A2C'\n",
    "NAME = 'default'\n",
    "DATASET = 'nslkdd'\n",
    "AMOUNT = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/stable_baselines/__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n",
      "  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 14329347093908320363,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 12729172228825899442\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 10424042707606951127\n",
       " physical_device_desc: \"device: XLA_GPU device\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from datetime import datetime\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.deepq.policies import FeedForwardPolicy, register_policy\n",
    "from stable_baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "\n",
    "from stable_baselines import deepq\n",
    "from stable_baselines import A2C\n",
    "from stable_baselines import bench\n",
    "from stable_baselines import logger\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [dueling, double, per, pn]\n",
    "parameters_presets = {\n",
    "    'vanilla': [False, False, False, False],\n",
    "    'dueling': [True, False, False, False],\n",
    "    'double': [False, True, False, False],\n",
    "    'dd': [True, True, False, False],\n",
    "    'pn_dd': [True, True, False, True],\n",
    "    'per_dd': [True, True, True, False],\n",
    "    'pn_per_dd': [True, True, True, True],\n",
    "    'default': [],\n",
    "}\n",
    "# [separate datasets?, train set dir, test set dir or empty, dummies list(only for sets with split train and test), topfeat?]\n",
    "dataset_presets = {\n",
    "    'nslkdd': [\n",
    "        True, \n",
    "        \"/project/datasets/clean-ids-collection/nsl-kdd/clean/KDDTrain.feather\", \n",
    "        \"/project/datasets/clean-ids-collection/nsl-kdd/clean/KDDTest.feather\",\n",
    "        [\n",
    "            'protocol_type', \n",
    "            'service',\n",
    "            'flag'\n",
    "        ],\n",
    "        'class',\n",
    "        'normal',\n",
    "        False\n",
    "              ],\n",
    "    'unswnb15': [\n",
    "        True, \n",
    "        \"/project/datasets/clean-ids-collection/unsw-nb15/clean/designated-train-test-sets/UNSW_NB15_training-set.feather\", \n",
    "        \"/project/datasets/clean-ids-collection/unsw-nb15/clean/designated-train-test-sets/UNSW_NB15_testing-set.feather\",\n",
    "        [\n",
    "            'proto', \n",
    "            'service',\n",
    "            'state'\n",
    "        ],\n",
    "        'attack_cat',\n",
    "        'normal',\n",
    "        False\n",
    "              ],\n",
    "    'cicddos2019': [\n",
    "        False, \n",
    "        \"/project/datasets/clean-ids-collection/cic-ddos2019/clean/cicddos2019.feather\", \n",
    "        None,\n",
    "        [],\n",
    "        'Label',\n",
    "        'Benign',\n",
    "        False\n",
    "              ],\n",
    "    'cicddos2019-top': [\n",
    "        False, \n",
    "        \"/project/datasets/clean-ids-collection/cic-ddos2019/clean/cicddos2019.feather\", \n",
    "        None,\n",
    "        [],\n",
    "        'Label',\n",
    "        'Benign',\n",
    "        True\n",
    "              ],\n",
    "    'cicdos2017': [\n",
    "        False, \n",
    "        \"/project/datasets/clean-ids-collection/cic-dos2017/clean/cicdos2017.feather\", \n",
    "        None,\n",
    "        [],\n",
    "        'Label',\n",
    "        'Benign',\n",
    "        False\n",
    "              ],\n",
    "    'cicids2017': [\n",
    "        False, \n",
    "        \"/project/datasets/clean-ids-collection/cic-ids2017/clean/cicids2017.feather\", \n",
    "        None,\n",
    "        [],\n",
    "        'Label',\n",
    "        'Benign',\n",
    "        False\n",
    "              ],\n",
    "}\n",
    "parameters = parameters_presets[NAME]\n",
    "data_parameters = dataset_presets[DATASET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mal_ben(df, label, normal):\n",
    "    print(\"Preprocessing: make labels binary\")\n",
    "    print()\n",
    "    df[label] = df[label].astype('object')\n",
    "    atk_idx = df.loc[df[label] != normal].index\n",
    "    df.loc[atk_idx, label] = 1.0\n",
    "    df.loc[df.index.difference(atk_idx), label] = 0.0\n",
    "    df[label] = df[label].astype(dtype=np.float32)\n",
    "\n",
    "def preproc(df_train, df_test, columns):\n",
    "    print(\"Preprocessing: One hot encoding + abundant features\")\n",
    "    print()\n",
    "    df_train = pd.get_dummies(df_train,columns=columns)\n",
    "    df_test = pd.get_dummies(df_test,columns=columns)\n",
    "    extra_removables = df_test.columns ^ df_train.columns\n",
    "    for to_remove in extra_removables:\n",
    "            try:\n",
    "                df_test.drop(to_remove, inplace=True, axis=1)\n",
    "            except:\n",
    "                print(f\"{to_remove} already not part of test set, skipping\")\n",
    "            try:\n",
    "                df_train.drop(to_remove, inplace=True, axis=1)\n",
    "            except:\n",
    "                print(f\"{to_remove} already not part of train set, skipping\")\n",
    "    return df_train, df_test\n",
    "\n",
    "def split_df(df):\n",
    "    print(\"Preprocessing: split into X and Y sets\")\n",
    "    print()\n",
    "    col = df.columns[-1]\n",
    "    cols = df.columns[:-1]\n",
    "    return df[cols], df[col]\n",
    "\n",
    "def top_feat(df, dataset):\n",
    "    print(\"Preprocessing: convert to top features\")\n",
    "    print()\n",
    "    features = {\n",
    "        'cicdos2017-top': [\"Init Bwd Win Bytes\",  \"Idle Min\", \"ACK Flag Count\", \"Fwd Packet Length Min\", \"Fwd PSH Flags\"],\n",
    "        'cicids2017-top': [\"Avg Packet Size\", \"Packet Length Variance\", \"Fwd Packet Length Mean\", \"Init Fwd Win Bytes\", \"Bwd IAT Total\"],\n",
    "        'cicddos2019-top': [\"URG Flag Count\", \"Down/Up Ratio\", \"Bwd Packet Length Min\", \"ACK Flag Count\", \"Fwd Packets Length Total\"],\n",
    "        'nslkdd-top': [\"dst_host_serror_rate\", \"service_private\", \"count\",\"dst_host_count\", \"service_domain_u\", \"flag_REJ\", \"dst_host_diff_srv_rate\"],\n",
    "        'unswnb15-top': [\"sttl\", \"dttl\", \"ct_state_ttl\", \"service\", \"dload\", \"rate\", \"dmean\", \"dbytes\", \"dur\", \"is_sm_ips_ports\", \"dloss\"],\n",
    "    }\n",
    "    important_features = features[dataset] + [\"Label\"] # Adding class for custom environment logic\n",
    "    important_features = list(set(important_features))\n",
    "    removable_features = df.columns ^ important_features\n",
    "    \n",
    "    return df.drop(labels=removable_features, axis='columns')\n",
    "\n",
    "def nslkdd_split_df(train_df, test_df):\n",
    "    train_col = data_parameters[4]\n",
    "    print(train_col)\n",
    "    train_cols = train_df.loc[:, train_df.columns != data_parameters[4]].columns\n",
    "    print(train_cols)\n",
    "    test_col = data_parameters[4]\n",
    "    print(test_col)\n",
    "    test_cols = test_df.loc[:, test_df.columns != data_parameters[4]].columns\n",
    "    print(test_cols)\n",
    "\n",
    "    return train_df[train_cols], train_df[train_col], test_df[test_cols], test_df[test_col]\n",
    "\n",
    "def balancing_dataset_sampler_df(df, train_frac=0.2, val_frac=0.1, test_frac=0.7):\n",
    "    col = df.columns[-1]\n",
    "    print(col)\n",
    "    cols = df.columns[:-1]\n",
    "    print(cols)\n",
    "    vc = df[col].value_counts()\n",
    "    print(vc)\n",
    "    n = vc.iloc[-1]\n",
    "    print(n)\n",
    "    m = vc.iloc[0]\n",
    "    print(m)\n",
    "    print(int(m-n))\n",
    "    initial_cut = df.loc[df[col] == vc.index[0]].sample(n=int(m-n), replace=False)\n",
    "    print(initial_cut.index)\n",
    "    df = df.drop(index=initial_cut.index)\n",
    "    vc = df[col].value_counts()\n",
    "    print(vc)\n",
    "    print(int(n*train_frac))\n",
    "    train_df = df.groupby(col).apply(lambda x: x.sample(n=int(n*train_frac), replace=False))\n",
    "    train_df.reset_index(level=0, inplace=True, drop=True)\n",
    "    df = df.drop(index=train_df.index)\n",
    "\n",
    "    validation_df = df.groupby(col).apply(lambda x: x.sample(n=int(n*val_frac), replace=False))\n",
    "    validation_df.reset_index(level=0, inplace=True, drop=True)\n",
    "    df = df.drop(index=validation_df.index)\n",
    "\n",
    "    test_df = df.groupby(col).apply(lambda x: x.sample(n=int(n*test_frac), replace=False))\n",
    "    test_df.reset_index(level=0, inplace=True, drop=True)\n",
    "    df = df.drop(index=test_df.index)\n",
    "\n",
    "    return train_df[cols], train_df[col], validation_df[cols], validation_df[col], test_df[cols], test_df[col]\n",
    "\n",
    "\n",
    "def ids_eval(model, x_test, y_test):\n",
    "    print(\"Evaluation: starting validation of the model\")\n",
    "    print()\n",
    "    TP, FP, TN, FN = 0,0,0,0\n",
    "    env = IdsEnv(images_per_episode=1, dataset=(x_test, y_test), random=False)\n",
    "    obs, done = env.reset(), False\n",
    "    try:\n",
    "        while True:\n",
    "            obs, done = env.reset(), False\n",
    "            while not done:\n",
    "                obs, rew, done, info = env.step(model.predict(obs)[0])\n",
    "                label = info['label']\n",
    "                if label == 0 and rew > 0:\n",
    "                    TP += 1\n",
    "                if label == 0 and rew == 0:\n",
    "                    FP += 1\n",
    "                if label == 1 and rew > 0:\n",
    "                    TN += 1\n",
    "                if label == 1 and rew == 0:\n",
    "                    FN += 1\n",
    "\n",
    "    except StopIteration:\n",
    "        accuracy = (float(TP + TN) / (TP + FP + FN + TN)) \n",
    "        precision = (float(TP) / (TP + FP))\n",
    "        recall = (float(TP) / (TP + FN)) # = TPR = Sensitivity\n",
    "        try:\n",
    "            FPR = (float(FP) / (TN + FP)) # 1 - specificity\n",
    "        except:\n",
    "            FPR = 0.0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        print()\n",
    "        print('Evaluation: validation done...')\n",
    "        print('Accuracy: {0}%'.format(accuracy * 100))\n",
    "        print('Precision: {0}%'.format(precision * 100))\n",
    "        print('Recall/TPR/Sensitivity: {0}%'.format(recall * 100))\n",
    "        print('FPR: {0}%'.format(FPR * 100))\n",
    "        print('F1 score: {0}'.format(f1_score))\n",
    "    return [accuracy, precision, recall, FPR, f1_score]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing: One hot encoding + abundant features\n",
      "\n",
      "service_aol already not part of test set, skipping\n",
      "service_harvest already not part of test set, skipping\n",
      "service_http_2784 already not part of test set, skipping\n",
      "service_http_8001 already not part of test set, skipping\n",
      "service_red_i already not part of test set, skipping\n",
      "service_urh_i already not part of test set, skipping\n"
     ]
    }
   ],
   "source": [
    "if data_parameters[0]:\n",
    "    df_train = pd.read_feather(data_parameters[1])\n",
    "    df_test = pd.read_feather(data_parameters[2])\n",
    "    df_train, df_test = preproc(df_train, df_test, data_parameters[3])\n",
    "    LENGTH = len(df_train.columns) - 1\n",
    "else:\n",
    "    df = pd.read_feather(data_parameters[1])\n",
    "    LENGTH = len(df.columns) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125973 entries, 0 to 125972\n",
      "Columns: 117 entries, duration to flag_SH\n",
      "dtypes: float64(15), int64(23), object(1), uint8(78)\n",
      "memory usage: 46.9+ MB\n"
     ]
    }
   ],
   "source": [
    "if data_parameters[0]:\n",
    "    df_train.info()\n",
    "    df_train.describe()\n",
    "else:\n",
    "    df.info()\n",
    "    df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_parameters[6]:\n",
    "    if data_parameters[0]:\n",
    "        df_train = top_feat(df_train, DATASET)\n",
    "        df_test = top_feat(df_test, DATASET)\n",
    "    else:\n",
    "        df = top_feat(df, DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing done by Laurens D'Hooge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "- Build an agent to classify network flow automatically\n",
    "- Feed a packet that gets classified\n",
    "- Want the classification to be equal to the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing: make labels binary\n",
      "\n",
      "Preprocessing: make labels binary\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if data_parameters[0]:\n",
    "    mal_ben(df_train, data_parameters[4], data_parameters[5])\n",
    "    mal_ben(df_test, data_parameters[4], data_parameters[5])\n",
    "else: \n",
    "    mal_ben(df, data_parameters[4], data_parameters[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_parameters[0]:\n",
    "    LENGTH = len(df_train.columns) - 1\n",
    "else:\n",
    "    LENGTH = len(df.columns) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "Index(['duration', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment',\n",
      "       'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n",
      "       ...\n",
      "       'flag_REJ', 'flag_RSTO', 'flag_RSTOS0', 'flag_RSTR', 'flag_S0',\n",
      "       'flag_S1', 'flag_S2', 'flag_S3', 'flag_SF', 'flag_SH'],\n",
      "      dtype='object', length=116)\n",
      "class\n",
      "Index(['duration', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment',\n",
      "       'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n",
      "       ...\n",
      "       'flag_REJ', 'flag_RSTO', 'flag_RSTOS0', 'flag_RSTR', 'flag_S0',\n",
      "       'flag_S1', 'flag_S2', 'flag_S3', 'flag_SF', 'flag_SH'],\n",
      "      dtype='object', length=116)\n"
     ]
    }
   ],
   "source": [
    "if data_parameters[0]:\n",
    "    x_train, y_train, x_test, y_test  = nslkdd_split_df(df_train, df_test)\n",
    "else:\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = balancing_dataset_sampler_df(df, train_frac=0.8, val_frac=0.0, test_frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.to_frame()\n",
    "y_test = y_test.to_frame()\n",
    "\n",
    "# custom keys -> replace by index\n",
    "\n",
    "x_train = x_train.set_index([pd.Index(range (0, len(x_train)))])\n",
    "y_train = y_train.set_index([pd.Index(range (0, len(y_train)))])\n",
    "x_test = x_test.set_index([pd.Index(range (0, len(x_test)))])\n",
    "y_test = y_test.set_index([pd.Index(range (0, len(y_test)))])\n",
    "\n",
    "\n",
    "for col in y_train.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdsEnv(gym.Env):\n",
    "    def __init__(self, images_per_episode=1, dataset=(x_train, y_train), random=True):\n",
    "        # Actions we can take, classify as malicious or non-malicious (later also the correct attack)\n",
    "        # change to 19 if detectiong all different attacks\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "         # All the features we have, len(important_features) - 1 features and 1 label. Label should not be included\n",
    "        self.observation_space = gym.spaces.Box(low=float('-inf'), high=float('inf'), shape=(LENGTH,))\n",
    "        self.images_per_episode = images_per_episode\n",
    "        self.step_count = 0\n",
    "\n",
    "        self.x, self.y = dataset\n",
    "        self.random = random\n",
    "        self.dataset_idx = 0\n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = int(action == self.expected_action)\n",
    "        current_label = self.expected_action\n",
    "        obs = self._next_obs()\n",
    "\n",
    "        self.step_count += 1\n",
    "        if self.step_count >= self.images_per_episode:\n",
    "            done = True\n",
    "\n",
    "        return obs, reward, done, {'label': current_label}\n",
    "\n",
    "    def _next_obs(self):\n",
    "        if self.random:\n",
    "            next_obs_idx = random.randint(0, len(self.x) - 1)\n",
    "            self.expected_action = int(self.y.iloc[next_obs_idx,:])\n",
    "            obs = self.x.iloc[next_obs_idx,:]\n",
    "\n",
    "        else:\n",
    "            obs = self.x.iloc[self.dataset_idx]\n",
    "            self.expected_action = int(self.y.iloc[self.dataset_idx])\n",
    "\n",
    "            self.dataset_idx += 1\n",
    "            if self.dataset_idx >= len(self.x):\n",
    "                raise StopIteration()\n",
    "        return obs\n",
    "    \n",
    "    def reset(self):\n",
    "        self.step_count = 0\n",
    "\n",
    "        obs = self._next_obs()\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train classifier using dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPolicy(FeedForwardPolicy):\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch,\n",
    "                 reuse=False, obs_phs=None, dueling=True, **_kwargs):\n",
    "        super(CustomPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse,\n",
    "                                        feature_extraction=\"mlp\", obs_phs=obs_phs, dueling=dueling,\n",
    "                                        layer_norm=True, **_kwargs)\n",
    "        \n",
    "register_policy(\"CustomPolicy\", CustomPolicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7ff95c09d510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7ff95c09d510>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7ff954256d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7ff954256d10>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "---------------------------------\n",
      "| explained_variance | nan      |\n",
      "| fps                | 14       |\n",
      "| nupdates           | 1        |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 5        |\n",
      "| value_loss         | 0.0691   |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "def ids(params, tp):\n",
    "    \n",
    "    env = IdsEnv(images_per_episode=1)\n",
    "    if tp == 'DQN':\n",
    "        model = deepq.DQN(\n",
    "            CustomPolicy, \n",
    "            env, \n",
    "            policy_kwargs=dict(dueling=params[0], layers=[128,128]),\n",
    "            double_q=params[1],\n",
    "            verbose=1, \n",
    "            learning_rate=0.00025,\n",
    "            buffer_size=1000000,\n",
    "            exploration_fraction=0.1,\n",
    "            exploration_final_eps=0.1,\n",
    "            train_freq=4,\n",
    "            learning_starts=5000,\n",
    "            target_network_update_freq=10000,\n",
    "            gamma=1.0,\n",
    "            param_noise=params[3],\n",
    "            prioritized_replay=params[2],\n",
    "            prioritized_replay_alpha=0.6,\n",
    "            batch_size=32,\n",
    "        )\n",
    "    if tp == 'A2C':\n",
    "        model = A2C(\n",
    "            MlpPolicy, \n",
    "            env, \n",
    "            verbose=1,\n",
    "        )\n",
    "    model.learn(\n",
    "        total_timesteps=int(1.0e5),\n",
    "        log_interval=int(1.0e4),\n",
    "    )\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "    return model\n",
    "\n",
    "times = []\n",
    "models = []\n",
    "\n",
    "for i in range(AMOUNT):\n",
    "    start_time = time.time()\n",
    "    models.append(ids(parameters, TYPE))\n",
    "    print()\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"{TYPE} {i + 1} Training Time:\", duration)\n",
    "    times.append(duration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 is benign (positive), 1 is malicious (negative) \n",
    "def ids_eval(model):\n",
    "    TP, FP, TN, FN = 0,0,0,0\n",
    "    env = IdsEnv(images_per_episode=1, dataset=(x_test, y_test), random=False)\n",
    "    obs, done = env.reset(), False\n",
    "    try:\n",
    "        while True:\n",
    "            obs, done = env.reset(), False\n",
    "            while not done:\n",
    "                obs, rew, done, info = env.step(model.predict(obs)[0])\n",
    "                label = info['label']\n",
    "                if label == 0 and rew > 0:\n",
    "                    TP += 1\n",
    "                if label == 0 and rew == 0:\n",
    "                    FP += 1\n",
    "                if label == 1 and rew > 0:\n",
    "                    TN += 1\n",
    "                if label == 1 and rew == 0:\n",
    "                    FN += 1\n",
    "\n",
    "    except StopIteration:\n",
    "        accuracy = (float(TP + TN) / (TP + FP + FN + TN)) \n",
    "        precision = (float(TP) / (TP + FP))\n",
    "        try:\n",
    "            recall = (float(TP) / (TP + FN)) # = TPR = Sensitivity\n",
    "        except:\n",
    "            recall = 0\n",
    "        try:\n",
    "            FPR = (float(FP) / (TN + FP)) # 1 - specificity\n",
    "        except:\n",
    "            FPR = 0\n",
    "        try:\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        except:\n",
    "            f1_score = 0\n",
    "        print()\n",
    "        print('validation done...')\n",
    "        print('Accuracy: {0}%'.format(accuracy * 100))\n",
    "        print('Precision: {0}%'.format(precision * 100))\n",
    "        print('Recall/TPR/Sensitivity: {0}%'.format(recall * 100))\n",
    "        print('FPR: {0}%'.format(FPR * 100))\n",
    "        print('F1 score: {0}'.format(f1_score))\n",
    "    return [accuracy, precision, recall, FPR, f1_score]\n",
    "\n",
    "results = []\n",
    "for i, m in enumerate(models):\n",
    "    results.append(ids_eval(m))\n",
    "\n",
    "total_results = []\n",
    "total_results = [0] * len(results[0])\n",
    "accuracies = [result[0] for result in results]\n",
    "\n",
    "for j in range(AMOUNT):\n",
    "    for i in range(len(results[0])):\n",
    "        total_results[i] =  total_results[i] + results[j][i]\n",
    "for i in range(len(results[0])):\n",
    "        total_results[i] =  total_results[i] / AMOUNT\n",
    "        \n",
    "print()    \n",
    "print('Total validation done...')\n",
    "print('Accuracy: {0}%'.format(total_results[0] * 100))\n",
    "print('Precision: {0}%'.format(total_results[1] * 100))\n",
    "print('Recall/TPR/Sensitivity: {0}%'.format(total_results[2] * 100))\n",
    "print('FPR: {0}%'.format(total_results[3] * 100))\n",
    "print('F1 score: {0}'.format(total_results[4]))\n",
    "\n",
    "\n",
    "path = f'/project/normalization-runs/models'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    print(f\"{path} doesn't exist, creating it + subfolders\")\n",
    "    os.makedirs(path)\n",
    "    os.makedirs(f'{path}/{DATASET}')\n",
    "    os.makedirs(f'{path}/{DATASET}/DQN')\n",
    "    os.makedirs(f'{path}/{DATASET}/A2C')\n",
    "    \n",
    "if not os.path.exists(f'{path}/{DATASET}'):\n",
    "    print(f\"{path}/{DATASET} doesn't exist, creating it + subfolders\")\n",
    "    os.makedirs(f'{path}/{DATASET}')\n",
    "    os.makedirs(f'{path}/{DATASET}/DQN')\n",
    "    os.makedirs(f'{path}/{DATASET}/A2C')\n",
    "\n",
    "for i in range(AMOUNT):\n",
    "    print()\n",
    "    print(f'Saving model {i + 1} to {path}/{DATASET}/{TYPE}/{NAME}_{TYPE}_{i + 1}.pkl')\n",
    "    models[i].save(f'{path}/{DATASET}/{TYPE}/{NAME}_{TYPE}_{i + 1}.pkl')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

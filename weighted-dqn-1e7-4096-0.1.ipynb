{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'baselines' already exists and is not an empty directory.\n",
      "/project/baselines\n",
      "Obtaining file:///project/baselines\n",
      "Collecting gym<0.16.0,>=0.15.4\n",
      "  Downloading gym-0.15.7.tar.gz (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from baselines==0.1.6) (1.4.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from baselines==0.1.6) (4.46.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from baselines==0.1.6) (0.15.1)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.7/site-packages (from baselines==0.1.6) (1.4.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from baselines==0.1.6) (7.1.2)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 60.5 MB 154 kB/s  eta 0:00:01    |▊                               | 1.4 MB 21.8 MB/s eta 0:00:03     |██                              | 3.8 MB 21.8 MB/s eta 0:00:03     |███▎                            | 6.2 MB 21.8 MB/s eta 0:00:03     |████▋                           | 8.6 MB 21.8 MB/s eta 0:00:03     |██████▋                         | 12.4 MB 21.8 MB/s eta 0:00:03     |████████████▎                   | 23.1 MB 10.8 MB/s eta 0:00:04     |█████████████▋                  | 25.7 MB 10.8 MB/s eta 0:00:04     |██████████████▏                 | 26.9 MB 10.8 MB/s eta 0:00:04     |███████████████▌                | 29.3 MB 10.8 MB/s eta 0:00:03     |████████████████▉               | 31.8 MB 10.8 MB/s eta 0:00:03     |██████████████████              | 34.0 MB 10.8 MB/s eta 0:00:03     |███████████████████▎            | 36.4 MB 10.8 MB/s eta 0:00:03     |███████████████████▉            | 37.5 MB 10.8 MB/s eta 0:00:03     |█████████████████████           | 39.6 MB 10.8 MB/s eta 0:00:02     |█████████████████████████▏      | 47.7 MB 11.1 MB/s eta 0:00:02     |████████████████████████████▌   | 53.9 MB 11.1 MB/s eta 0:00:01     |███████████████████████████████▋| 59.7 MB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /opt/conda/lib/python3.7/site-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.18.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.15.0)\n",
      "Collecting pyglet<=1.5.0,>=1.4.0\n",
      "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 20.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 17.8 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: gym, future\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.15.7-py3-none-any.whl size=1648839 sha256=c44a8b9c3b2e3921486858acd9acf66adc18b126f1a66442488e685509ba80c3\n",
      "  Stored in directory: /home/rppeeter/.cache/pip/wheels/be/72/05/d3dfcfc2a31bbf886112b6373881bdf2e9e00d2c943f3b4f91\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=99e006a8d8d9ed861a54cd250812c77cdbb1ed7088be5e62ba977403dfe769a9\n",
      "  Stored in directory: /home/rppeeter/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "Successfully built gym future\n",
      "\u001b[31mERROR: gym 0.15.7 has requirement cloudpickle~=1.2.0, but you'll have cloudpickle 1.4.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: future, pyglet, gym, opencv-python, baselines\n",
      "  Running setup.py develop for baselines\n",
      "Successfully installed baselines future-0.18.2 gym-0.15.7 opencv-python-4.5.5.64 pyglet-1.5.0\n",
      "Collecting tensorflow==1.14.0\n",
      "  Downloading tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 109.3 MB 97 kB/s s eta 0:00:01   |█▎                              | 4.5 MB 10.6 MB/s eta 0:00:10     |██                              | 6.9 MB 10.6 MB/s eta 0:00:10     |██▊                             | 9.4 MB 10.6 MB/s eta 0:00:10     |███▏                            | 10.7 MB 10.6 MB/s eta 0:00:10     |███▌                            | 11.9 MB 10.6 MB/s eta 0:00:10     |███▉                            | 13.1 MB 10.6 MB/s eta 0:00:10     |████▋                           | 15.6 MB 10.6 MB/s eta 0:00:09     |█████                           | 16.8 MB 10.6 MB/s eta 0:00:09     |█████▋                          | 19.3 MB 10.6 MB/s eta 0:00:09     |██████▉                         | 23.2 MB 25.5 MB/s eta 0:00:04     |███████▏                        | 24.5 MB 25.5 MB/s eta 0:00:04     |████████▎                       | 28.2 MB 25.5 MB/s eta 0:00:04     |█████████▋                      | 32.9 MB 25.5 MB/s eta 0:00:03     |██████████▎                     | 35.0 MB 25.5 MB/s eta 0:00:03     |██████████▋                     | 36.3 MB 25.5 MB/s eta 0:00:03     |███████████                     | 37.7 MB 25.5 MB/s eta 0:00:03     |███████████▉                    | 40.5 MB 25.5 MB/s eta 0:00:03     |████████████▎                   | 41.9 MB 25.5 MB/s eta 0:00:03     |█████████████▌                  | 46.0 MB 28.6 MB/s eta 0:00:03     |█████████████▉                  | 47.3 MB 28.6 MB/s eta 0:00:03     |██████████████▎                 | 48.7 MB 28.6 MB/s eta 0:00:03     |██████████████▋                 | 50.1 MB 28.6 MB/s eta 0:00:03     |███████████████                 | 51.4 MB 28.6 MB/s eta 0:00:03     |████████████████▏               | 55.3 MB 28.6 MB/s eta 0:00:02     |█████████████████▍              | 59.4 MB 28.6 MB/s eta 0:00:02     |█████████████████▊              | 60.7 MB 28.6 MB/s eta 0:00:02     |██████████████████▏             | 62.0 MB 28.6 MB/s eta 0:00:02     |███████████████████▊            | 67.5 MB 28.6 MB/s eta 0:00:02     |████████████████████▏           | 68.9 MB 28.6 MB/s eta 0:00:02     |█████████████████████           | 71.5 MB 27.1 MB/s eta 0:00:02     |█████████████████████▎          | 72.8 MB 27.1 MB/s eta 0:00:02     |███████████████████████▏        | 79.2 MB 27.1 MB/s eta 0:00:02     |█████████████████████████       | 85.1 MB 27.1 MB/s eta 0:00:01     |███████████████████████████▎    | 93.2 MB 23.0 MB/s eta 0:00:01     |████████████████████████████▎   | 96.6 MB 23.0 MB/s eta 0:00:01     |████████████████████████████▋   | 97.7 MB 23.0 MB/s eta 0:00:01     |██████████████████████████████▎ | 103.5 MB 23.0 MB/s eta 0:00:01     |███████████████████████████████ | 105.8 MB 23.0 MB/s eta 0:00:01     |███████████████████████████████▍| 107.0 MB 23.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
      "  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
      "\u001b[K     |████████████████████████████████| 488 kB 20.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.18.5)\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (0.2.0)\n",
      "Collecting keras-applications>=1.0.6\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 6.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (3.11.4)\n",
      "Collecting tensorboard<1.15.0,>=1.14.0\n",
      "  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 20.0 MB/s eta 0:00:01     |█████████████▌                  | 1.3 MB 20.0 MB/s eta 0:00:01     |█████████████████████████       | 2.5 MB 20.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==1.14.0) (1.29.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow==1.14.0) (47.1.1.post20200529)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.2.2)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.6.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.1.0)\n",
      "Installing collected packages: tensorflow-estimator, astor, keras-applications, tensorboard, tensorflow\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.2.0\n",
      "    Uninstalling tensorflow-estimator-2.2.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.2.2\n",
      "    Uninstalling tensorboard-2.2.2:\n",
      "      Successfully uninstalled tensorboard-2.2.2\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.2.0\n",
      "    Uninstalling tensorflow-2.2.0:\n",
      "      Successfully uninstalled tensorflow-2.2.0\n",
      "Successfully installed astor-0.8.1 keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n",
      "Requirement already satisfied: gym in /opt/conda/lib/python3.7/site-packages (0.15.7)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/conda/lib/python3.7/site-packages (from gym) (1.18.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gym) (1.15.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from gym) (1.4.1)\n",
      "Collecting cloudpickle~=1.2.0\n",
      "  Downloading cloudpickle-1.2.2-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "\u001b[31mERROR: distributed 2.18.0 has requirement cloudpickle>=1.3.0, but you'll have cloudpickle 1.2.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: cloudpickle\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.4.1\n",
      "    Uninstalling cloudpickle-1.4.1:\n",
      "      Successfully uninstalled cloudpickle-1.4.1\n",
      "Successfully installed cloudpickle-1.2.2\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/openai/baselines.git\n",
    "%cd baselines\n",
    "!pip install -e .\n",
    "!pip install --upgrade tensorflow==1.14.0\n",
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from baselines.ppo2 import ppo2\n",
    "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "\n",
    "from baselines import deepq\n",
    "from baselines import bench\n",
    "from baselines import logger\n",
    "import tensorflow as tf\n",
    "\n",
    "from baselines.common.tf_util import make_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packets</th>\n",
       "      <th>Total Backward Packets</th>\n",
       "      <th>Fwd Packets Length Total</th>\n",
       "      <th>Bwd Packets Length Total</th>\n",
       "      <th>Fwd Packet Length Max</th>\n",
       "      <th>Fwd Packet Length Min</th>\n",
       "      <th>Fwd Packet Length Mean</th>\n",
       "      <th>...</th>\n",
       "      <th>Fwd Seg Size Min</th>\n",
       "      <th>Active Mean</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-11-03 09:18:16.964447</td>\n",
       "      <td>114456999</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>8185.5835</td>\n",
       "      <td>28337.111</td>\n",
       "      <td>98168.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9529897.0</td>\n",
       "      <td>351582.62</td>\n",
       "      <td>10001143.0</td>\n",
       "      <td>9048097.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-11-03 09:18:18.506537</td>\n",
       "      <td>114347504</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>35028.4180</td>\n",
       "      <td>121314.914</td>\n",
       "      <td>420255.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9493930.0</td>\n",
       "      <td>351541.10</td>\n",
       "      <td>9978130.0</td>\n",
       "      <td>8820294.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 09:18:18.610576</td>\n",
       "      <td>36435473</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>116.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.333334</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>62416.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>62416.0</td>\n",
       "      <td>62416.0</td>\n",
       "      <td>36373056.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>36373056.0</td>\n",
       "      <td>36373056.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 09:18:18.610579</td>\n",
       "      <td>36434705</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>116.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.333334</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>62413.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>62413.0</td>\n",
       "      <td>62413.0</td>\n",
       "      <td>36372292.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>36372292.0</td>\n",
       "      <td>36372292.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>2018-11-03 09:18:18.610581</td>\n",
       "      <td>36434626</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>116.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.333334</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>62409.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>62409.0</td>\n",
       "      <td>62409.0</td>\n",
       "      <td>36372216.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>36372216.0</td>\n",
       "      <td>36372216.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Protocol                   Timestamp  Flow Duration  Total Fwd Packets  \\\n",
       "0         0  2018-11-03 09:18:16.964447      114456999                 45   \n",
       "1         0  2018-11-03 09:18:18.506537      114347504                 56   \n",
       "2         6  2018-11-03 09:18:18.610576       36435473                  6   \n",
       "3         6  2018-11-03 09:18:18.610579       36434705                  6   \n",
       "4         6  2018-11-03 09:18:18.610581       36434626                  6   \n",
       "\n",
       "   Total Backward Packets  Fwd Packets Length Total  Bwd Packets Length Total  \\\n",
       "0                       0                       0.0                       0.0   \n",
       "1                       0                       0.0                       0.0   \n",
       "2                       2                     116.0                      92.0   \n",
       "3                       2                     116.0                      92.0   \n",
       "4                       2                     116.0                      92.0   \n",
       "\n",
       "   Fwd Packet Length Max  Fwd Packet Length Min  Fwd Packet Length Mean  ...  \\\n",
       "0                    0.0                    0.0                0.000000  ...   \n",
       "1                    0.0                    0.0                0.000000  ...   \n",
       "2                   46.0                    6.0               19.333334  ...   \n",
       "3                   46.0                    6.0               19.333334  ...   \n",
       "4                   46.0                    6.0               19.333334  ...   \n",
       "\n",
       "   Fwd Seg Size Min  Active Mean  Active Std  Active Max  Active Min  \\\n",
       "0                 0    8185.5835   28337.111     98168.0         3.0   \n",
       "1                 0   35028.4180  121314.914    420255.0         4.0   \n",
       "2                20   62416.0000       0.000     62416.0     62416.0   \n",
       "3                20   62413.0000       0.000     62413.0     62413.0   \n",
       "4                20   62409.0000       0.000     62409.0     62409.0   \n",
       "\n",
       "    Idle Mean   Idle Std    Idle Max    Idle Min   Label  \n",
       "0   9529897.0  351582.62  10001143.0   9048097.0  Benign  \n",
       "1   9493930.0  351541.10   9978130.0   8820294.0  Benign  \n",
       "2  36373056.0       0.00  36373056.0  36373056.0  Benign  \n",
       "3  36372292.0       0.00  36372292.0  36372292.0  Benign  \n",
       "4  36372216.0       0.00  36372216.0  36372216.0  Benign  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/project/datasets/clean-ids-collection/cic-ddos2019/clean/cicddos2019.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 138007 entries, 0 to 138006\n",
      "Data columns (total 69 columns):\n",
      " #   Column                    Non-Null Count   Dtype  \n",
      "---  ------                    --------------   -----  \n",
      " 0   Protocol                  138007 non-null  int64  \n",
      " 1   Timestamp                 138007 non-null  object \n",
      " 2   Flow Duration             138007 non-null  int64  \n",
      " 3   Total Fwd Packets         138007 non-null  int64  \n",
      " 4   Total Backward Packets    138007 non-null  int64  \n",
      " 5   Fwd Packets Length Total  138007 non-null  float64\n",
      " 6   Bwd Packets Length Total  138007 non-null  float64\n",
      " 7   Fwd Packet Length Max     138007 non-null  float64\n",
      " 8   Fwd Packet Length Min     138007 non-null  float64\n",
      " 9   Fwd Packet Length Mean    138007 non-null  float64\n",
      " 10  Fwd Packet Length Std     138007 non-null  float64\n",
      " 11  Bwd Packet Length Max     138007 non-null  float64\n",
      " 12  Bwd Packet Length Min     138007 non-null  float64\n",
      " 13  Bwd Packet Length Mean    138007 non-null  float64\n",
      " 14  Bwd Packet Length Std     138007 non-null  float64\n",
      " 15  Flow Bytes/s              138007 non-null  float64\n",
      " 16  Flow Packets/s            138007 non-null  float64\n",
      " 17  Flow IAT Mean             138007 non-null  float64\n",
      " 18  Flow IAT Std              138007 non-null  float64\n",
      " 19  Flow IAT Max              138007 non-null  float64\n",
      " 20  Flow IAT Min              138007 non-null  float64\n",
      " 21  Fwd IAT Total             138007 non-null  float64\n",
      " 22  Fwd IAT Mean              138007 non-null  float64\n",
      " 23  Fwd IAT Std               138007 non-null  float64\n",
      " 24  Fwd IAT Max               138007 non-null  float64\n",
      " 25  Fwd IAT Min               138007 non-null  float64\n",
      " 26  Bwd IAT Total             138007 non-null  float64\n",
      " 27  Bwd IAT Mean              138007 non-null  float64\n",
      " 28  Bwd IAT Std               138007 non-null  float64\n",
      " 29  Bwd IAT Max               138007 non-null  float64\n",
      " 30  Bwd IAT Min               138007 non-null  float64\n",
      " 31  Fwd PSH Flags             138007 non-null  int64  \n",
      " 32  Fwd Header Length         138007 non-null  int64  \n",
      " 33  Bwd Header Length         138007 non-null  int64  \n",
      " 34  Fwd Packets/s             138007 non-null  float64\n",
      " 35  Bwd Packets/s             138007 non-null  float64\n",
      " 36  Packet Length Min         138007 non-null  float64\n",
      " 37  Packet Length Max         138007 non-null  float64\n",
      " 38  Packet Length Mean        138007 non-null  float64\n",
      " 39  Packet Length Std         138007 non-null  float64\n",
      " 40  Packet Length Variance    138007 non-null  float64\n",
      " 41  FIN Flag Count            138007 non-null  int64  \n",
      " 42  SYN Flag Count            138007 non-null  int64  \n",
      " 43  RST Flag Count            138007 non-null  int64  \n",
      " 44  PSH Flag Count            138007 non-null  int64  \n",
      " 45  ACK Flag Count            138007 non-null  int64  \n",
      " 46  URG Flag Count            138007 non-null  int64  \n",
      " 47  ECE Flag Count            138007 non-null  int64  \n",
      " 48  Down/Up Ratio             138007 non-null  float64\n",
      " 49  Avg Packet Size           138007 non-null  float64\n",
      " 50  Avg Fwd Segment Size      138007 non-null  float64\n",
      " 51  Avg Bwd Segment Size      138007 non-null  float64\n",
      " 52  Subflow Fwd Packets       138007 non-null  int64  \n",
      " 53  Subflow Fwd Bytes         138007 non-null  int64  \n",
      " 54  Subflow Bwd Packets       138007 non-null  int64  \n",
      " 55  Subflow Bwd Bytes         138007 non-null  int64  \n",
      " 56  Init Fwd Win Bytes        138007 non-null  int64  \n",
      " 57  Init Bwd Win Bytes        138007 non-null  int64  \n",
      " 58  Fwd Act Data Packets      138007 non-null  int64  \n",
      " 59  Fwd Seg Size Min          138007 non-null  int64  \n",
      " 60  Active Mean               138007 non-null  float64\n",
      " 61  Active Std                138007 non-null  float64\n",
      " 62  Active Max                138007 non-null  float64\n",
      " 63  Active Min                138007 non-null  float64\n",
      " 64  Idle Mean                 138007 non-null  float64\n",
      " 65  Idle Std                  138007 non-null  float64\n",
      " 66  Idle Max                  138007 non-null  float64\n",
      " 67  Idle Min                  138007 non-null  float64\n",
      " 68  Label                     138007 non-null  object \n",
      "dtypes: float64(45), int64(22), object(2)\n",
      "memory usage: 72.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packets</th>\n",
       "      <th>Total Backward Packets</th>\n",
       "      <th>Fwd Packets Length Total</th>\n",
       "      <th>Bwd Packets Length Total</th>\n",
       "      <th>Fwd Packet Length Max</th>\n",
       "      <th>Fwd Packet Length Min</th>\n",
       "      <th>Fwd Packet Length Mean</th>\n",
       "      <th>Fwd Packet Length Std</th>\n",
       "      <th>...</th>\n",
       "      <th>Fwd Act Data Packets</th>\n",
       "      <th>Fwd Seg Size Min</th>\n",
       "      <th>Active Mean</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>138007.000000</td>\n",
       "      <td>1.380070e+05</td>\n",
       "      <td>138007.000000</td>\n",
       "      <td>138007.000000</td>\n",
       "      <td>1.380070e+05</td>\n",
       "      <td>1.380070e+05</td>\n",
       "      <td>138007.000000</td>\n",
       "      <td>138007.000000</td>\n",
       "      <td>138007.000000</td>\n",
       "      <td>138007.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>138007.000000</td>\n",
       "      <td>1.380070e+05</td>\n",
       "      <td>1.380070e+05</td>\n",
       "      <td>1.380070e+05</td>\n",
       "      <td>1.380070e+05</td>\n",
       "      <td>1.380070e+05</td>\n",
       "      <td>1.380070e+05</td>\n",
       "      <td>1.380070e+05</td>\n",
       "      <td>1.380070e+05</td>\n",
       "      <td>1.380070e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.592050</td>\n",
       "      <td>1.035663e+07</td>\n",
       "      <td>13.812169</td>\n",
       "      <td>6.636576</td>\n",
       "      <td>4.019830e+03</td>\n",
       "      <td>5.102600e+03</td>\n",
       "      <td>249.273218</td>\n",
       "      <td>132.853515</td>\n",
       "      <td>161.376744</td>\n",
       "      <td>37.345936</td>\n",
       "      <td>...</td>\n",
       "      <td>10.525227</td>\n",
       "      <td>-1.175840e+07</td>\n",
       "      <td>7.345821e+04</td>\n",
       "      <td>3.066152e+04</td>\n",
       "      <td>1.140858e+05</td>\n",
       "      <td>5.479838e+04</td>\n",
       "      <td>3.445620e+06</td>\n",
       "      <td>3.282404e+05</td>\n",
       "      <td>3.770891e+06</td>\n",
       "      <td>3.153606e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.607782</td>\n",
       "      <td>2.853154e+07</td>\n",
       "      <td>70.298079</td>\n",
       "      <td>99.503208</td>\n",
       "      <td>5.151998e+04</td>\n",
       "      <td>1.880790e+05</td>\n",
       "      <td>419.156188</td>\n",
       "      <td>234.493611</td>\n",
       "      <td>241.043532</td>\n",
       "      <td>116.216344</td>\n",
       "      <td>...</td>\n",
       "      <td>62.973954</td>\n",
       "      <td>1.112129e+08</td>\n",
       "      <td>5.176193e+05</td>\n",
       "      <td>2.392317e+05</td>\n",
       "      <td>6.900226e+05</td>\n",
       "      <td>4.802156e+05</td>\n",
       "      <td>1.157949e+07</td>\n",
       "      <td>2.158705e+06</td>\n",
       "      <td>1.230392e+07</td>\n",
       "      <td>1.120812e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.408238e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.178000e+03</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.800000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.489200e+04</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.600000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.999380e+06</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.064000e+03</td>\n",
       "      <td>1.660000e+02</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>349.500000</td>\n",
       "      <td>14.433757</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.199987e+08</td>\n",
       "      <td>20444.000000</td>\n",
       "      <td>31700.000000</td>\n",
       "      <td>1.526642e+07</td>\n",
       "      <td>5.842950e+07</td>\n",
       "      <td>32120.000000</td>\n",
       "      <td>2131.000000</td>\n",
       "      <td>3015.290500</td>\n",
       "      <td>2221.556200</td>\n",
       "      <td>...</td>\n",
       "      <td>18766.000000</td>\n",
       "      <td>1.480000e+03</td>\n",
       "      <td>4.050800e+07</td>\n",
       "      <td>1.100562e+07</td>\n",
       "      <td>4.050800e+07</td>\n",
       "      <td>4.050800e+07</td>\n",
       "      <td>1.192194e+08</td>\n",
       "      <td>6.961402e+07</td>\n",
       "      <td>1.192194e+08</td>\n",
       "      <td>1.192194e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Protocol  Flow Duration  Total Fwd Packets  \\\n",
       "count  138007.000000   1.380070e+05      138007.000000   \n",
       "mean       11.592050   1.035663e+07          13.812169   \n",
       "std         5.607782   2.853154e+07          70.298079   \n",
       "min         0.000000   1.000000e+00           1.000000   \n",
       "25%         6.000000   1.178000e+03           2.000000   \n",
       "50%        17.000000   2.489200e+04           3.000000   \n",
       "75%        17.000000   2.999380e+06           8.000000   \n",
       "max        17.000000   1.199987e+08       20444.000000   \n",
       "\n",
       "       Total Backward Packets  Fwd Packets Length Total  \\\n",
       "count           138007.000000              1.380070e+05   \n",
       "mean                 6.636576              4.019830e+03   \n",
       "std                 99.503208              5.151998e+04   \n",
       "min                  0.000000              0.000000e+00   \n",
       "25%                  0.000000              1.800000e+01   \n",
       "50%                  2.000000              8.600000e+01   \n",
       "75%                  2.000000              2.064000e+03   \n",
       "max              31700.000000              1.526642e+07   \n",
       "\n",
       "       Bwd Packets Length Total  Fwd Packet Length Max  Fwd Packet Length Min  \\\n",
       "count              1.380070e+05          138007.000000          138007.000000   \n",
       "mean               5.102600e+03             249.273218             132.853515   \n",
       "std                1.880790e+05             419.156188             234.493611   \n",
       "min                0.000000e+00               0.000000               0.000000   \n",
       "25%                0.000000e+00               6.000000               0.000000   \n",
       "50%                0.000000e+00              43.000000              30.000000   \n",
       "75%                1.660000e+02             440.000000             152.000000   \n",
       "max                5.842950e+07           32120.000000            2131.000000   \n",
       "\n",
       "       Fwd Packet Length Mean  Fwd Packet Length Std  ...  \\\n",
       "count           138007.000000          138007.000000  ...   \n",
       "mean               161.376744              37.345936  ...   \n",
       "std                241.043532             116.216344  ...   \n",
       "min                  0.000000               0.000000  ...   \n",
       "25%                  6.000000               0.000000  ...   \n",
       "50%                 39.000000               0.000000  ...   \n",
       "75%                349.500000              14.433757  ...   \n",
       "max               3015.290500            2221.556200  ...   \n",
       "\n",
       "       Fwd Act Data Packets  Fwd Seg Size Min   Active Mean    Active Std  \\\n",
       "count         138007.000000      1.380070e+05  1.380070e+05  1.380070e+05   \n",
       "mean              10.525227     -1.175840e+07  7.345821e+04  3.066152e+04   \n",
       "std               62.973954      1.112129e+08  5.176193e+05  2.392317e+05   \n",
       "min                0.000000     -1.408238e+09  0.000000e+00  0.000000e+00   \n",
       "25%                1.000000      2.000000e+01  0.000000e+00  0.000000e+00   \n",
       "50%                1.000000      2.000000e+01  0.000000e+00  0.000000e+00   \n",
       "75%                5.000000      2.000000e+01  0.000000e+00  0.000000e+00   \n",
       "max            18766.000000      1.480000e+03  4.050800e+07  1.100562e+07   \n",
       "\n",
       "         Active Max    Active Min     Idle Mean      Idle Std      Idle Max  \\\n",
       "count  1.380070e+05  1.380070e+05  1.380070e+05  1.380070e+05  1.380070e+05   \n",
       "mean   1.140858e+05  5.479838e+04  3.445620e+06  3.282404e+05  3.770891e+06   \n",
       "std    6.900226e+05  4.802156e+05  1.157949e+07  2.158705e+06  1.230392e+07   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "max    4.050800e+07  4.050800e+07  1.192194e+08  6.961402e+07  1.192194e+08   \n",
       "\n",
       "           Idle Min  \n",
       "count  1.380070e+05  \n",
       "mean   3.153606e+06  \n",
       "std    1.120812e+07  \n",
       "min    0.000000e+00  \n",
       "25%    0.000000e+00  \n",
       "50%    0.000000e+00  \n",
       "75%    0.000000e+00  \n",
       "max    1.192194e+08  \n",
       "\n",
       "[8 rows x 67 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.info()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing done by Laurens D'Hooge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "- Build an agent to classify network flow automatically\n",
    "- Feed a packet that gets classified\n",
    "- Want the classification to be equal to the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balancing_dataset_sampler_df(df, train_frac=0.2, val_frac=0.1, test_frac=0.7):\n",
    "    col = df.columns[-1]\n",
    "    cols = df.columns[:-1]\n",
    "    vc = df[col].value_counts()\n",
    "    n = vc.iloc[-1]\n",
    "    m = vc.iloc[0]\n",
    "    initial_cut = df.loc[df[col] == vc.index[0]].sample(n=int(m-n), replace=False)\n",
    "    df = df.drop(index=initial_cut.index)\n",
    "\n",
    "    train_df = df.groupby(col).apply(lambda x: x.sample(n=int(n*train_frac), replace=False))\n",
    "    train_df.reset_index(level=0, inplace=True, drop=True)\n",
    "    df = df.drop(index=train_df.index)\n",
    "\n",
    "    validation_df = df.groupby(col).apply(lambda x: x.sample(n=int(n*val_frac), replace=False))\n",
    "    validation_df.reset_index(level=0, inplace=True, drop=True)\n",
    "    df = df.drop(index=validation_df.index)\n",
    "\n",
    "    test_df = df.groupby(col).apply(lambda x: x.sample(n=int(n*test_frac), replace=False))\n",
    "    test_df.reset_index(level=0, inplace=True, drop=True)\n",
    "    df = df.drop(index=test_df.index)\n",
    "\n",
    "    return train_df[cols], train_df[col], validation_df[cols], validation_df[col], test_df[cols], test_df[col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign           95095\n",
      "DrDoS_NTP        13687\n",
      "TFTP             13027\n",
      "Syn               6713\n",
      "UDP               2158\n",
      "MSSQL             1763\n",
      "DrDoS_UDP         1174\n",
      "UDP-lag           1097\n",
      "DrDoS_MSSQL        927\n",
      "DrDoS_DNS          552\n",
      "LDAP               484\n",
      "DrDoS_SSDP         413\n",
      "DrDoS_SNMP         392\n",
      "Portmap            215\n",
      "DrDoS_LDAP         174\n",
      "NetBIOS             73\n",
      "DrDoS_NetBIOS       51\n",
      "UDPLag              10\n",
      "WebDDoS              2\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data.drop('Timestamp', inplace=True, axis=1)\n",
    "print(data['Label'].value_counts())\n",
    "data['Label'] = data['Label'].astype('object')\n",
    "atk_idx = data.loc[data['Label'] != \"Benign\"].index\n",
    "data.loc[atk_idx, 'Label'] = 1.0\n",
    "data.loc[data.index.difference(atk_idx), 'Label'] = 0.0\n",
    "data['Label'] = data['Label'].astype(dtype=np.float32)\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test  = balancing_dataset_sampler_df(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "(17164, 67)\n",
      "(17164,)\n",
      "Validation\n",
      "(8582, 67)\n",
      "(8582,)\n",
      "Testing\n",
      "(60076, 67)\n",
      "(60076,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"Validation\")\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "print(\"Testing\")\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all attacks\n",
    "# attacks_dict = {\n",
    "#     'Benign': 0,\n",
    "#     'Portmap': 1,\n",
    "#     'NetBIOS': 2,\n",
    "#     'LDAP': 3,\n",
    "#     'MSSQL': 4,\n",
    "#     'UDP': 5,\n",
    "#     'UDPLag': 6,\n",
    "#     'Syn': 7,\n",
    "#     'DrDoS_NTP': 8,\n",
    "#     'DrDoS_DNS': 9,\n",
    "#     'DrDoS_LDAP': 10,\n",
    "#     'DrDoS_MSSQL': 11,\n",
    "#     'DrDoS_NetBIOS': 12,\n",
    "#     'DrDoS_SNMP': 13,\n",
    "#     'DrDoS_SSDP': 14,\n",
    "#     'DrDoS_UDP': 15,\n",
    "#     'UDP-lag': 16,\n",
    "#     'WebDDoS': 17,\n",
    "#     'TFTP': 18,\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "x_val = np.expand_dims(x_val, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.expand_dims(y_train, -1)\n",
    "y_test = np.expand_dims(y_test, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdsEnv(gym.Env):\n",
    "    def __init__(self, images_per_episode=1, dataset=(x_train, y_train), random=True):\n",
    "        # Actions we can take, classify as malicious or non-malicious (later also the correct attack)\n",
    "        # change to 19 if detectiong all different attacks\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        # All the features we have, 67 features and 1 label. Label should not be included\n",
    "        self.observation_space = gym.spaces.Box(low=float('-inf'), high=float('inf'), shape=(67,))\n",
    "        self.images_per_episode = images_per_episode\n",
    "        self.step_count = 0\n",
    "\n",
    "        self.x, self.y = dataset\n",
    "        self.random = random\n",
    "        self.dataset_idx = 0\n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = int(action == self.expected_action)\n",
    "\n",
    "        obs = self._next_obs()\n",
    "\n",
    "        self.step_count += 1\n",
    "        if self.step_count >= self.images_per_episode:\n",
    "            done = True\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def _next_obs(self):\n",
    "        if self.random:\n",
    "            next_obs_idx = random.randint(0, len(self.x) - 1)\n",
    "            self.expected_action = int(self.y[next_obs_idx])\n",
    "            obs = self.x[next_obs_idx]\n",
    "\n",
    "        else:\n",
    "            obs = self.x[self.dataset_idx]\n",
    "            self.expected_action = int(self.y[self.dataset_idx])\n",
    "\n",
    "            self.dataset_idx += 1\n",
    "            if self.dataset_idx >= len(self.x):\n",
    "                raise StopIteration()\n",
    "\n",
    "        return obs\n",
    "    \n",
    "    def reset(self):\n",
    "        self.step_count = 0\n",
    "\n",
    "        obs = self._next_obs()\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train classifier using dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./Training/Logs/ids_dqn\n",
      "WARNING:tensorflow:From /project/baselines/baselines/common/tf_util.py:53: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /project/baselines/baselines/common/tf_util.py:63: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /project/baselines/baselines/common/tf_util.py:70: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.\n",
      "\n",
      "WARNING:tensorflow:From /project/baselines/baselines/common/misc_util.py:58: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From /project/baselines/baselines/deepq/deepq.py:205: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /project/baselines/baselines/deepq/build_graph.py:176: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /project/baselines/baselines/common/input.py:57: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /project/baselines/baselines/common/models.py:94: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f514515e950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f514515e950>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f514515e950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f514515e950>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510db72390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510db72390>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510db72390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510db72390>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510db59190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510db59190>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510db59190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510db59190>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f514492fc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f514492fc90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f514492fc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f514492fc90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510db59190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510db59190>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510db59190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510db59190>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510db72210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510db72210>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510db72210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510db72210>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From /project/baselines/baselines/deepq/build_graph.py:189: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510da06a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510da06a90>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510da06a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510da06a90>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510da7ea50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510da7ea50>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510da7ea50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510da7ea50>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d99fc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d99fc50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d99fc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d99fc50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d99f950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d99f950>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d99f950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d99f950>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510da06150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510da06150>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510da06150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510da06150>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510dae13d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510dae13d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510dae13d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510dae13d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510daa9b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510daa9b90>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510daa9b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510daa9b90>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510da06a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510da06a50>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510da06a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510da06a50>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510da7e6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510da7e6d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510da7e6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510da7e6d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510daa9b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510daa9b90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510daa9b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510daa9b90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510dafb390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510dafb390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510dafb390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510dafb390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510dafb390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510dafb390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510dafb390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510dafb390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510d9d1a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510d9d1a10>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510d9d1a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510d9d1a10>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510da92a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510da92a10>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510da92a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f510da92a10>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d887fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d887fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d887fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d887fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d887fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d887fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d887fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d887fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d79e810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d79e810>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d79e810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d79e810>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d887a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d887a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d887a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f510d887a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 98       |\n",
      "| episodes                | 10       |\n",
      "| mean 100 episode reward | 2.04e+03 |\n",
      "| steps                   | 3.69e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 96       |\n",
      "| episodes                | 20       |\n",
      "| mean 100 episode reward | 2.04e+03 |\n",
      "| steps                   | 7.78e+04 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 94       |\n",
      "| episodes                | 30       |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 1.19e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 92       |\n",
      "| episodes                | 40       |\n",
      "| mean 100 episode reward | 2.06e+03 |\n",
      "| steps                   | 1.6e+05  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 90       |\n",
      "| episodes                | 50       |\n",
      "| mean 100 episode reward | 2.06e+03 |\n",
      "| steps                   | 2.01e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 88       |\n",
      "| episodes                | 60       |\n",
      "| mean 100 episode reward | 2.06e+03 |\n",
      "| steps                   | 2.42e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 86       |\n",
      "| episodes                | 70       |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 2.83e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 83       |\n",
      "| episodes                | 80       |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 3.24e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 81       |\n",
      "| episodes                | 90       |\n",
      "| mean 100 episode reward | 2.06e+03 |\n",
      "| steps                   | 3.65e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 79       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 2.06e+03 |\n",
      "| steps                   | 4.06e+05 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: None -> 2064.8\n",
      "Saving model due to mean reward increase: 2064.8 -> 2067.2\n",
      "Saving model due to mean reward increase: 2067.2 -> 2068.5\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 77       |\n",
      "| episodes                | 110      |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 4.46e+05 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 2068.5 -> 2069.3\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 75       |\n",
      "| episodes                | 120      |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 4.87e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 73       |\n",
      "| episodes                | 130      |\n",
      "| mean 100 episode reward | 2.06e+03 |\n",
      "| steps                   | 5.28e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 71       |\n",
      "| episodes                | 140      |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 5.69e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 69       |\n",
      "| episodes                | 150      |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 6.1e+05  |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 2069.3 -> 2069.4\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 67       |\n",
      "| episodes                | 160      |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 6.51e+05 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 2069.4 -> 2069.7\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 65       |\n",
      "| episodes                | 170      |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 6.92e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 63       |\n",
      "| episodes                | 180      |\n",
      "| mean 100 episode reward | 2.06e+03 |\n",
      "| steps                   | 7.33e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 61       |\n",
      "| episodes                | 190      |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 7.74e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 59       |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 2.06e+03 |\n",
      "| steps                   | 8.15e+05 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 2069.7 -> 2071.3\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 57       |\n",
      "| episodes                | 210      |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 8.56e+05 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 2071.3 -> 2074.9\n",
      "Saving model due to mean reward increase: 2074.9 -> 2076.4\n",
      "Saving model due to mean reward increase: 2076.4 -> 2076.9\n",
      "Saving model due to mean reward increase: 2076.9 -> 2079.7\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 55       |\n",
      "| episodes                | 220      |\n",
      "| mean 100 episode reward | 2.08e+03 |\n",
      "| steps                   | 8.97e+05 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 2079.7 -> 2081.1\n",
      "Saving model due to mean reward increase: 2081.1 -> 2082.7\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 53       |\n",
      "| episodes                | 230      |\n",
      "| mean 100 episode reward | 2.08e+03 |\n",
      "| steps                   | 9.38e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 51       |\n",
      "| episodes                | 240      |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 9.79e+05 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 49       |\n",
      "| episodes                | 250      |\n",
      "| mean 100 episode reward | 2.06e+03 |\n",
      "| steps                   | 1.02e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 47       |\n",
      "| episodes                | 260      |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 1.06e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 45       |\n",
      "| episodes                | 270      |\n",
      "| mean 100 episode reward | 2.06e+03 |\n",
      "| steps                   | 1.1e+06  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 43       |\n",
      "| episodes                | 280      |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 1.14e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 41       |\n",
      "| episodes                | 290      |\n",
      "| mean 100 episode reward | 2.08e+03 |\n",
      "| steps                   | 1.18e+06 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 2082.7 -> 2085.8\n",
      "Saving model due to mean reward increase: 2085.8 -> 2095.7\n",
      "Saving model due to mean reward increase: 2095.7 -> 2097.9\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 39       |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 2.1e+03  |\n",
      "| steps                   | 1.22e+06 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 2097.9 -> 2101.7\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 37       |\n",
      "| episodes                | 310      |\n",
      "| mean 100 episode reward | 2.1e+03  |\n",
      "| steps                   | 1.27e+06 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 2101.7 -> 2102.4\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 35       |\n",
      "| episodes                | 320      |\n",
      "| mean 100 episode reward | 2.1e+03  |\n",
      "| steps                   | 1.31e+06 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 2102.4 -> 2108.4\n",
      "Saving model due to mean reward increase: 2108.4 -> 2114.3\n",
      "Saving model due to mean reward increase: 2114.3 -> 2124.3\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 33       |\n",
      "| episodes                | 330      |\n",
      "| mean 100 episode reward | 2.13e+03 |\n",
      "| steps                   | 1.35e+06 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 2124.3 -> 2134.2\n",
      "Saving model due to mean reward increase: 2134.2 -> 2144.0\n",
      "Saving model due to mean reward increase: 2144.0 -> 2144.3\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 31       |\n",
      "| episodes                | 340      |\n",
      "| mean 100 episode reward | 2.15e+03 |\n",
      "| steps                   | 1.39e+06 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 2144.3 -> 2145.3\n",
      "Saving model due to mean reward increase: 2145.3 -> 2150.3\n",
      "Saving model due to mean reward increase: 2150.3 -> 2158.0\n",
      "Saving model due to mean reward increase: 2158.0 -> 2164.9\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 29       |\n",
      "| episodes                | 350      |\n",
      "| mean 100 episode reward | 2.16e+03 |\n",
      "| steps                   | 1.43e+06 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 2164.9 -> 2169.3\n",
      "Saving model due to mean reward increase: 2169.3 -> 2171.5\n",
      "Saving model due to mean reward increase: 2171.5 -> 2171.7\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 27       |\n",
      "| episodes                | 360      |\n",
      "| mean 100 episode reward | 2.17e+03 |\n",
      "| steps                   | 1.47e+06 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 2171.7 -> 2174.6\n",
      "Saving model due to mean reward increase: 2174.6 -> 2181.2\n",
      "Saving model due to mean reward increase: 2181.2 -> 2184.6\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 25       |\n",
      "| episodes                | 370      |\n",
      "| mean 100 episode reward | 2.19e+03 |\n",
      "| steps                   | 1.51e+06 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 2184.6 -> 2189.3\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 23       |\n",
      "| episodes                | 380      |\n",
      "| mean 100 episode reward | 2.16e+03 |\n",
      "| steps                   | 1.55e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 21       |\n",
      "| episodes                | 390      |\n",
      "| mean 100 episode reward | 2.16e+03 |\n",
      "| steps                   | 1.59e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 19       |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 2.13e+03 |\n",
      "| steps                   | 1.63e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 17       |\n",
      "| episodes                | 410      |\n",
      "| mean 100 episode reward | 2.1e+03  |\n",
      "| steps                   | 1.68e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 15       |\n",
      "| episodes                | 420      |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 1.72e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 13       |\n",
      "| episodes                | 430      |\n",
      "| mean 100 episode reward | 2e+03    |\n",
      "| steps                   | 1.76e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 10       |\n",
      "| episodes                | 440      |\n",
      "| mean 100 episode reward | 1.98e+03 |\n",
      "| steps                   | 1.8e+06  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 8        |\n",
      "| episodes                | 450      |\n",
      "| mean 100 episode reward | 1.94e+03 |\n",
      "| steps                   | 1.84e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 6        |\n",
      "| episodes                | 460      |\n",
      "| mean 100 episode reward | 1.88e+03 |\n",
      "| steps                   | 1.88e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 4        |\n",
      "| episodes                | 470      |\n",
      "| mean 100 episode reward | 1.83e+03 |\n",
      "| steps                   | 1.92e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 480      |\n",
      "| mean 100 episode reward | 1.81e+03 |\n",
      "| steps                   | 1.96e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 490      |\n",
      "| mean 100 episode reward | 1.76e+03 |\n",
      "| steps                   | 2e+06    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | 1.75e+03 |\n",
      "| steps                   | 2.04e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 510      |\n",
      "| mean 100 episode reward | 1.77e+03 |\n",
      "| steps                   | 2.08e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 520      |\n",
      "| mean 100 episode reward | 1.8e+03  |\n",
      "| steps                   | 2.13e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 530      |\n",
      "| mean 100 episode reward | 1.84e+03 |\n",
      "| steps                   | 2.17e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 540      |\n",
      "| mean 100 episode reward | 1.86e+03 |\n",
      "| steps                   | 2.21e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 550      |\n",
      "| mean 100 episode reward | 1.88e+03 |\n",
      "| steps                   | 2.25e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 560      |\n",
      "| mean 100 episode reward | 1.93e+03 |\n",
      "| steps                   | 2.29e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 570      |\n",
      "| mean 100 episode reward | 2.02e+03 |\n",
      "| steps                   | 2.33e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 580      |\n",
      "| mean 100 episode reward | 2.12e+03 |\n",
      "| steps                   | 2.37e+06 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 2189.3 -> 2206.7\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 590      |\n",
      "| mean 100 episode reward | 2.21e+03 |\n",
      "| steps                   | 2.41e+06 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 2206.7 -> 2213.6\n",
      "Saving model due to mean reward increase: 2213.6 -> 2236.5\n",
      "Saving model due to mean reward increase: 2236.5 -> 2243.2\n",
      "Saving model due to mean reward increase: 2243.2 -> 2255.9\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | 2.26e+03 |\n",
      "| steps                   | 2.45e+06 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 2255.9 -> 2257.7\n",
      "Saving model due to mean reward increase: 2257.7 -> 2265.2\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 610      |\n",
      "| mean 100 episode reward | 2.26e+03 |\n",
      "| steps                   | 2.49e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 620      |\n",
      "| mean 100 episode reward | 2.26e+03 |\n",
      "| steps                   | 2.54e+06 |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 2265.2 -> 2266.8\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 630      |\n",
      "| mean 100 episode reward | 2.27e+03 |\n",
      "| steps                   | 2.58e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 640      |\n",
      "| mean 100 episode reward | 2.25e+03 |\n",
      "| steps                   | 2.62e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 650      |\n",
      "| mean 100 episode reward | 2.26e+03 |\n",
      "| steps                   | 2.66e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 660      |\n",
      "| mean 100 episode reward | 2.26e+03 |\n",
      "| steps                   | 2.7e+06  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 670      |\n",
      "| mean 100 episode reward | 2.21e+03 |\n",
      "| steps                   | 2.74e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 680      |\n",
      "| mean 100 episode reward | 2.15e+03 |\n",
      "| steps                   | 2.78e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 690      |\n",
      "| mean 100 episode reward | 2.1e+03  |\n",
      "| steps                   | 2.82e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | 2.09e+03 |\n",
      "| steps                   | 2.86e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 710      |\n",
      "| mean 100 episode reward | 2.08e+03 |\n",
      "| steps                   | 2.9e+06  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 720      |\n",
      "| mean 100 episode reward | 2.08e+03 |\n",
      "| steps                   | 2.95e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 730      |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 2.99e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 740      |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 3.03e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 750      |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 3.07e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 760      |\n",
      "| mean 100 episode reward | 2.08e+03 |\n",
      "| steps                   | 3.11e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 770      |\n",
      "| mean 100 episode reward | 2.08e+03 |\n",
      "| steps                   | 3.15e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 780      |\n",
      "| mean 100 episode reward | 2.08e+03 |\n",
      "| steps                   | 3.19e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 790      |\n",
      "| mean 100 episode reward | 2.08e+03 |\n",
      "| steps                   | 3.23e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | 2.08e+03 |\n",
      "| steps                   | 3.27e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 810      |\n",
      "| mean 100 episode reward | 2.08e+03 |\n",
      "| steps                   | 3.31e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 820      |\n",
      "| mean 100 episode reward | 2.08e+03 |\n",
      "| steps                   | 3.35e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 830      |\n",
      "| mean 100 episode reward | 2.08e+03 |\n",
      "| steps                   | 3.4e+06  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 840      |\n",
      "| mean 100 episode reward | 2.08e+03 |\n",
      "| steps                   | 3.44e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 850      |\n",
      "| mean 100 episode reward | 2.08e+03 |\n",
      "| steps                   | 3.48e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 860      |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 3.52e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 870      |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 3.56e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 880      |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 3.6e+06  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 890      |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 3.64e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 900      |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 3.68e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 910      |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 3.72e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 920      |\n",
      "| mean 100 episode reward | 2.08e+03 |\n",
      "| steps                   | 3.76e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 930      |\n",
      "| mean 100 episode reward | 2.08e+03 |\n",
      "| steps                   | 3.81e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 940      |\n",
      "| mean 100 episode reward | 2.09e+03 |\n",
      "| steps                   | 3.85e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 950      |\n",
      "| mean 100 episode reward | 2.08e+03 |\n",
      "| steps                   | 3.89e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 960      |\n",
      "| mean 100 episode reward | 2.09e+03 |\n",
      "| steps                   | 3.93e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 970      |\n",
      "| mean 100 episode reward | 2.09e+03 |\n",
      "| steps                   | 3.97e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 980      |\n",
      "| mean 100 episode reward | 2.09e+03 |\n",
      "| steps                   | 4.01e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 990      |\n",
      "| mean 100 episode reward | 2.08e+03 |\n",
      "| steps                   | 4.05e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1e+03    |\n",
      "| mean 100 episode reward | 2.08e+03 |\n",
      "| steps                   | 4.09e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.01e+03 |\n",
      "| mean 100 episode reward | 2.08e+03 |\n",
      "| steps                   | 4.13e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.02e+03 |\n",
      "| mean 100 episode reward | 2.08e+03 |\n",
      "| steps                   | 4.17e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.03e+03 |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 4.21e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.04e+03 |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 4.26e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.05e+03 |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 4.3e+06  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.06e+03 |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 4.34e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.07e+03 |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 4.38e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.08e+03 |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 4.42e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.09e+03 |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 4.46e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.1e+03  |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 4.5e+06  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.11e+03 |\n",
      "| mean 100 episode reward | 2.07e+03 |\n",
      "| steps                   | 4.54e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.12e+03 |\n",
      "| mean 100 episode reward | 2.06e+03 |\n",
      "| steps                   | 4.58e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.13e+03 |\n",
      "| mean 100 episode reward | 2.06e+03 |\n",
      "| steps                   | 4.62e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.14e+03 |\n",
      "| mean 100 episode reward | 2.06e+03 |\n",
      "| steps                   | 4.67e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.15e+03 |\n",
      "| mean 100 episode reward | 2.06e+03 |\n",
      "| steps                   | 4.71e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.16e+03 |\n",
      "| mean 100 episode reward | 2.06e+03 |\n",
      "| steps                   | 4.75e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.17e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 4.79e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.18e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 4.83e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.19e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 4.87e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.2e+03  |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 4.91e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.21e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 4.95e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.22e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 4.99e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.23e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.03e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.24e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.07e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.25e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.12e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.26e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.16e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.27e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.2e+06  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.28e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.24e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.29e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.28e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.3e+03  |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.32e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.31e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.36e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.32e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.4e+06  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.33e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.44e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.34e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.48e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.35e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.53e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.36e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.57e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.37e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.61e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.38e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.65e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.39e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.69e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.4e+03  |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.73e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.41e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.77e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.42e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.81e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.43e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.85e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.44e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.89e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.45e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.94e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.46e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 5.98e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.47e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 6.02e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.48e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 6.06e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.49e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 6.1e+06  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.5e+03  |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 6.14e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.51e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 6.18e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.52e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 6.22e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.53e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 6.26e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.54e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 6.3e+06  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.55e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 6.34e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.56e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 6.39e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.57e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 6.43e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.58e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 6.47e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.59e+03 |\n",
      "| mean 100 episode reward | 2.06e+03 |\n",
      "| steps                   | 6.51e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.6e+03  |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 6.55e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.61e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 6.59e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.62e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 6.63e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.63e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 6.67e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.64e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 6.71e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.65e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 6.75e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.66e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 6.8e+06  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.67e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 6.84e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.68e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 6.88e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.69e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 6.92e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.7e+03  |\n",
      "| mean 100 episode reward | 2.04e+03 |\n",
      "| steps                   | 6.96e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.71e+03 |\n",
      "| mean 100 episode reward | 2.04e+03 |\n",
      "| steps                   | 7e+06    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.72e+03 |\n",
      "| mean 100 episode reward | 2.04e+03 |\n",
      "| steps                   | 7.04e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.73e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 7.08e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.74e+03 |\n",
      "| mean 100 episode reward | 2.04e+03 |\n",
      "| steps                   | 7.12e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.75e+03 |\n",
      "| mean 100 episode reward | 2.04e+03 |\n",
      "| steps                   | 7.16e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.76e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 7.2e+06  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.77e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 7.25e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.78e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 7.29e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.79e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 7.33e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.8e+03  |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 7.37e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.81e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 7.41e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.82e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 7.45e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.83e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 7.49e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.84e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 7.53e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.85e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 7.57e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.86e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 7.61e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.87e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 7.66e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.88e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 7.7e+06  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.89e+03 |\n",
      "| mean 100 episode reward | 2.04e+03 |\n",
      "| steps                   | 7.74e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.9e+03  |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 7.78e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.91e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 7.82e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.92e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 7.86e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.93e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 7.9e+06  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.94e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 7.94e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.95e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 7.98e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.96e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.02e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.97e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.07e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.98e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.11e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 1.99e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.15e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2e+03    |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.19e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.01e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.23e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.02e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.27e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.03e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.31e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.04e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.35e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.05e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.39e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.06e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.43e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.07e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.47e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.08e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.52e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.09e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.56e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.1e+03  |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.6e+06  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.11e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.64e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.12e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.68e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.13e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.72e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.14e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.76e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.15e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.8e+06  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.16e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.84e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.17e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.88e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.18e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.93e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.19e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 8.97e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.2e+03  |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 9.01e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.21e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 9.05e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.22e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 9.09e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.23e+03 |\n",
      "| mean 100 episode reward | 2.06e+03 |\n",
      "| steps                   | 9.13e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.24e+03 |\n",
      "| mean 100 episode reward | 2.06e+03 |\n",
      "| steps                   | 9.17e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.25e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 9.21e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.26e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 9.25e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.27e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 9.29e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.28e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 9.33e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.29e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 9.38e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.3e+03  |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 9.42e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.31e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 9.46e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.32e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 9.5e+06  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.33e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 9.54e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.34e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 9.58e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.35e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 9.62e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.36e+03 |\n",
      "| mean 100 episode reward | 2.04e+03 |\n",
      "| steps                   | 9.66e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.37e+03 |\n",
      "| mean 100 episode reward | 2.04e+03 |\n",
      "| steps                   | 9.7e+06  |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.38e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 9.74e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.39e+03 |\n",
      "| mean 100 episode reward | 2.04e+03 |\n",
      "| steps                   | 9.79e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.4e+03  |\n",
      "| mean 100 episode reward | 2.04e+03 |\n",
      "| steps                   | 9.83e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.41e+03 |\n",
      "| mean 100 episode reward | 2.04e+03 |\n",
      "| steps                   | 9.87e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.42e+03 |\n",
      "| mean 100 episode reward | 2.04e+03 |\n",
      "| steps                   | 9.91e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.43e+03 |\n",
      "| mean 100 episode reward | 2.05e+03 |\n",
      "| steps                   | 9.95e+06 |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 1        |\n",
      "| episodes                | 2.44e+03 |\n",
      "| mean 100 episode reward | 2.04e+03 |\n",
      "| steps                   | 9.99e+06 |\n",
      "--------------------------------------\n",
      "Restored model with mean reward: 2266.8\n",
      "DQN Training Time: 31808.277006864548\n"
     ]
    }
   ],
   "source": [
    "def ids_dqn():\n",
    "    logger.configure(dir='./Training/Logs/ids_dqn', format_strs=['stdout', 'tensorboard'])\n",
    "    env = IdsEnv(images_per_episode=4096)\n",
    "    env = bench.Monitor(env, logger.get_dir())\n",
    "\n",
    "    model = deepq.learn(\n",
    "        env,\n",
    "        \"mlp\",\n",
    "        num_layers=1,\n",
    "        num_hidden=64,\n",
    "        batch_size=16,\n",
    "        activation=tf.nn.relu,\n",
    "        hiddens=[32],\n",
    "        dueling=True,\n",
    "        lr=1e-4,\n",
    "        total_timesteps=int(1.0e7),\n",
    "        buffer_size=10000,\n",
    "        exploration_fraction=0.2,\n",
    "        exploration_final_eps=0.01,\n",
    "        train_freq=4,\n",
    "        learning_starts=10000,\n",
    "        target_network_update_freq=1000,\n",
    "        print_freq=10,\n",
    "#         double_q=True,\n",
    "        \n",
    "    )\n",
    "\n",
    "    model.save('dqn_ids.pkl')\n",
    "    env.close()\n",
    "    \n",
    "    return model\n",
    "\n",
    "start_time = time.time()\n",
    "dqn_model = ids_dqn()\n",
    "print(\"DQN Training Time:\", time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation done...\n",
      "Accuracy: 51.78945966641143%\n"
     ]
    }
   ],
   "source": [
    "def ids_dqn_eval(dqn_model):\n",
    "    attempts, correct = 0,0\n",
    "\n",
    "    env = IdsEnv(images_per_episode=1, dataset=(x_test, y_test), random=False)\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            obs, done = env.reset(), False\n",
    "            while not done:\n",
    "                obs, rew, done, _ = env.step(dqn_model(obs[None])[0])\n",
    "\n",
    "                attempts += 1\n",
    "                if rew > 0:\n",
    "                    correct += 1\n",
    "\n",
    "    except StopIteration:\n",
    "        print()\n",
    "        print('validation done...')\n",
    "        print('Accuracy: {0}%'.format((float(correct) / attempts) * 100))\n",
    "\n",
    "ids_dqn_eval(dqn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

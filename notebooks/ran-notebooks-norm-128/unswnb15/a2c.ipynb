{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rub setup.ipynb first when using on jupyterlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE FOR TRAINING DIFFERENT MODELS\n",
    "TYPE = 'A2C'\n",
    "NAME = 'default'\n",
    "DATASET = 'unswnb15'\n",
    "AMOUNT = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/stable_baselines/__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n",
      "  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 15363143930518774936,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 9179172281801232211\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 17168145891708721739\n",
       " physical_device_desc: \"device: XLA_GPU device\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from datetime import datetime\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.deepq.policies import FeedForwardPolicy, register_policy\n",
    "from stable_baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "\n",
    "from stable_baselines import deepq\n",
    "from stable_baselines import A2C\n",
    "from stable_baselines import bench\n",
    "from stable_baselines import logger\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [dueling, double, per, pn]\n",
    "parameters_presets = {\n",
    "    'vanilla': [False, False, False, False],\n",
    "    'dueling': [True, False, False, False],\n",
    "    'double': [False, True, False, False],\n",
    "    'dd': [True, True, False, False],\n",
    "    'pn_dd': [True, True, False, True],\n",
    "    'per_dd': [True, True, True, False],\n",
    "    'pn_per_dd': [True, True, True, True],\n",
    "    'default': [],\n",
    "}\n",
    "# [separate datasets?, train set dir, test set dir or empty, dummies list(only for sets with split train and test), topfeat?]\n",
    "dataset_presets = {\n",
    "    'nslkdd': [\n",
    "        True, \n",
    "        \"/project/datasets/clean-ids-collection/nsl-kdd/clean/KDDTrain.feather\", \n",
    "        \"/project/datasets/clean-ids-collection/nsl-kdd/clean/KDDTest.feather\",\n",
    "        [\n",
    "            'protocol_type', \n",
    "            'service',\n",
    "            'flag'\n",
    "        ],\n",
    "        'class',\n",
    "        'normal',\n",
    "        False\n",
    "              ],\n",
    "    'unswnb15': [\n",
    "        True, \n",
    "        \"/project/datasets/clean-ids-collection/unsw-nb15/clean/designated-train-test-sets/UNSW_NB15_training-set.feather\", \n",
    "        \"/project/datasets/clean-ids-collection/unsw-nb15/clean/designated-train-test-sets/UNSW_NB15_testing-set.feather\",\n",
    "        [\n",
    "            'proto', \n",
    "            'service',\n",
    "            'state'\n",
    "        ],\n",
    "        'attack_cat',\n",
    "        'normal',\n",
    "        False\n",
    "              ],\n",
    "    'cicddos2019': [\n",
    "        False, \n",
    "        \"/project/datasets/clean-ids-collection/cic-ddos2019/clean/cicddos2019.feather\", \n",
    "        None,\n",
    "        [],\n",
    "        'Label',\n",
    "        'Benign',\n",
    "        False\n",
    "              ],\n",
    "    'cicddos2019-top': [\n",
    "        False, \n",
    "        \"/project/datasets/clean-ids-collection/cic-ddos2019/clean/cicddos2019.feather\", \n",
    "        None,\n",
    "        [],\n",
    "        'Label',\n",
    "        'Benign',\n",
    "        True\n",
    "              ],\n",
    "    'cicdos2017': [\n",
    "        False, \n",
    "        \"/project/datasets/clean-ids-collection/cic-dos2017/clean/cicdos2017.feather\", \n",
    "        None,\n",
    "        [],\n",
    "        'Label',\n",
    "        'Benign',\n",
    "        False\n",
    "              ],\n",
    "    'cicids2017': [\n",
    "        False, \n",
    "        \"/project/datasets/clean-ids-collection/cic-ids2017/clean/cicids2017.feather\", \n",
    "        None,\n",
    "        [],\n",
    "        'Label',\n",
    "        'Benign',\n",
    "        False\n",
    "              ],\n",
    "}\n",
    "parameters = parameters_presets[NAME]\n",
    "data_parameters = dataset_presets[DATASET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mal_ben(df, label, normal):\n",
    "    print(\"Preprocessing: make labels binary\")\n",
    "    print()\n",
    "    df[label] = df[label].astype('object')\n",
    "    atk_idx = df.loc[df[label] != normal].index\n",
    "    df.loc[atk_idx, label] = 1.0\n",
    "    df.loc[df.index.difference(atk_idx), label] = 0.0\n",
    "    df[label] = df[label].astype(dtype=np.float32)\n",
    "\n",
    "def preproc(df_train, df_test, columns):\n",
    "    print(\"Preprocessing: One hot encoding + abundant features\")\n",
    "    print()\n",
    "    df_train = pd.get_dummies(df_train,columns=columns)\n",
    "    df_test = pd.get_dummies(df_test,columns=columns)\n",
    "    extra_removables = df_test.columns ^ df_train.columns\n",
    "    print(extra_removables)\n",
    "    for to_remove in extra_removables:\n",
    "            try:\n",
    "                df_test.drop(to_remove, inplace=True, axis=1)\n",
    "            except:\n",
    "                print(f\"{to_remove} already not part of test set, skipping\")\n",
    "            try:\n",
    "                df_train.drop(to_remove, inplace=True, axis=1)\n",
    "            except:\n",
    "                print(f\"{to_remove} already not part of train set, skipping\")\n",
    "    return df_train, df_test\n",
    "\n",
    "def split_df(df):\n",
    "    print(\"Preprocessing: split into X and Y sets\")\n",
    "    print()\n",
    "    # parameters[4]\n",
    "    col = df.columns[-1]\n",
    "    cols = df.loc[:-1]\n",
    "    print(col)\n",
    "    print(cols)\n",
    "    return df[cols], df[col]\n",
    "\n",
    "def top_feat(df, dataset):\n",
    "    print(\"Preprocessing: convert to top features\")\n",
    "    print()\n",
    "    features = {\n",
    "        'cicdos2017-top': [\"Init Bwd Win Bytes\",  \"Idle Min\", \"ACK Flag Count\", \"Fwd Packet Length Min\", \"Fwd PSH Flags\"],\n",
    "        'cicids2017-top': [\"Avg Packet Size\", \"Packet Length Variance\", \"Fwd Packet Length Mean\", \"Init Fwd Win Bytes\", \"Bwd IAT Total\"],\n",
    "        'cicddos2019-top': [\"URG Flag Count\", \"Down/Up Ratio\", \"Bwd Packet Length Min\", \"ACK Flag Count\", \"Fwd Packets Length Total\"],\n",
    "        'nslkdd-top': [\"dst_host_serror_rate\", \"service_private\", \"count\",\"dst_host_count\", \"service_domain_u\", \"flag_REJ\", \"dst_host_diff_srv_rate\"],\n",
    "        'unswnb15-top': [\"sttl\", \"dttl\", \"ct_state_ttl\", \"service\", \"dload\", \"rate\", \"dmean\", \"dbytes\", \"dur\", \"is_sm_ips_ports\", \"dloss\"],\n",
    "    }\n",
    "    important_features = features[dataset] + [\"Label\"] # Adding class for custom environment logic\n",
    "    important_features = list(set(important_features))\n",
    "    removable_features = df.columns ^ important_features\n",
    "    \n",
    "    return df.drop(labels=removable_features, axis='columns')\n",
    "\n",
    "def nslkdd_split_df(train_df, test_df):\n",
    "    train_col = data_parameters[4]\n",
    "    print(train_col)\n",
    "    train_cols = train_df.loc[:, train_df.columns != data_parameters[4]].columns\n",
    "    print(train_cols)\n",
    "    test_col = data_parameters[4]\n",
    "    print(test_col)\n",
    "    test_cols = test_df.loc[:, test_df.columns != data_parameters[4]].columns\n",
    "    print(test_cols)\n",
    "\n",
    "    return train_df[train_cols], train_df[train_col], test_df[test_cols], test_df[test_col]\n",
    "\n",
    "def balancing_dataset_sampler_df(df, train_frac=0.2, val_frac=0.1, test_frac=0.7):\n",
    "    col = df.columns[-1]\n",
    "    print(col)\n",
    "    cols = df.columns[:-1]\n",
    "    print(cols)\n",
    "    vc = df[col].value_counts()\n",
    "    print(vc)\n",
    "    n = vc.iloc[-1]\n",
    "    print(n)\n",
    "    m = vc.iloc[0]\n",
    "    print(m)\n",
    "    print(int(m-n))\n",
    "    initial_cut = df.loc[df[col] == vc.index[0]].sample(n=int(m-n), replace=False)\n",
    "    print(initial_cut.index)\n",
    "    df = df.drop(index=initial_cut.index)\n",
    "    vc = df[col].value_counts()\n",
    "    print(vc)\n",
    "    print(int(n*train_frac))\n",
    "    train_df = df.groupby(col).apply(lambda x: x.sample(n=int(n*train_frac), replace=False))\n",
    "    train_df.reset_index(level=0, inplace=True, drop=True)\n",
    "    df = df.drop(index=train_df.index)\n",
    "\n",
    "    validation_df = df.groupby(col).apply(lambda x: x.sample(n=int(n*val_frac), replace=False))\n",
    "    validation_df.reset_index(level=0, inplace=True, drop=True)\n",
    "    df = df.drop(index=validation_df.index)\n",
    "\n",
    "    test_df = df.groupby(col).apply(lambda x: x.sample(n=int(n*test_frac), replace=False))\n",
    "    test_df.reset_index(level=0, inplace=True, drop=True)\n",
    "    df = df.drop(index=test_df.index)\n",
    "\n",
    "    return train_df[cols], train_df[col], validation_df[cols], validation_df[col], test_df[cols], test_df[col]\n",
    "\n",
    "\n",
    "def ids_eval(model, x_test, y_test):\n",
    "    print(\"Evaluation: starting validation of the model\")\n",
    "    print()\n",
    "    TP, FP, TN, FN = 0,0,0,0\n",
    "    env = IdsEnv(images_per_episode=1, dataset=(x_test, y_test), random=False)\n",
    "    obs, done = env.reset(), False\n",
    "    try:\n",
    "        while True:\n",
    "            obs, done = env.reset(), False\n",
    "            while not done:\n",
    "                obs, rew, done, info = env.step(model.predict(obs)[0])\n",
    "                label = info['label']\n",
    "                if label == 0 and rew > 0:\n",
    "                    TP += 1\n",
    "                if label == 0 and rew == 0:\n",
    "                    FP += 1\n",
    "                if label == 1 and rew > 0:\n",
    "                    TN += 1\n",
    "                if label == 1 and rew == 0:\n",
    "                    FN += 1\n",
    "\n",
    "    except StopIteration:\n",
    "        accuracy = (float(TP + TN) / (TP + FP + FN + TN)) \n",
    "        precision = (float(TP) / (TP + FP))\n",
    "        recall = (float(TP) / (TP + FN)) # = TPR = Sensitivity\n",
    "        try:\n",
    "            FPR = (float(FP) / (TN + FP)) # 1 - specificity\n",
    "        except:\n",
    "            FPR = 0.0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        print()\n",
    "        print('Evaluation: validation done...')\n",
    "        print('Accuracy: {0}%'.format(accuracy * 100))\n",
    "        print('Precision: {0}%'.format(precision * 100))\n",
    "        print('Recall/TPR/Sensitivity: {0}%'.format(recall * 100))\n",
    "        print('FPR: {0}%'.format(FPR * 100))\n",
    "        print('F1 score: {0}'.format(f1_score))\n",
    "    return [accuracy, precision, recall, FPR, f1_score]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing: One hot encoding + abundant features\n",
      "\n",
      "Index(['proto_icmp', 'proto_rtp', 'state_ACC', 'state_CLO', 'state_ECO',\n",
      "       'state_PAR', 'state_URN', 'state_no'],\n",
      "      dtype='object')\n",
      "proto_icmp already not part of train set, skipping\n",
      "proto_rtp already not part of train set, skipping\n",
      "state_ACC already not part of test set, skipping\n",
      "state_CLO already not part of test set, skipping\n",
      "state_ECO already not part of train set, skipping\n",
      "state_PAR already not part of train set, skipping\n",
      "state_URN already not part of train set, skipping\n",
      "state_no already not part of train set, skipping\n"
     ]
    }
   ],
   "source": [
    "if data_parameters[0]:\n",
    "    df_train = pd.read_feather(data_parameters[1])\n",
    "    df_test = pd.read_feather(data_parameters[2])\n",
    "    df_train, df_test = preproc(df_train, df_test, data_parameters[3])\n",
    "    LENGTH = len(df_train.columns) - 1\n",
    "else:\n",
    "    df = pd.read_feather(data_parameters[1])\n",
    "    LENGTH = len(df.columns) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53946 entries, 0 to 53945\n",
      "Columns: 190 entries, dur to state_RST\n",
      "dtypes: bool(3), category(1), float32(11), int16(11), int32(3), int64(2), int8(10), uint8(149)\n",
      "memory usage: 13.2 MB\n"
     ]
    }
   ],
   "source": [
    "if data_parameters[0]:\n",
    "    df_train.info()\n",
    "    df_train.describe()\n",
    "else:\n",
    "    df.info()\n",
    "    df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_parameters[6]:\n",
    "    if data_parameters[0]:\n",
    "        df_train = top_feat(df_train, DATASET)\n",
    "        df_test = top_feat(df_test, DATASET)\n",
    "    else:\n",
    "        df = top_feat(df, DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing done by Laurens D'Hooge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "- Build an agent to classify network flow automatically\n",
    "- Feed a packet that gets classified\n",
    "- Want the classification to be equal to the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing: make labels binary\n",
      "\n",
      "Preprocessing: make labels binary\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if data_parameters[0]:\n",
    "    mal_ben(df_train, data_parameters[4], data_parameters[5])\n",
    "    mal_ben(df_test, data_parameters[4], data_parameters[5])\n",
    "else: \n",
    "    mal_ben(df, data_parameters[4], data_parameters[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSW-NB15 specific\n",
    "df_train.drop('label', inplace=True, axis=1)\n",
    "df_test.drop('label', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_parameters[0]:\n",
    "    LENGTH = len(df_train.columns) - 1\n",
    "else:\n",
    "    LENGTH = len(df.columns) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_cat\n",
      "Index(['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl',\n",
      "       'sload', 'dload',\n",
      "       ...\n",
      "       'service_radius', 'service_smtp', 'service_snmp', 'service_ssh',\n",
      "       'service_ssl', 'state_CON', 'state_FIN', 'state_INT', 'state_REQ',\n",
      "       'state_RST'],\n",
      "      dtype='object', length=188)\n",
      "attack_cat\n",
      "Index(['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl',\n",
      "       'sload', 'dload',\n",
      "       ...\n",
      "       'service_radius', 'service_smtp', 'service_snmp', 'service_ssh',\n",
      "       'service_ssl', 'state_CON', 'state_FIN', 'state_INT', 'state_REQ',\n",
      "       'state_RST'],\n",
      "      dtype='object', length=188)\n"
     ]
    }
   ],
   "source": [
    "if data_parameters[0]:\n",
    "    x_train, y_train, x_test, y_test  = nslkdd_split_df(df_train, df_test)\n",
    "else:\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = balancing_dataset_sampler_df(df, train_frac=0.8, val_frac=0.0, test_frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dur\n",
      "spkts\n",
      "dpkts\n",
      "sbytes\n",
      "dbytes\n",
      "rate\n",
      "sttl\n",
      "dttl\n",
      "sload\n",
      "dload\n",
      "sloss\n",
      "dloss\n",
      "sinpkt\n",
      "dinpkt\n",
      "sjit\n",
      "djit\n",
      "swin\n",
      "stcpb\n",
      "dtcpb\n",
      "dwin\n",
      "tcprtt\n",
      "synack\n",
      "ackdat\n",
      "smean\n",
      "dmean\n",
      "trans_depth\n",
      "response_body_len\n",
      "ct_srv_src\n",
      "ct_state_ttl\n",
      "ct_dst_ltm\n",
      "ct_src_dport_ltm\n",
      "ct_dst_sport_ltm\n",
      "ct_dst_src_ltm\n",
      "is_ftp_login\n",
      "ct_ftp_cmd\n",
      "ct_flw_http_mthd\n",
      "ct_src_ltm\n",
      "ct_srv_dst\n",
      "is_sm_ips_ports\n",
      "proto_3pc\n",
      "proto_a/n\n",
      "proto_aes-sp3-d\n",
      "proto_any\n",
      "proto_argus\n",
      "proto_aris\n",
      "proto_arp\n",
      "proto_ax.25\n",
      "proto_bbn-rcc\n",
      "proto_bna\n",
      "proto_br-sat-mon\n",
      "proto_cbt\n",
      "proto_cftp\n",
      "proto_chaos\n",
      "proto_compaq-peer\n",
      "proto_cphb\n",
      "proto_cpnx\n",
      "proto_crtp\n",
      "proto_crudp\n",
      "proto_dcn\n",
      "proto_ddp\n",
      "proto_ddx\n",
      "proto_dgp\n",
      "proto_egp\n",
      "proto_eigrp\n",
      "proto_emcon\n",
      "proto_encap\n",
      "proto_etherip\n",
      "proto_fc\n",
      "proto_fire\n",
      "proto_ggp\n",
      "proto_gmtp\n",
      "proto_gre\n",
      "proto_hmp\n",
      "proto_i-nlsp\n",
      "proto_iatp\n",
      "proto_ib\n",
      "proto_idpr\n",
      "proto_idpr-cmtp\n",
      "proto_idrp\n",
      "proto_ifmp\n",
      "proto_igmp\n",
      "proto_igp\n",
      "proto_il\n",
      "proto_ip\n",
      "proto_ipcomp\n",
      "proto_ipcv\n",
      "proto_ipip\n",
      "proto_iplt\n",
      "proto_ipnip\n",
      "proto_ippc\n",
      "proto_ipv6\n",
      "proto_ipv6-frag\n",
      "proto_ipv6-no\n",
      "proto_ipv6-opts\n",
      "proto_ipv6-route\n",
      "proto_ipx-n-ip\n",
      "proto_irtp\n",
      "proto_isis\n",
      "proto_iso-ip\n",
      "proto_iso-tp4\n",
      "proto_kryptolan\n",
      "proto_l2tp\n",
      "proto_larp\n",
      "proto_leaf-1\n",
      "proto_leaf-2\n",
      "proto_merit-inp\n",
      "proto_mfe-nsp\n",
      "proto_mhrp\n",
      "proto_micp\n",
      "proto_mobile\n",
      "proto_mtp\n",
      "proto_mux\n",
      "proto_narp\n",
      "proto_netblt\n",
      "proto_nsfnet-igp\n",
      "proto_nvp\n",
      "proto_ospf\n",
      "proto_pgm\n",
      "proto_pim\n",
      "proto_pipe\n",
      "proto_pnni\n",
      "proto_pri-enc\n",
      "proto_prm\n",
      "proto_ptp\n",
      "proto_pup\n",
      "proto_pvp\n",
      "proto_qnx\n",
      "proto_rdp\n",
      "proto_rsvp\n",
      "proto_rvd\n",
      "proto_sat-expak\n",
      "proto_sat-mon\n",
      "proto_sccopmce\n",
      "proto_scps\n",
      "proto_sctp\n",
      "proto_sdrp\n",
      "proto_secure-vmtp\n",
      "proto_sep\n",
      "proto_skip\n",
      "proto_sm\n",
      "proto_smp\n",
      "proto_snp\n",
      "proto_sprite-rpc\n",
      "proto_sps\n",
      "proto_srp\n",
      "proto_st2\n",
      "proto_stp\n",
      "proto_sun-nd\n",
      "proto_swipe\n",
      "proto_tcf\n",
      "proto_tcp\n",
      "proto_tlsp\n",
      "proto_tp++\n",
      "proto_trunk-1\n",
      "proto_trunk-2\n",
      "proto_ttp\n",
      "proto_udp\n",
      "proto_unas\n",
      "proto_uti\n",
      "proto_vines\n",
      "proto_visa\n",
      "proto_vmtp\n",
      "proto_vrrp\n",
      "proto_wb-expak\n",
      "proto_wb-mon\n",
      "proto_wsn\n",
      "proto_xnet\n",
      "proto_xns-idp\n",
      "proto_xtp\n",
      "proto_zero\n",
      "service_-\n",
      "service_dhcp\n",
      "service_dns\n",
      "service_ftp\n",
      "service_ftp-data\n",
      "service_http\n",
      "service_irc\n",
      "service_pop3\n",
      "service_radius\n",
      "service_smtp\n",
      "service_snmp\n",
      "service_ssh\n",
      "service_ssl\n",
      "state_CON\n",
      "state_FIN\n",
      "state_INT\n",
      "state_REQ\n",
      "state_RST\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.to_frame()\n",
    "y_test = y_test.to_frame()\n",
    "\n",
    "# custom keys -> replace by index\n",
    "\n",
    "x_train = x_train.set_index([pd.Index(range (0, len(x_train)))])\n",
    "y_train = y_train.set_index([pd.Index(range (0, len(y_train)))])\n",
    "x_test = x_test.set_index([pd.Index(range (0, len(x_test)))])\n",
    "y_test = y_test.set_index([pd.Index(range (0, len(y_test)))])\n",
    "\n",
    "for col in x_train.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdsEnv(gym.Env):\n",
    "    def __init__(self, images_per_episode=1, dataset=(x_train, y_train), random=True):\n",
    "        # Actions we can take, classify as malicious or non-malicious (later also the correct attack)\n",
    "        # change to 19 if detectiong all different attacks\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "         # All the features we have, len(important_features) - 1 features and 1 label. Label should not be included\n",
    "        self.observation_space = gym.spaces.Box(low=float('-inf'), high=float('inf'), shape=(LENGTH,))\n",
    "        self.images_per_episode = images_per_episode\n",
    "        self.step_count = 0\n",
    "\n",
    "        self.x, self.y = dataset\n",
    "        self.random = random\n",
    "        self.dataset_idx = 0\n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = int(action == self.expected_action)\n",
    "        current_label = self.expected_action\n",
    "        obs = self._next_obs()\n",
    "\n",
    "        self.step_count += 1\n",
    "        if self.step_count >= self.images_per_episode:\n",
    "            done = True\n",
    "\n",
    "        return obs, reward, done, {'label': current_label}\n",
    "\n",
    "    def _next_obs(self):\n",
    "        if self.random:\n",
    "            next_obs_idx = random.randint(0, len(self.x) - 1)\n",
    "            self.expected_action = int(self.y.iloc[next_obs_idx,:])\n",
    "            obs = self.x.iloc[next_obs_idx,:]\n",
    "\n",
    "        else:\n",
    "            obs = self.x.iloc[self.dataset_idx]\n",
    "            self.expected_action = int(self.y.iloc[self.dataset_idx])\n",
    "\n",
    "            self.dataset_idx += 1\n",
    "            if self.dataset_idx >= len(self.x):\n",
    "                raise StopIteration()\n",
    "        return obs\n",
    "    \n",
    "    def reset(self):\n",
    "        self.step_count = 0\n",
    "\n",
    "        obs = self._next_obs()\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train classifier using dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPolicy(FeedForwardPolicy):\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch,\n",
    "                 reuse=False, obs_phs=None, dueling=True, **_kwargs):\n",
    "        super(CustomPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse,\n",
    "                                        feature_extraction=\"mlp\", obs_phs=obs_phs, dueling=dueling,\n",
    "                                        layer_norm=True, **_kwargs)\n",
    "        \n",
    "register_policy(\"CustomPolicy\", CustomPolicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f6b98769d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f6b98769d50>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f6b987a1c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f6b987a1c50>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "---------------------------------\n",
      "| explained_variance | 0.908    |\n",
      "| fps                | 8        |\n",
      "| nupdates           | 1        |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 5        |\n",
      "| value_loss         | 0.303    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.161    |\n",
      "| fps                | 291      |\n",
      "| nupdates           | 10000    |\n",
      "| policy_entropy     | 0.00247  |\n",
      "| total_timesteps    | 50000    |\n",
      "| value_loss         | 0.326    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | nan      |\n",
      "| fps                | 305      |\n",
      "| nupdates           | 20000    |\n",
      "| policy_entropy     | 0.0051   |\n",
      "| total_timesteps    | 100000   |\n",
      "| value_loss         | 0.0473   |\n",
      "---------------------------------\n",
      "\n",
      "A2C 1 Training Time: 328.8157579898834\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f6aef7a8d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f6aef7a8d90>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f6adc2e1710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f6adc2e1710>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "---------------------------------\n",
      "| explained_variance | -0.335   |\n",
      "| fps                | 17       |\n",
      "| nupdates           | 1        |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 5        |\n",
      "| value_loss         | 0.343    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | nan      |\n",
      "| fps                | 322      |\n",
      "| nupdates           | 10000    |\n",
      "| policy_entropy     | 0.00546  |\n",
      "| total_timesteps    | 50000    |\n",
      "| value_loss         | 0.159    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.107   |\n",
      "| fps                | 324      |\n",
      "| nupdates           | 20000    |\n",
      "| policy_entropy     | 0.00314  |\n",
      "| total_timesteps    | 100000   |\n",
      "| value_loss         | 0.337    |\n",
      "---------------------------------\n",
      "\n",
      "A2C 2 Training Time: 309.5610740184784\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f6acc63f450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f6acc63f450>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f6acc392810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f6acc392810>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "---------------------------------\n",
      "| explained_variance | -0.778   |\n",
      "| fps                | 17       |\n",
      "| nupdates           | 1        |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 5        |\n",
      "| value_loss         | 0.984    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.0923   |\n",
      "| fps                | 321      |\n",
      "| nupdates           | 10000    |\n",
      "| policy_entropy     | 0.00789  |\n",
      "| total_timesteps    | 50000    |\n",
      "| value_loss         | 0.228    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0678  |\n",
      "| fps                | 321      |\n",
      "| nupdates           | 20000    |\n",
      "| policy_entropy     | 0.00243  |\n",
      "| total_timesteps    | 100000   |\n",
      "| value_loss         | 0.264    |\n",
      "---------------------------------\n",
      "\n",
      "A2C 3 Training Time: 312.41051411628723\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f6ac45c2dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f6ac45c2dd0>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f6ac4458850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f6ac4458850>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "---------------------------------\n",
      "| explained_variance | -0.833   |\n",
      "| fps                | 16       |\n",
      "| nupdates           | 1        |\n",
      "| policy_entropy     | 0.693    |\n",
      "| total_timesteps    | 5        |\n",
      "| value_loss         | 0.698    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | -0.0616  |\n",
      "| fps                | 320      |\n",
      "| nupdates           | 10000    |\n",
      "| policy_entropy     | 0.00476  |\n",
      "| total_timesteps    | 50000    |\n",
      "| value_loss         | 0.258    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| explained_variance | 0.0092   |\n",
      "| fps                | 318      |\n",
      "| nupdates           | 20000    |\n",
      "| policy_entropy     | 0.0143   |\n",
      "| total_timesteps    | 100000   |\n",
      "| value_loss         | 0.175    |\n",
      "---------------------------------\n",
      "\n",
      "A2C 4 Training Time: 314.6314535140991\n"
     ]
    }
   ],
   "source": [
    "def ids(params, tp):\n",
    "    \n",
    "    env = IdsEnv(images_per_episode=1)\n",
    "    if tp == 'DQN':\n",
    "        model = deepq.DQN(\n",
    "            CustomPolicy, \n",
    "            env, \n",
    "            policy_kwargs=dict(dueling=params[0], layers=[128,128]),\n",
    "            double_q=params[1],\n",
    "            verbose=1, \n",
    "            learning_rate=0.00025,\n",
    "            buffer_size=1000000,\n",
    "            exploration_fraction=0.1,\n",
    "            exploration_final_eps=0.1,\n",
    "            train_freq=4,\n",
    "            learning_starts=5000,\n",
    "            target_network_update_freq=10000,\n",
    "            gamma=1.0,\n",
    "            param_noise=params[3],\n",
    "            prioritized_replay=params[2],\n",
    "            prioritized_replay_alpha=0.6,\n",
    "            batch_size=32,\n",
    "        )\n",
    "    if tp == 'A2C':\n",
    "        model = A2C(\n",
    "            MlpPolicy, \n",
    "            env, \n",
    "            verbose=1,\n",
    "        )\n",
    "    model.learn(\n",
    "        total_timesteps=int(1.0e5),\n",
    "        log_interval=int(1.0e4),\n",
    "    )\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "    return model\n",
    "\n",
    "times = []\n",
    "models = []\n",
    "\n",
    "for i in range(AMOUNT):\n",
    "    start_time = time.time()\n",
    "    models.append(ids(parameters, TYPE))\n",
    "    print()\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"{TYPE} {i + 1} Training Time:\", duration)\n",
    "    times.append(duration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation done...\n",
      "Accuracy: 59.59539975058888%\n",
      "Precision: 95.3481183834325%\n",
      "Recall/TPR/Sensitivity: 56.232055780886846%\n",
      "FPR: 18.151665912859944%\n",
      "F1 score: 0.7074303405572755\n",
      "\n",
      "validation done...\n",
      "Accuracy: 59.583523030938856%\n",
      "Precision: 95.32493624913066%\n",
      "Recall/TPR/Sensitivity: 56.2260711030082%\n",
      "FPR: 18.225636391022746%\n",
      "F1 score: 0.7073191708953298\n",
      "\n",
      "validation done...\n",
      "Accuracy: 59.60925592351393%\n",
      "Precision: 95.278571980527%\n",
      "Recall/TPR/Sensitivity: 56.24615103893438%\n",
      "FPR: 18.3043738765728%\n",
      "F1 score: 0.7073503047687343\n",
      "\n",
      "validation done...\n",
      "Accuracy: 59.593420297313884%\n",
      "Precision: 95.20516188857121%\n",
      "Recall/TPR/Sensitivity: 56.24129824481319%\n",
      "FPR: 18.50581568744408%\n",
      "F1 score: 0.7071095487481168\n",
      "\n",
      "Total validation done...\n",
      "Accuracy: 59.59539975058888%\n",
      "Precision: 95.28919712541534%\n",
      "Recall/TPR/Sensitivity: 56.23639404191066%\n",
      "FPR: 18.29687296697489%\n",
      "F1 score: 0.707302341242364\n",
      "\n",
      "Saving model 1 to /project/normalization-runs/models/unswnb15/A2C/default_A2C_1.pkl\n",
      "\n",
      "Saving model 2 to /project/normalization-runs/models/unswnb15/A2C/default_A2C_2.pkl\n",
      "\n",
      "Saving model 3 to /project/normalization-runs/models/unswnb15/A2C/default_A2C_3.pkl\n",
      "\n",
      "Saving model 4 to /project/normalization-runs/models/unswnb15/A2C/default_A2C_4.pkl\n"
     ]
    }
   ],
   "source": [
    "# 0 is benign (positive), 1 is malicious (negative) \n",
    "def ids_eval(model):\n",
    "    TP, FP, TN, FN = 0,0,0,0\n",
    "    env = IdsEnv(images_per_episode=1, dataset=(x_test, y_test), random=False)\n",
    "    obs, done = env.reset(), False\n",
    "    try:\n",
    "        while True:\n",
    "            obs, done = env.reset(), False\n",
    "            while not done:\n",
    "                obs, rew, done, info = env.step(model.predict(obs)[0])\n",
    "                label = info['label']\n",
    "                if label == 0 and rew > 0:\n",
    "                    TP += 1\n",
    "                if label == 0 and rew == 0:\n",
    "                    FP += 1\n",
    "                if label == 1 and rew > 0:\n",
    "                    TN += 1\n",
    "                if label == 1 and rew == 0:\n",
    "                    FN += 1\n",
    "\n",
    "    except StopIteration:\n",
    "        accuracy = (float(TP + TN) / (TP + FP + FN + TN)) \n",
    "        precision = (float(TP) / (TP + FP))\n",
    "        try:\n",
    "            recall = (float(TP) / (TP + FN)) # = TPR = Sensitivity\n",
    "        except:\n",
    "            recall = 0\n",
    "        try:\n",
    "            FPR = (float(FP) / (TN + FP)) # 1 - specificity\n",
    "        except:\n",
    "            FPR = 0\n",
    "        try:\n",
    "            f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        except:\n",
    "            f1_score = 0\n",
    "        print()\n",
    "        print('validation done...')\n",
    "        print('Accuracy: {0}%'.format(accuracy * 100))\n",
    "        print('Precision: {0}%'.format(precision * 100))\n",
    "        print('Recall/TPR/Sensitivity: {0}%'.format(recall * 100))\n",
    "        print('FPR: {0}%'.format(FPR * 100))\n",
    "        print('F1 score: {0}'.format(f1_score))\n",
    "    return [accuracy, precision, recall, FPR, f1_score]\n",
    "\n",
    "results = []\n",
    "for i, m in enumerate(models):\n",
    "    results.append(ids_eval(m))\n",
    "\n",
    "total_results = []\n",
    "total_results = [0] * len(results[0])\n",
    "accuracies = [result[0] for result in results]\n",
    "\n",
    "for j in range(AMOUNT):\n",
    "    for i in range(len(results[0])):\n",
    "        total_results[i] =  total_results[i] + results[j][i]\n",
    "for i in range(len(results[0])):\n",
    "        total_results[i] =  total_results[i] / AMOUNT\n",
    "        \n",
    "print()    \n",
    "print('Total validation done...')\n",
    "print('Accuracy: {0}%'.format(total_results[0] * 100))\n",
    "print('Precision: {0}%'.format(total_results[1] * 100))\n",
    "print('Recall/TPR/Sensitivity: {0}%'.format(total_results[2] * 100))\n",
    "print('FPR: {0}%'.format(total_results[3] * 100))\n",
    "print('F1 score: {0}'.format(total_results[4]))\n",
    "\n",
    "\n",
    "path = f'/project/normalization-runs/models'\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    print(f\"{path} doesn't exist, creating it + subfolders\")\n",
    "    os.makedirs(path)\n",
    "    os.makedirs(f'{path}/{DATASET}')\n",
    "    os.makedirs(f'{path}/{DATASET}/DQN')\n",
    "    os.makedirs(f'{path}/{DATASET}/A2C')\n",
    "    \n",
    "if not os.path.exists(f'{path}/{DATASET}'):\n",
    "    print(f\"{path}/{DATASET} doesn't exist, creating it + subfolders\")\n",
    "    os.makedirs(f'{path}/{DATASET}')\n",
    "    os.makedirs(f'{path}/{DATASET}/DQN')\n",
    "    os.makedirs(f'{path}/{DATASET}/A2C')\n",
    "\n",
    "for i in range(AMOUNT):\n",
    "    print()\n",
    "    print(f'Saving model {i + 1} to {path}/{DATASET}/{TYPE}/{NAME}_{TYPE}_{i + 1}.pkl')\n",
    "    models[i].save(f'{path}/{DATASET}/{TYPE}/{NAME}_{TYPE}_{i + 1}.pkl')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
